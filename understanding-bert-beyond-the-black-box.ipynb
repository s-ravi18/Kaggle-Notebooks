{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üîç Breaking Down BERT: A Hands-On Dive into Transformers","metadata":{}},{"cell_type":"markdown","source":"Hey there! üëã\n\nIn this notebook, I‚Äôve taken up a personal challenge ‚Äî to debunk the BERT model and truly understand what‚Äôs happening under the hood. While BERT has become a go-to model in the world of NLP, I often found myself using it like a black box. So, I decided to go beyond just running code ‚Äî I wanted to get into the math, the mechanics, and the magic behind the Transformer architecture that powers it.\n\nThis notebook is a result of that journey ‚Äî a hands-on, intuitive walkthrough that aims to simplify concepts without skipping the key mathematical ideas. If you‚Äôve ever felt like you wanted to ‚Äúget‚Äù BERT instead of just ‚Äúuse‚Äù BERT, you‚Äôre in the right place.\n\nLet‚Äôs get started üöÄ","metadata":{}},{"cell_type":"markdown","source":"# About the Dataset\n\nFor this breakdown, I‚Äôm using the Cornell Movie Dialogues Corpus ‚Äî a classic dataset packed with movie conversations between characters. It‚Äôs perfect for experimenting with language models like BERT because it offers natural, back-and-forth human dialogue.\n\nIn this step, I'm:\n\n- Installing the necessary NLP libraries from Hugging Face.\n\n- Downloading and unzipping the dataset.\n\n- Organizing the relevant files (movie_conversations.txt and movie_lines.txt) into a datasets/ folder for easy access.","metadata":{}},{"cell_type":"code","source":"!pip install -qq transformers datasets tokenizers\n!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n!unzip -oq cornell_movie_dialogs_corpus.zip \n!rm cornell_movie_dialogs_corpus.zip\n!mkdir -p datasets\n!mv -f \"cornell movie-dialogs corpus/movie_conversations.txt\" ./datasets/\n!mv -f \"cornell movie-dialogs corpus/movie_lines.txt\" ./datasets/\n","metadata":{"id":"UN_CS32N7kvL","outputId":"8f540604-fc71-433c-e126-9432420d189f","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:12:59.428032Z","iopub.execute_input":"2025-05-01T07:12:59.428675Z","iopub.status.idle":"2025-05-01T07:13:05.614393Z","shell.execute_reply.started":"2025-05-01T07:12:59.428647Z","shell.execute_reply":"2025-05-01T07:13:05.613395Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m--2025-05-01 07:13:03--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\nResolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.53\nConnecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.53|:80... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip [following]\n--2025-05-01 07:13:04--  https://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\nConnecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.53|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9916637 (9.5M) [application/zip]\nSaving to: ‚Äòcornell_movie_dialogs_corpus.zip‚Äô\n\ncornell_movie_dialo 100%[===================>]   9.46M  21.6MB/s    in 0.4s    \n\n2025-05-01 07:13:04 (21.6 MB/s) - ‚Äòcornell_movie_dialogs_corpus.zip‚Äô saved [9916637/9916637]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"## Importing the necessary libraries.\n\nimport os\nfrom pathlib import Path\nimport torch\nimport re\nimport random\nimport transformers, datasets\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer\nimport tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport itertools\nimport math\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.optim import Adam","metadata":{"id":"OJy9FBBA8tQv","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:05.616206Z","iopub.execute_input":"2025-05-01T07:13:05.616475Z","iopub.status.idle":"2025-05-01T07:13:13.303680Z","shell.execute_reply.started":"2025-05-01T07:13:05.616450Z","shell.execute_reply":"2025-05-01T07:13:13.303067Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# üóÇÔ∏è Understanding the Dataset Structure\n\nBefore diving into modeling, let‚Äôs quickly understand the structure of the two main files we'll be working with:\n\n**movie_lines.txt:**\nThis file contains every individual line of dialogue from the dataset.\nEach line follows this format:\n\n```\nlineID +++$+++ characterID +++$+++ movieID +++$+++ character name +++$+++ actual dialogue\n```\n\n**movie_conversations.txt:**\nThis file connects those individual lines into conversations.\nEach row lists two characters, the movie they‚Äôre from, and the sequence of utterance IDs that form a full conversation:\n\n```\ncharacter1ID +++$+++ character2ID +++$+++ movieID +++$+++ [list of utterance IDs]\n```","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing:\n\nWe are extracting question-answer pairs from the above text files, which are in that format mentioned above.","metadata":{}},{"cell_type":"code","source":"# Setting maximum token length\nMAX_LEN = 64  # Controls the number of tokens per sentence (truncates longer ones)\n\n\n### loading all data into memory\ncorpus_movie_conv = './datasets/movie_conversations.txt'\ncorpus_movie_lines = './datasets/movie_lines.txt'\n\n## Reading the files\nwith open(corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n    conv = c.readlines()\n\nwith open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n    lines = l.readlines()\n\n\n### splitting text using special lines ('+++$+++' delimiter)\nlines_dic = {}\nfor line in lines:\n    objects = line.split(\" +++$+++ \")\n    lines_dic[objects[0]] = objects[-1]\n    \n\n### generate question answer pairs\npairs = []\nfor con in conv:\n    ids = eval(con.split(\" +++$+++ \")[-1])\n    for i in range(len(ids)):\n        qa_pairs = []\n\n        if i == len(ids) - 1:\n            break\n\n        first = lines_dic[ids[i]].strip()\n        second = lines_dic[ids[i+1]].strip()\n\n        qa_pairs.append(' '.join(first.split()[:MAX_LEN]))\n        qa_pairs.append(' '.join(second.split()[:MAX_LEN]))\n        pairs.append(qa_pairs)\n","metadata":{"id":"w2sfs5gQ9S-2","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:13.304390Z","iopub.execute_input":"2025-05-01T07:13:13.304801Z","iopub.status.idle":"2025-05-01T07:13:15.673287Z","shell.execute_reply.started":"2025-05-01T07:13:13.304774Z","shell.execute_reply":"2025-05-01T07:13:15.672645Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"## Checking a sample of the input.\npairs[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:15.674028Z","iopub.execute_input":"2025-05-01T07:13:15.674306Z","iopub.status.idle":"2025-05-01T07:13:15.679866Z","shell.execute_reply.started":"2025-05-01T07:13:15.674276Z","shell.execute_reply":"2025-05-01T07:13:15.679108Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['Can we make this quick? Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad. Again.',\n \"Well, I thought we'd start with pronunciation, if that's okay with you.\"]"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Importing the Tokenizer;\n\nThe goal of this section is to train a custom WordPiece tokenizer from scratch on our movie dialogue dataset.\nInstead of using a pre-trained tokenizer (like the default BERT tokenizer), we build one tailored to the vocabulary and linguistic patterns present in conversational data.\nThis helps:\n\nüß† Improve tokenization for informal, chat-style text (e.g., contractions, slang).\n\nüî§ Generate a vocabulary that better reflects the specific domain of movie conversations.\n\nüîÑ Enable training a BERT-style model from scratch or fine-tuning on custom vocab, ensuring consistency between tokenizer and model. \n\n\n\n\nWe are using the **BertWordPieceTokenizer** from the tokenizers library because....\n\n- This is a fast and efficient tokenizer designed for training WordPiece models like BERT from scratch. It's significantly faster than the HuggingFace transformers tokenizer and gives you fine-grained control over training parameters (e.g., vocab size, token frequencies).\n\n- These are its parameters and its purpose;\n\n| Parameter | Purpose |\n|----------|---------|\n| `clean_text=True` | Normalizes strange unicode, punctuation, and whitespace for consistency. |\n| `handle_chinese_chars=False` | Not relevant here, so we turn it off. |\n| `strip_accents=False` | Keeps accented characters (like caf√©), which can matter in dialogue. |\n| `lowercase=True` | Makes everything lowercase to reduce vocabulary size and noise. |\n| `special_tokens=[...]` | Adds necessary BERT tokens like `[PAD]`, `[CLS]`, `[SEP]`, etc. |\n\n\n\n**tokenizer.train(...)**\n\nThis line trains the tokenizer on all the collected .txt files. We specify:\n\n- vocab_size=30_000: A good size for English corpora ‚Äî small enough for efficiency but large enough to capture most frequent words/subwords.\n\n- min_frequency=5: Removes rare noise-like tokens.\n\n- limit_alphabet=1000: Caps the character set used to construct the vocabulary.\n\n- special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']. These tokens serve structural purposes: <ul><li>[PAD] ‚Äì padding</li><li>[CLS] ‚Äì classification tasks</li><li>[SEP] ‚Äì sentence separation</li><li>[MASK] ‚Äì masking for MLM</li><li>[UNK] ‚Äì unknown tokens</li></ul>\n\nThese are predefined tokens needed for training BERT.","metadata":{"id":"REAmGnBqB7qr"}},{"cell_type":"code","source":"# WordPiece tokenizer\n\n### save data as txt file\nos.mkdir('./data')\ntext_data = []\nfile_count = 0\n\nfor sample in tqdm.tqdm([x[0] for x in pairs]):\n    text_data.append(sample)\n\n    # once we hit the 10K mark, save to file\n    if len(text_data) == 10000:\n        with open(f'./data/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n            fp.write('\\n'.join(text_data))\n        text_data = []\n        file_count += 1\n\npaths = [str(x) for x in Path('./data').glob('**/*.txt')]\n\n### training own tokenizer\ntokenizer = BertWordPieceTokenizer(\n    clean_text=True,\n    handle_chinese_chars=False,\n    strip_accents=False,\n    lowercase=True\n)\n\n\n## We pass in the file path, vocab size, \ntokenizer.train(\n    files=paths,\n    vocab_size=30_000,\n    min_frequency=5,\n    limit_alphabet=1000,\n    wordpieces_prefix='##',\n    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n    )\n\nos.mkdir('./bert-it-1')\n\n## Saving the Tokenizer;\ntokenizer.save_model('./bert-it-1', 'bert-it')\n\n## Loading it;\ntokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)","metadata":{"id":"_gYE8zddGYdy","outputId":"30d0d810-45e9-47a9-d46b-1ac01a77bae2","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:15.681742Z","iopub.execute_input":"2025-05-01T07:13:15.681970Z","iopub.status.idle":"2025-05-01T07:13:21.211769Z","shell.execute_reply.started":"2025-05-01T07:13:15.681953Z","shell.execute_reply":"2025-05-01T07:13:21.211188Z"}},"outputs":[{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 221616/221616 [00:00<00:00, 1995296.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1962: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"tokenizer","metadata":{"id":"MPjG8-ZTGYf6","outputId":"e78bb6de-d3bd-475c-bbf5-1d77770b1f00","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.213122Z","iopub.execute_input":"2025-05-01T07:13:21.213535Z","iopub.status.idle":"2025-05-01T07:13:21.218275Z","shell.execute_reply.started":"2025-05-01T07:13:21.213515Z","shell.execute_reply":"2025-05-01T07:13:21.217472Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"BertTokenizer(name_or_path='./bert-it-1/bert-it-vocab.txt', vocab_size=21160, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t4: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"## Example of a word tokenised into two word - this is because of our tokeniser not only identifies words\n## as tokens, but rather tries to break down a word into subwords and tokenises these subwords also;\n\n\nsample_token = 'normalized'\nprint(f\"The token id for the word {sample_token} in the vocabulary is \\n{tokenizer(sample_token)['input_ids']}\")\n\n## To view the \nid_token_mapping = {j:i for i,j in dict(tokenizer.vocab).items()}\n\nprint(id_token_mapping[2609])  ### token id of \"normal\"\nprint(id_token_mapping[1808])  ### token id of \"##ized\"\n","metadata":{"id":"9XX_lzFhB--B","outputId":"838e4d4a-def6-4aab-9df7-f590a0451595","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.219137Z","iopub.execute_input":"2025-05-01T07:13:21.219382Z","iopub.status.idle":"2025-05-01T07:13:21.238118Z","shell.execute_reply.started":"2025-05-01T07:13:21.219355Z","shell.execute_reply":"2025-05-01T07:13:21.237548Z"}},"outputs":[{"name":"stdout","text":"The token id for the word normalized in the vocabulary is \n[1, 2609, 1808, 2]\nnormal\n##ized\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":" # Building the BERT Dataset Class\n\n\nTo train BERT from scratch, we need to feed it data in the format it expects ‚Äî this includes:\n\n- **Masked Language Modeling (MLM):**\n\nBERT is trained to predict missing words in a sentence, which helps it learn deep bidirectional representations of language.\n\n```\n- We randomly select 15% of the tokens in the input for possible masking.\n\n- But if we always replace them with [MASK], the model learns to rely too much on that specific token ‚Äî which never appears during fine-tuning or inference.\n\n- To solve this, BERT applies the following masking strategy:\n\n* 80% of the selected tokens are replaced with [MASK].\n\n* 10% are replaced with a random token.\n\n* 10% are left unchanged (even though we still ask the model to predict them).\n\n```\n\nThis way, the model learns to make predictions even when no explicit clue (like [MASK]) is present in the input ‚Äî making it more robust.\n\n- **Next Sentence Prediction (NSP):** \n\nThe NSP task helps BERT understand sentence-level relationships, such as question-answer pairs or sequential dialogue.\n\n```\n- For each training example, BERT is fed a pair of sentences.\n\n- 50% of the time, the second sentence is the actual next sentence.\n\n- 50% of the time, it is a randomly picked sentence from the dataset.\n\n- The model is trained to predict whether the second sentence follows the first.\n```\n\nThis binary classification task improves the model‚Äôs ability to capture context across sentences ‚Äî crucial for downstream tasks like QA, natural language inference, and conversation modeling.\n","metadata":{"id":"kXLk8yg3B_ST"}},{"cell_type":"markdown","source":"This custom BERTDataset class helps us prepare sentence pairs in this exact format.\n\n\n### ‚úÖ What This Class Does:\n\n1. Loads a list of sentence pairs.\n\n2. Randomly decides if the second sentence should be the real next sentence or a randomly selected one.\n\n3. Applies word masking to 15% of the tokens as required for MLM.\n\n4. Adds [CLS], [SEP], and [PAD] tokens to structure the input properly.\n\n5. Assigns segment labels to distinguish sentence 1 from sentence 2.\n\n6. Returns a dictionary containing everything BERT needs: **input IDs, token labels, segment labels, and NSP labels.**","metadata":{}},{"cell_type":"code","source":"class BERTDataset(Dataset):\n\n    ## data_pair is the list of pair of sentences;\n    def __init__(self, data_pair, tokenizer, seq_len=64):\n\n        self.tokenizer = tokenizer\n        self.seq_len = seq_len\n        self.corpus_lines = len(data_pair)\n        self.lines = data_pair\n\n    def __len__(self):\n        return self.corpus_lines\n\n\n    def get_corpus_line(self, item):\n        '''return sentence pair'''\n        return self.lines[item][0], self.lines[item][1]\n\n    def get_random_line(self):\n        '''return random single sentence'''\n        return self.lines[random.randrange(len(self.lines))][1]\n\n    def get_sent(self, index):\n        '''return random sentence pair'''\n        t1, t2 = self.get_corpus_line(index)\n\n        # negative or positive pair, for next sentence prediction\n        if random.random() > 0.5:\n            return t1, t2, 1\n        else:\n            return t1, self.get_random_line(), 0\n\n    def random_word(self, sentence):\n\n        ## Split the sentence into words;\n        tokens = sentence.split()\n        output_label = []  ### Whichever positions have MLM applied upon, there we pass in the token id, else 0\n        output = []  ###\n\n        # 15% of the tokens would be replaced\n        for i, token in enumerate(tokens):\n\n            ## Generate a probability;\n            prob = random.random()\n\n            # remove cls and sep token\n            token_id = self.tokenizer(token)['input_ids'][1:-1]   ### tokenizer(\"normalized\")['input_ids']  ## [2609, 1808]\n\n            if prob < 0.15:\n                prob /= 0.15  ### 0.5 /\n\n                # 80% chance change token to mask token\n                if prob < 0.8:\n                    for i in range(len(token_id)): ### Some words could be tokenised into multiple sub-words\n                        output.append(self.tokenizer.vocab['[MASK]'])\n\n                # 10% chance change token to random token\n                elif prob < 0.9:\n                    for i in range(len(token_id)): ### Some words could be tokenised into multiple sub-words\n                        output.append(random.randrange(len(self.tokenizer.vocab)))\n\n                # 10% chance change token to current token\n                else:\n                    output.append(token_id)\n\n                output_label.append(token_id) ### Some words could be tokenised into multiple sub-words, hence in such case [] will be appended to output_label.\n\n            else:\n                output.append(token_id)\n\n                for i in range(len(token_id)): ### Some words could be tokenised into multiple sub-words\n                    output_label.append(0)\n\n        # flattening\n        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n        assert len(output) == len(output_label)\n        return output, output_label\n\n    def __getitem__(self, item):  #### item is an index\n\n        # Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\n        t1, t2, is_next_label = self.get_sent(item)\n\n        # Step 2: replace random words in sentence with mask / random words\n        t1_random, t1_label = self.random_word(t1)\n        t2_random, t2_label = self.random_word(t2)\n\n        # Step 3: Adding CLS and SEP tokens to the start and end of sentences\n         # Adding PAD token for labels\n        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n\n        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n\n        # Step 4: combine sentence 1 and 2 as one input\n        # adding PAD tokens to make the sentence same length as seq_len\n        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n\n        bert_input = (t1 + t2)[:self.seq_len]\n        bert_label = (t1_label + t2_label)[:self.seq_len]\n\n        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n\n        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)  ### Adding padding at the end fullfil the sequence length\n\n        output = {\"bert_input\": bert_input,\n                  \"bert_label\": bert_label,\n                  \"segment_label\": segment_label,   ### Used to separate the two sentence - first sentence's tokens is marked as 1, the other is 2\n                  \"is_next\": is_next_label}\n\n        return {key: torch.tensor(value) for key, value in output.items()}\n","metadata":{"id":"H8v7nN8rAxCA","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.238970Z","iopub.execute_input":"2025-05-01T07:13:21.239145Z","iopub.status.idle":"2025-05-01T07:13:21.253279Z","shell.execute_reply.started":"2025-05-01T07:13:21.239125Z","shell.execute_reply":"2025-05-01T07:13:21.252614Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_data = BERTDataset(\n   pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n\n\n## Data Loader always requires a __getitem__() attribute\ntrain_loader = DataLoader(\n   train_data, batch_size=32, shuffle=True, pin_memory=True)\n","metadata":{"id":"Zq1klUK1AxEZ","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.254319Z","iopub.execute_input":"2025-05-01T07:13:21.254580Z","iopub.status.idle":"2025-05-01T07:13:21.268660Z","shell.execute_reply.started":"2025-05-01T07:13:21.254563Z","shell.execute_reply":"2025-05-01T07:13:21.268031Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"## Viewing a sample;\nsample_data = next(iter(train_loader))\n","metadata":{"id":"txLjIWSrZk_e","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.269425Z","iopub.execute_input":"2025-05-01T07:13:21.269663Z","iopub.status.idle":"2025-05-01T07:13:21.671425Z","shell.execute_reply.started":"2025-05-01T07:13:21.269643Z","shell.execute_reply":"2025-05-01T07:13:21.670824Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"sample_data","metadata":{"id":"q8jh3ZF1kFyq","outputId":"610f6cf4-9e7d-48a6-84aa-4a550e7cf105","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.672149Z","iopub.execute_input":"2025-05-01T07:13:21.672381Z","iopub.status.idle":"2025-05-01T07:13:21.690738Z","shell.execute_reply.started":"2025-05-01T07:13:21.672365Z","shell.execute_reply":"2025-05-01T07:13:21.690223Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'bert_input': tensor([[   1,   48, 1809,  ...,    0,    0,    0],\n         [   1,  162,   11,  ...,    0,    0,    0],\n         [   1,  314,   17,  ...,    0,    0,    0],\n         ...,\n         [   1,  182,  187,  ...,    0,    0,    0],\n         [   1,  303,   15,  ...,    2,    0,    0],\n         [   1, 2352,  435,  ...,    0,    0,    0]]),\n 'bert_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         ...,\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0]]),\n 'segment_label': tensor([[1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         ...,\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 2, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0]]),\n 'is_next': tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n         0, 1, 0, 1, 1, 0, 0, 0])}"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# The BERT Embedding\n\n\nIn BERT, each token is represented not just by its identity (word) but also by its position in the sentence and which sentence it belongs to.\n\nSo, instead of feeding plain word embeddings to the model, we enrich each token representation with:\n\n- Token Embedding ‚Äì what the word is.\n\n- Positional Embedding ‚Äì where it appears in the sentence.\n\n- Segment Embedding ‚Äì which sentence it belongs to (important for Next Sentence Prediction).\n\nThese three vectors are summed together for each token before being passed to the transformer.","metadata":{}},{"cell_type":"code","source":"class PositionalEmbedding(torch.nn.Module):\n\n    def __init__(self, d_model, max_len=128):\n        super().__init__()\n\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model).float()\n        pe.require_grad = False\n\n        for pos in range(max_len):\n            # for each dimension of the each position\n            for i in range(0, d_model, 2):\n                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n\n        # include the batch size\n        self.pe = pe.unsqueeze(0)\n        # self.register_buffer('pe', pe)\n\n    def forward(self, x):   ### Doubt here\n        return self.pe\n\nclass BERTEmbedding(torch.nn.Module):\n    \"\"\"\n    BERT Embedding which is consisted with under features\n        1. TokenEmbedding : normal embedding matrix\n        2. PositionalEmbedding : adding positional information using sin, cos\n        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n        sum of all these features are output of BERTEmbedding\n    \"\"\"\n\n    def __init__(self, vocab_size, embed_size, seq_len=64, dropout=0.1):\n        \"\"\"\n        :param vocab_size: total vocab size\n        :param embed_size: embedding size of token embedding\n        :param dropout: dropout rate\n        \"\"\"\n\n        super().__init__()\n        self.embed_size = embed_size\n        # (m, seq_len) --> (m, seq_len, embed_size)\n        # padding_idx is not updated during training, remains as fixed pad (0)\n        self.token = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0)\n        self.segment = torch.nn.Embedding(3, embed_size, padding_idx=0)\n        self.position = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n        self.dropout = torch.nn.Dropout(p=dropout)\n\n    def forward(self, sequence, segment_label):\n        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n        return self.dropout(x)\n","metadata":{"id":"ZCFi7DYqvXa_","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.691466Z","iopub.execute_input":"2025-05-01T07:13:21.691686Z","iopub.status.idle":"2025-05-01T07:13:21.698848Z","shell.execute_reply.started":"2025-05-01T07:13:21.691668Z","shell.execute_reply":"2025-05-01T07:13:21.698217Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# The Multi-Head Attention Layer;","metadata":{}},{"cell_type":"markdown","source":"## Purpose: What is Multi-Head Attention?\n\nMulti-head attention allows the model to focus on different parts of a sentence in parallel, capturing diverse relationships between words (e.g., ‚Äúthe cat‚Äù and ‚Äúsat on the mat‚Äù). Instead of one attention mechanism, we compute multiple attention heads and then combine them.\n\n\n## üî¢ Core Math & Intuition:\n\n**1. Linear Projections (Q, K, V)**\nInput shape: (batch_size, seq_len, d_model)\n\nWe project the input into:\n\n```\n- Query: What am I looking for?\n\n- Key: What content do I have?\n\n- Value: What do I return if I attend here?\n```\n\nThese are learned linear layers:\n\n```\nself.query = torch.nn.Linear(d_model, d_model)\nself.key = torch.nn.Linear(d_model, d_model)\nself.value = torch.nn.Linear(d_model, d_model)\n```\n\n\n**2. Split into Multiple Heads**\n\nEach head has dimension d_k = d_model / heads.\n\n```\nquery.view(..., heads, d_k).permute(...)\n```\n\nNow we compute attention in parallel for different parts of the representation space.\n\n**3. Scaled Dot-Product Attention**\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V$$\n\nThis gives attention weights that score how much each token attends to others.\n\n- **QK^T** gives similarity between tokens.\n\n- **softmax** ensures all attention scores sum to 1.\n\n- **mask** hides padding or unwanted tokens.\n\n\n**4. Concat Heads and Final Linear Projection**\n\nAfter computing attention for each head, we concatenate them and apply a final linear transformation:\n\n","metadata":{}},{"cell_type":"code","source":"### attention layers\nclass MultiHeadedAttention(torch.nn.Module):\n\n    def __init__(self, heads, d_model, dropout=0.1):\n        super(MultiHeadedAttention, self).__init__()\n\n        assert d_model % heads == 0\n        self.d_k = d_model // heads\n        self.heads = heads\n        self.dropout = torch.nn.Dropout(dropout)\n\n        self.query = torch.nn.Linear(d_model, d_model)\n        self.key = torch.nn.Linear(d_model, d_model)\n        self.value = torch.nn.Linear(d_model, d_model)\n        self.output_linear = torch.nn.Linear(d_model, d_model)\n\n    def forward(self, query, key, value, mask):\n        \"\"\"\n        query, key, value of shape: (batch_size, max_len, d_model)\n        mask of shape: (batch_size, 1, 1, max_words)\n        \"\"\"\n        # (batch_size, max_len, d_model)\n        query = self.query(query)\n        key = self.key(key)\n        value = self.value(value)\n\n        # (batch_size, max_len, d_model) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n\n        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n\n        # fill 0 mask with super small number so it wont affect the softmax weight\n        # (batch_size, h, max_len, max_len)\n        scores = scores.masked_fill(mask == 0, -1e9)  ### mask ---> (batch_size, 1, max_len, max_len), scores ---> (batch_size, h, max_len, max_len)\n        ### mask == 0 marks True in places where condition is satisfied, so wherever True, it will be populated as 1e-9, meaning to ignore that index/token.\n\n\n        # (batch_size, h, max_len, max_len)\n        # softmax to put attention weight for all non-pad tokens\n        # max_len X max_len matrix of attention\n        weights = F.softmax(scores, dim=-1)\n        weights = self.dropout(weights)\n\n        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n        context = torch.matmul(weights, value)\n\n        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, d_model)\n        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n\n        # (batch_size, max_len, d_model)\n        return self.output_linear(context)","metadata":{"id":"FDPOBlo_oEkI","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.699604Z","iopub.execute_input":"2025-05-01T07:13:21.699851Z","iopub.status.idle":"2025-05-01T07:13:21.714753Z","shell.execute_reply.started":"2025-05-01T07:13:21.699830Z","shell.execute_reply":"2025-05-01T07:13:21.714095Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# The Feed-Forward Network Layer;\n\n## Why is the Feed-Forward Network (FFN) needed?\n\n- With FFN: The Transformer model becomes more expressive and powerful, capable of learning **complex, non-linear relationships** (bacause of non-linear transformations). It can process the data in deeper and more nuanced ways, leading to better performance on most tasks.\n\n- Without FFN: The Transformer model would be much **simpler and less powerful**, as it would lack the non-linear transformations and feature refinement that the FFN provides. While it might still perform adequately on simpler tasks, its overall capacity to handle complex patterns and abstractions would be greatly diminished.\n\n## üî¢ Core Math & Intuition:\n\n1. Linear Layers (fc1 and fc2):\n\n- fc1 takes input with shape (d_model) and transforms it into a higher-dimensional space with shape (middle_dim). This is a typical strategy to allow the model to learn more complex representations.\n\n- fc2 then takes the result of fc1 and projects it back to the original size, (d_model). This creates a \"bottleneck\" structure where the input is expanded and then reduced, enabling the network to learn complex non-linear transformations in a lower-dimensional space.\n\n2. Activation Function (GELU):\n\n- After the first linear transformation (fc1), the output goes through a non-linear activation function, GELU (Gaussian Error Linear Unit). This activation function helps the model learn non-linear relationships, which is important for capturing complex patterns.\n\n- GELU is often preferred in Transformer models over ReLU due to its smoother gradient behavior, which helps with training stability.\n\n| Property                         | **GELU**                                          | **ReLU**                                         |\n|-----------------------------------|---------------------------------------------------|--------------------------------------------------|\n| **Formula**                       | $\\text{GELU}(x) = 0.5 \\cdot x \\left( 1 + \\tanh \\left( \\sqrt{\\frac{2}{\\pi}} \\cdot \\left( x + 0.044715 x^3 \\right) \\right) \\right)$ | $\\text{ReLU}(x) = \\max(0, x)$ |\n| **Handling Negative Inputs**      | Small negative values are allowed, output is smooth and non-zero for negative inputs | Negative values are zeroed out (no gradient) |\n| **Differentiability**             | Smooth and differentiable everywhere              | Not differentiable at \\(x = 0\\)                 |\n| **Gradient Behavior**             | Provides stable gradients even for negative values | Zero gradient for negative inputs               |\n| **Computational Complexity**      | Slightly more complex due to the non-linear components | Simple, fast to compute                        |\n| **Usage in Transformers**         | Preferred in transformer models (BERT, GPT, etc.)  | Less commonly used in transformers              |\n\n3. Dropout:\n\n- The Dropout layer is used to prevent the model from overfitting by randomly setting some of the activations to zero during training. This helps improve generalization by forcing the model to not rely too heavily on any specific activation.","metadata":{}},{"cell_type":"code","source":"# self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)  ## Called from EncoderLayer\n\nclass FeedForward(torch.nn.Module):\n    \"Implements FFN equation.\"\n\n    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n        super(FeedForward, self).__init__()\n\n        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.activation = torch.nn.GELU()\n\n    def forward(self, x):\n        out = self.activation(self.fc1(x))\n        out = self.fc2(self.dropout(out))\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.717024Z","iopub.execute_input":"2025-05-01T07:13:21.717218Z","iopub.status.idle":"2025-05-01T07:13:21.732825Z","shell.execute_reply.started":"2025-05-01T07:13:21.717205Z","shell.execute_reply":"2025-05-01T07:13:21.732309Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"The below Encoder Layer integrates the Multi-head Attention and FFN, also introducing skip-connections (residual connections) as per the original architecture;","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(torch.nn.Module):\n    def __init__(\n        self,\n        d_model=768,\n        heads=12,\n        feed_forward_hidden=768 * 4,\n        dropout=0.1\n        ):\n        super(EncoderLayer, self).__init__()\n\n        self.layernorm = torch.nn.LayerNorm(d_model)\n\n        self.self_multihead = MultiHeadedAttention(heads, d_model)  ### defines the k, q, v,\n\n        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)  ## Normal NN\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, embeddings, mask):\n        # embeddings: (batch_size, max_len, d_model)\n        # encoder mask: (batch_size, 1, 1, max_len)\n        # result: (batch_size, max_len, d_model)\n\n        ## def forward(self, query, key, value, mask):  ## masking the 0 with -1e9, applying dropout, calculating the softmax on a head level, then reshaping\n        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n\n        # residual layer\n        interacted = self.layernorm(interacted + embeddings)\n\n        # bottleneck - includes the skip connection step.\n        feed_forward_out = self.dropout(self.feed_forward(interacted))\n        encoded = self.layernorm(feed_forward_out + interacted)\n\n        return encoded\n","metadata":{"id":"_8kr9PEuoEmX","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.733455Z","iopub.execute_input":"2025-05-01T07:13:21.733684Z","iopub.status.idle":"2025-05-01T07:13:21.748587Z","shell.execute_reply.started":"2025-05-01T07:13:21.733669Z","shell.execute_reply":"2025-05-01T07:13:21.747836Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Putting It All Together: Encoding the Input\n\nNow we connect all the key components that make BERT capable of understanding context-rich input.\n\n\n1. \n```\nself.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=d_model)\n\n```\n\nCreates the **embedding layer**, which converts input token indices into dense vector representations. This includes:\n\nToken embeddings\n\nPositional embeddings (to give order information)\n\nSegment embeddings (to distinguish sentence A vs B)\n\n\n\n2. \n```\nself.encoder_blocks = torch.nn.ModuleList(\n    [EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n```\n\nCreates a list of EncoderLayer blocks (Transformer encoder layers) ‚Äî 12 by default. Each one contains:\n\n- Multi-head self-attention\n\n- Feed-forward network\n\n- Residual + normalization\n\nThey're stored in a **ModuleList** so PyTorch can register them as trainable layers.\n\n\n3. \n```\nmask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n\n```\n\nCreates an **attention mask to ignore padding tokens** (0s). This mask will be used in attention layers to ensure the model doesn‚Äôt attend to padding positions.\n\n\n4. \n```\nfor encoder in self.encoder_blocks:\n    x = encoder.forward(x, mask)\n\n```\n\nRuns the input through each transformer encoder layer, one by one. Each layer applies **self-attention + feed-forward + skip connections + normalization**.","metadata":{}},{"cell_type":"code","source":"\nclass BERT(torch.nn.Module):\n    \"\"\"\n    BERT model : Bidirectional Encoder Representations from Transformers.\n    \"\"\"\n\n    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n        \"\"\"\n        :param vocab_size: vocab_size of total words\n        :param hidden: BERT model hidden size\n        :param n_layers: numbers of Transformer blocks(layers)\n        :param attn_heads: number of attention heads\n        :param dropout: dropout rate\n        \"\"\"\n\n        super().__init__()\n\n        self.d_model = d_model\n        self.n_layers = n_layers\n        self.heads = heads\n\n        # paper noted they used 4 * hidden_size for ff_network_hidden_size\n        self.feed_forward_hidden = d_model * 4\n\n        # embedding for BERT, sum of positional, segment, token embeddings\n        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=d_model)   ### sum of TokenEmbedding, PositionalEmbedding, SegmentEmbedding\n\n        # multi-layers transformer blocks, deep network\n        self.encoder_blocks = torch.nn.ModuleList(\n            [EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])  ## Creating layers, and we will sequentially compute while applying a loop\n\n\n\n    def forward(self, x, segment_info):   #### x --> (batch_size, seq_len), segment_info --> (batch_size, seq_len)  ### data[\"bert_input\"], data[\"segment_label\"]\n        # attention masking for padded token\n        # (batch_size, 1, seq_len, seq_len)\n\n        # ‚úÖ \"Hey, attend only to real tokens!\"\n        # ‚ùå \"Ignore padded tokens!\"\n        # [[1, 1, 1, 1, 1, 1, 0, 0, 0, ..., 0]]\n\n        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n\n        # embedding the indexed sequence to sequence of vectors\n        x = self.embedding(x, segment_info)   #### sum of TokenEmbedding, PositionalEmbedding, SegmentEmbedding\n\n        # running over multiple transformer blocks\n        for encoder in self.encoder_blocks:\n            x = encoder.forward(x, mask)  ### Applies the (Transformer Computation (Attention calculation, transformation) + FFN + skip connections) * 12 times\n\n        return x\n\n","metadata":{"id":"UQguINzYoEof","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.749339Z","iopub.execute_input":"2025-05-01T07:13:21.749520Z","iopub.status.idle":"2025-05-01T07:13:21.763670Z","shell.execute_reply.started":"2025-05-01T07:13:21.749506Z","shell.execute_reply":"2025-05-01T07:13:21.763066Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# BERT Pretraining Heads: Next Sentence & Masked Token Prediction\n\n### üß© 1. NextSentencePrediction Class\n\nThis is a binary classifier used in BERT‚Äôs pretraining phase.\nIt predicts whether sentence B logically follows sentence A.\n\n- Takes in the BERT output (specifically the [CLS] token).\n\n- Passes it through a linear layer ‚Üí 2 output scores: is_next or not_next.\n\n- Applies log-softmax to return log-probabilities.\n\nüìå Used to help BERT understand sentence-level relationships.\n\n### üß© 2. MaskedLanguageModel Class\n\nThis is a multi-class classifier for predicting the original token in place of a masked one.\n\n- Takes the full output of the BERT model (all tokens).\n\n- Applies a linear layer to map hidden states to vocabulary size.\n\n- Uses log-softmax for each token‚Äôs prediction.\n\nüìå Helps BERT learn deep word-level understanding during pretraining by predicting missing words.","metadata":{}},{"cell_type":"code","source":"class NextSentencePrediction(torch.nn.Module):\n    \"\"\"\n    2-class classification model : is_next, is_not_next\n    \"\"\"\n\n    def __init__(self, hidden):\n        \"\"\"\n        :param hidden: BERT model output size\n        \"\"\"\n        super().__init__()\n        self.linear = torch.nn.Linear(hidden, 2)\n        self.softmax = torch.nn.LogSoftmax(dim=-1)\n\n    def forward(self, x):\n        # use only the first token which is the [CLS]\n        return self.softmax(self.linear(x[:, 0]))\n\nclass MaskedLanguageModel(torch.nn.Module):\n    \"\"\"\n    predicting origin token from masked input sequence\n    n-class classification problem, n-class = vocab_size\n    \"\"\"\n\n    def __init__(self, hidden, vocab_size):\n        \"\"\"\n        :param hidden: output size of BERT model\n        :param vocab_size: total vocab size\n        \"\"\"\n        super().__init__()\n        self.linear = torch.nn.Linear(hidden, vocab_size)\n        self.softmax = torch.nn.LogSoftmax(dim=-1)\n\n    def forward(self, x):\n        return self.softmax(self.linear(x))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.764512Z","iopub.execute_input":"2025-05-01T07:13:21.764750Z","iopub.status.idle":"2025-05-01T07:13:21.777531Z","shell.execute_reply.started":"2025-05-01T07:13:21.764730Z","shell.execute_reply":"2025-05-01T07:13:21.776707Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"This class wraps the full BERT model and connects it to both of its pretraining heads:\n\n1. Masked Language Modeling (MLM) ‚Äì Predicts masked tokens in the input sequence\n\n2. Next Sentence Prediction (NSP) ‚Äì Determines if one sentence follows another\n\n\nDuring the **forward pass**, the following steps are followed:\n\n- The input goes through the BERT encoder, generating contextual embeddings.\n\n- The [CLS] token is passed to the NSP head for sentence-level prediction.\n\n- The full sequence is passed to the MLM head for token prediction.","metadata":{}},{"cell_type":"code","source":"class BERTLM(torch.nn.Module):\n    \"\"\"\n    BERT Language Model\n    Next Sentence Prediction Model + Masked Language Model\n    \"\"\"\n\n    def __init__(self, bert: BERT, vocab_size):\n        \"\"\"\n        :param bert: BERT model which should be trained\n        :param vocab_size: total vocab size for masked_lm\n        \"\"\"\n\n        super().__init__()\n        self.bert = bert\n        self.next_sentence = NextSentencePrediction(self.bert.d_model)\n        self.mask_lm = MaskedLanguageModel(self.bert.d_model, vocab_size)\n\n    def forward(self, x, segment_label):\n        x = self.bert(x, segment_label)\n        return self.next_sentence(x), self.mask_lm(x)\n","metadata":{"id":"BolNR6iNoEqh","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.778408Z","iopub.execute_input":"2025-05-01T07:13:21.778586Z","iopub.status.idle":"2025-05-01T07:13:21.791413Z","shell.execute_reply.started":"2025-05-01T07:13:21.778572Z","shell.execute_reply":"2025-05-01T07:13:21.790833Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Custom Learning Rate Scheduler for BERT Pre-Training;\n\n\n### üöÄ **ScheduledOptim**: \n\nThis class wraps around a PyTorch optimizer (like Adam) and **controls the learning rate dynamically** during training, as suggested in the original Transformer paper.\n\n### ‚úÖ Purpose:\nInstead of using a fixed learning rate, this schedule:\n\n1. Warms up the learning rate for a few thousand steps (small ‚Üí large)\n\n2. Then decays it smoothly as training continues (large ‚Üí small)\n\nThis helps with stable and effective training of deep transformer models like BERT.","metadata":{}},{"cell_type":"code","source":"### Creating the optimiser;\n\nclass ScheduledOptim():\n    '''A simple wrapper class for learning rate scheduling'''\n\n    def __init__(self, optimizer, d_model, n_warmup_steps):\n        self._optimizer = optimizer\n        self.n_warmup_steps = n_warmup_steps\n        self.n_current_steps = 0\n        self.init_lr = np.power(d_model, -0.5)\n\n    def step_and_update_lr(self):\n        \"Step with the inner optimizer\"\n        self._update_learning_rate()\n        self._optimizer.step()\n\n    def zero_grad(self):\n        \"Zero out the gradients by the inner optimizer\"\n        self._optimizer.zero_grad()\n\n    def _get_lr_scale(self):\n        return np.min([\n            np.power(self.n_current_steps, -0.5),\n            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n\n    def _update_learning_rate(self):\n        ''' Learning rate scheduling per step '''\n\n        self.n_current_steps += 1\n        lr = self.init_lr * self._get_lr_scale()\n\n        for param_group in self._optimizer.param_groups:\n            param_group['lr'] = lr","metadata":{"id":"EOrHAaJzbJEv","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.792023Z","iopub.execute_input":"2025-05-01T07:13:21.792273Z","iopub.status.idle":"2025-05-01T07:13:21.803974Z","shell.execute_reply.started":"2025-05-01T07:13:21.792257Z","shell.execute_reply":"2025-05-01T07:13:21.803366Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Finally Pre-Training Begins!!","metadata":{}},{"cell_type":"code","source":"### Integrating all the above in this trainer class;\n\n# The BERTTrainer class manages the full training loop for the BERT model, \n# including optimization, loss calculation, and evaluation.\n\n\n## We are using the Negative Log Likelihood Loss function as the objective to minimize.\n\nclass BERTTrainer:\n\n    def __init__(\n        self,\n        model,\n        train_dataloader,\n        test_dataloader=None,\n        lr= 1e-4,\n        weight_decay=0.01,\n        betas=(0.9, 0.999),\n        warmup_steps=10000,\n        log_freq=10,\n        device='cuda'\n        ):\n\n        self.device = device\n        self.model = model\n        self.train_data = train_dataloader\n        self.test_data = test_dataloader\n\n        # Setting the Adam optimizer with hyper-param\n        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n        self.optim_schedule = ScheduledOptim(\n            self.optim, self.model.bert.d_model, n_warmup_steps=warmup_steps\n            )\n\n\n        # Using Negative Log Likelihood Loss function for predicting the masked_token\n        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n        self.log_freq = log_freq\n        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n\n\n    def iteration(self, epoch, data_loader, train=True):\n\n        avg_loss = 0.0\n        total_correct = 0\n        total_element = 0\n\n        mode = \"train\" if train else \"test\"\n\n        # progress bar\n        data_iter = tqdm.tqdm(\n            enumerate(data_loader),\n            desc=\"EP_%s:%d\" % (mode, epoch),\n            total=len(data_loader),\n            bar_format=\"{l_bar}{r_bar}\"\n        )\n\n        for i, data in data_iter:\n\n            # 0. batch_data will be sent into the device(GPU or cpu)\n            data = {key: value.to(self.device) for key, value in data.items()}\n\n\n            # 1. forward the next_sentence_prediction and masked_lm model\n            next_sent_output, mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"segment_label\"])\n\n\n            # 2-1. NLL(negative log likelihood) loss of is_next classification result\n            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n\n            # 2-2. NLLLoss of predicting masked token word\n            # transpose to (m, vocab_size, seq_len) vs (m, seq_len)\n            # criterion(mask_lm_output.view(-1, mask_lm_output.size(-1)), data[\"bert_label\"].view(-1))\n            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n\n            # 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n            loss = next_loss + mask_loss\n\n            # 3. backward and optimization only in train\n            if train:\n                self.optim_schedule.zero_grad()\n                loss.backward()\n                self.optim_schedule.step_and_update_lr()\n\n            # next sentence prediction accuracy\n            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n            avg_loss += loss.item()\n            total_correct += correct\n            total_element += data[\"is_next\"].nelement()\n\n            post_fix = {\n                \"epoch\": epoch,\n                \"iter\": i,\n                \"avg_loss\": avg_loss / (i + 1),\n                \"avg_acc\": total_correct / total_element * 100,\n                \"loss\": loss.item()\n            }\n\n            if i % self.log_freq == 0:\n                data_iter.write(str(post_fix))\n        print(\n            f\"EP{epoch}, {mode}: \\\n            avg_loss={avg_loss / len(data_iter)}, \\\n            total_acc={total_correct * 100.0 / total_element}\"\n        )\n\n\n    def train(self, epoch):\n        self.iteration(epoch, self.train_data)\n\n    def test(self, epoch):\n        self.iteration(epoch, self.test_data, train=False)\n","metadata":{"id":"ukSEWw3JbJG_","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.804667Z","iopub.execute_input":"2025-05-01T07:13:21.804828Z","iopub.status.idle":"2025-05-01T07:13:21.820547Z","shell.execute_reply.started":"2025-05-01T07:13:21.804815Z","shell.execute_reply":"2025-05-01T07:13:21.819847Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"%%time\n\ntrain_data = BERTDataset(\n   pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n\ntrain_loader = DataLoader(\n   train_data, batch_size=32, shuffle=True, pin_memory=True)\n\nbert_model = BERT(\n  vocab_size=len(tokenizer.vocab),\n  d_model=768,\n  n_layers=2,\n  heads=12,\n  dropout=0.1\n)\n\nbert_lm = BERTLM(bert_model, len(tokenizer.vocab))\n\n\nbert_trainer = BERTTrainer(bert_lm, train_loader, device='cpu')\n\nepochs = 1  ## as a trial, I have taken epoch as 1, can be tweaked by the reader.\n\nfor epoch in range(epochs):\n  bert_trainer.train(epoch)","metadata":{"id":"7VnSpboJdi04","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:13:21.821239Z","iopub.execute_input":"2025-05-01T07:13:21.821448Z"}},"outputs":[{"name":"stdout","text":"Total Parameters: 46699434\n","output_type":"stream"},{"name":"stderr","text":"EP_train:0:   0%|| 1/6926 [00:03<6:14:23,  3.24s/it]","output_type":"stream"},{"name":"stdout","text":"{'epoch': 0, 'iter': 0, 'avg_loss': 10.940971374511719, 'avg_acc': 56.25, 'loss': 10.940971374511719}\n","output_type":"stream"},{"name":"stderr","text":"EP_train:0:   0%|| 11/6926 [00:31<5:27:08,  2.84s/it]","output_type":"stream"},{"name":"stdout","text":"{'epoch': 0, 'iter': 10, 'avg_loss': 10.830794247713955, 'avg_acc': 50.85227272727273, 'loss': 10.835719108581543}\n","output_type":"stream"},{"name":"stderr","text":"EP_train:0:   0%|| 21/6926 [00:58<5:22:27,  2.80s/it]","output_type":"stream"},{"name":"stdout","text":"{'epoch': 0, 'iter': 20, 'avg_loss': 10.78608953385126, 'avg_acc': 50.44642857142857, 'loss': 10.730015754699707}\n","output_type":"stream"},{"name":"stderr","text":"EP_train:0:   0%|| 31/6926 [01:25<5:09:12,  2.69s/it]","output_type":"stream"},{"name":"stdout","text":"{'epoch': 0, 'iter': 30, 'avg_loss': 10.70475661370062, 'avg_acc': 49.29435483870967, 'loss': 10.442561149597168}\n","output_type":"stream"},{"name":"stderr","text":"EP_train:0:   1%|| 41/6926 [01:52<5:09:47,  2.70s/it]","output_type":"stream"},{"name":"stdout","text":"{'epoch': 0, 'iter': 40, 'avg_loss': 10.627633815858422, 'avg_acc': 48.6280487804878, 'loss': 10.39102840423584}\n","output_type":"stream"},{"name":"stderr","text":"EP_train:0:   1%|| 51/6926 [02:20<5:08:17,  2.69s/it]","output_type":"stream"},{"name":"stdout","text":"{'epoch': 0, 'iter': 50, 'avg_loss': 10.55413938036152, 'avg_acc': 49.1421568627451, 'loss': 10.196638107299805}\n","output_type":"stream"},{"name":"stderr","text":"EP_train:0:   1%|| 61/6926 [02:47<5:11:24,  2.72s/it]","output_type":"stream"},{"name":"stdout","text":"{'epoch': 0, 'iter': 60, 'avg_loss': 10.475267394644316, 'avg_acc': 48.46311475409836, 'loss': 10.047904968261719}\n","output_type":"stream"},{"name":"stderr","text":"EP_train:0:   1%|| 71/6926 [03:14<5:09:53,  2.71s/it]","output_type":"stream"},{"name":"stdout","text":"{'epoch': 0, 'iter': 70, 'avg_loss': 10.408695408995722, 'avg_acc': 48.723591549295776, 'loss': 9.909573554992676}\n","output_type":"stream"},{"name":"stderr","text":"EP_train:0:   1%|| 81/6926 [03:42<5:22:05,  2.82s/it]","output_type":"stream"},{"name":"stdout","text":"{'epoch': 0, 'iter': 80, 'avg_loss': 10.34888215712559, 'avg_acc': 48.53395061728395, 'loss': 9.952644348144531}\n","output_type":"stream"},{"name":"stderr","text":"EP_train:0:   1%|| 91/6926 [04:08<5:01:08,  2.64s/it]","output_type":"stream"},{"name":"stdout","text":"{'epoch': 0, 'iter': 90, 'avg_loss': 10.292921495961618, 'avg_acc': 49.107142857142854, 'loss': 9.7589693069458}\n","output_type":"stream"},{"name":"stderr","text":"EP_train:0:   1%|| 96/6926 [04:22<5:10:34,  2.73s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# üìò End Notes\n\n\nThanks for walking through this notebook on \"Demystifying BERT from Scratch\"!\n\nWe covered:\n\n- Core components of the BERT architecture\n\n- How attention and embeddings work\n\n- Pretraining objectives (MLM + NSP)\n\n- Training loop and optimization setup\n\n\n### This notebook aims to help you understand what's going on under the hood, beyond just calling transformers.BertModel.\n\n\n### **If you found this helpful, please give it an upvote üëç.**\n\n\n\n### **üí¨ Have feedback or questions? Feel free to leave a comment ‚Äî I'm happy to clarify or improve anything!**\n\n\n### **Thanks again, and happy learning! üöÄ**","metadata":{}}]}