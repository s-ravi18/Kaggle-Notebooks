{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 📚 Fine-Tuning Phi-2 for Text Summarization using PEFT\n\n\n- Large Language Models (LLMs) like Phi-2 have shown impressive capabilities in understanding and generating human-like text. However, fine-tuning these powerful models from scratch can be expensive and computationally intensive.\n\n\n- In this notebook, we'll explore how to fine-tune Phi-2 for text summarization using a lightweight and efficient technique called **PEFT** (**Parameter-Efficient Fine-Tuning**). Instead of updating all of a model’s parameters, PEFT allows us to adapt pre-trained models using a small number of additional parameters, significantly reducing training cost while achieving competitive performance.\n\n\n- We’ll walk through the full process—loading the model, preparing the dataset, applying PEFT via LoRA (Low-Rank Adaptation), training, and evaluating results. Whether you're new to LLM fine-tuning or looking for a practical example of PEFT in action, this notebook is designed to help you understand the how and why behind efficient fine-tuning.\n\n\nLet’s get started and make Phi-2 a summarizer!","metadata":{}},{"cell_type":"code","source":"## Installing the necessary dependencies;\n\n!pip install -qq -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:48:59.334985Z","iopub.execute_input":"2025-05-07T16:48:59.335506Z","iopub.status.idle":"2025-05-07T16:50:29.164155Z","shell.execute_reply.started":"2025-05-07T16:48:59.335483Z","shell.execute_reply":"2025-05-07T16:50:29.163465Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"### Importing the necessary libraries;\n\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nfrom huggingface_hub import login\n\nimport pandas as pd\nimport numpy as np\n\n\n## Using kaggle's feature to add the hugging face token so that it restricts public visibility. Please find\n## this feature available by clicking the \"Add-ons\" button on the menu bar of the editor.\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hf_token\")\n\n\n### This sort of mimics the .env file that we create during project creation.\nlogin(token=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:50:29.165811Z","iopub.execute_input":"2025-05-07T16:50:29.166069Z","iopub.status.idle":"2025-05-07T16:50:56.301885Z","shell.execute_reply.started":"2025-05-07T16:50:29.166049Z","shell.execute_reply":"2025-05-07T16:50:56.301301Z"}},"outputs":[{"name":"stderr","text":"2025-05-07 16:50:41.805674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746636641.992996      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746636642.043959      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"### Utility function to get the GPU memory used at any point of time\nfrom pynvml import *\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n\n\nprint_gpu_utilization()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:50:56.302576Z","iopub.execute_input":"2025-05-07T16:50:56.302797Z","iopub.status.idle":"2025-05-07T16:50:56.308227Z","shell.execute_reply.started":"2025-05-07T16:50:56.302779Z","shell.execute_reply":"2025-05-07T16:50:56.307716Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 116 MB.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# 📑 Dataset: CNN/DailyMail for Abstractive Summarization\n\n\n- To fine-tune our Phi-2 model for summarization, we're using the CNN/DailyMail dataset—one of the most popular benchmarks for training and evaluating summarization models.\n\n\n- This dataset consists of news articles from CNN and the Daily Mail, along with human-written highlights that serve as target summaries. It's well-suited for abstractive summarization, where the model learns to generate concise, paraphrased summaries instead of merely copying parts of the original text.\n\nWe'll use the **datasets** library to load the pre-processed version:","metadata":{}},{"cell_type":"code","source":"## Loading the dataset;\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"abisee/cnn_dailymail\", \"1.0.0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:50:56.310298Z","iopub.execute_input":"2025-05-07T16:50:56.310579Z","iopub.status.idle":"2025-05-07T16:51:06.590054Z","shell.execute_reply.started":"2025-05-07T16:50:56.310555Z","shell.execute_reply":"2025-05-07T16:51:06.589558Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1938e42ab8104c589c2200e754b1fa5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/256M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5bd5d46537349158a7ce51ebb612ae6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc644e2adda64b1785c2871f7243257e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e49cc072b354d1f9d3a6c6e1f727966"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1501979d881d42e8a76b5f2014e19696"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74800028c1a14d1bbb108d625463f050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"504645bd61954b78aee156ede146ce6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18bb01f470ab477a899983326d717d1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e13b006967244482a735fc69cf52ef09"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"## The dataset is already divided into three partitions, so we dont have to split the same for inferencing\n\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:51:06.590716Z","iopub.execute_input":"2025-05-07T16:51:06.590930Z","iopub.status.idle":"2025-05-07T16:51:06.595724Z","shell.execute_reply.started":"2025-05-07T16:51:06.590913Z","shell.execute_reply":"2025-05-07T16:51:06.595184Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 287113\n    })\n    validation: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 11490\n    })\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"## Viewing a sample observation;\n\ndataset['train']['article'][0], dataset['train']['highlights'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:51:06.596345Z","iopub.execute_input":"2025-05-07T16:51:06.596686Z","iopub.status.idle":"2025-05-07T16:51:09.304245Z","shell.execute_reply.started":"2025-05-07T16:51:06.596664Z","shell.execute_reply":"2025-05-07T16:51:09.303672Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.',\n \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday . Young actor says he has no plans to fritter his cash away . Radcliffe's earnings from first five Potter films have been held in trust fund .\")"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ⚙️ Loading Phi-2 with 4-bit Quantization (BitsAndBytes)\n\n- To **reduce memory usage** and make training feasible on limited hardware, we’re loading the Phi-2 model with **4-bit quantization** using the **bitsandbytes** library. This dramatically reduces the model's memory footprint while preserving much of its performance.","metadata":{}},{"cell_type":"code","source":"## Defining the bnb config/parameters, which defines the qunatisation settings for loading the model; \ncompute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\n\n\n## To place the model on the first available GPU\ndevice_map = {\"\": 0}\n\n\n## Loading the model;\nmodel_name='microsoft/phi-2'\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name, \n                                                      device_map=device_map,\n                                                      quantization_config=bnb_config,  ## the bnb config is passed as a parameter\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)\n\n\n## Printing the GPU Util;\nprint_gpu_utilization()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:51:09.305033Z","iopub.execute_input":"2025-05-07T16:51:09.305272Z","iopub.status.idle":"2025-05-07T16:51:37.387575Z","shell.execute_reply.started":"2025-05-07T16:51:09.305246Z","shell.execute_reply":"2025-05-07T16:51:37.386834Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"651e2085364f4ce797de354349a57c71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01b9b9370d5545e3be55476256fb27fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3cdfd3cc1ef478981825a80ef1fea62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0be2cbff1f2f4c2cbf3bee8658801257"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d93bdaf9f15448fa30c8a1aa915d4b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f67c8ac5e504e67bc151fb99fcee387"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea2edd6335314fb191f5973b48995fe7"}},"metadata":{}},{"name":"stdout","text":"GPU memory occupied: 3104 MB.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🔤 Tokenizer Setup\n\n\n- Before we can feed text into the Phi-2 model, we need to load its corresponding tokenizer. The tokenizer is responsible for converting raw text into input tokens that the model understands, and vice versa.\n\nWe’ll demonstrate two ways to load the tokenizer, depending on whether you’re training or evaluating.","metadata":{}},{"cell_type":"code","source":"### Tokeniser\n\nmodel_name='microsoft/phi-2'\n\n\n## Loading the tokenizer for trainer;\n\ntokenizer = AutoTokenizer.from_pretrained(model_name,\n                                          trust_remote_code=True,\n                                          padding_side=\"left\",\n                                          add_eos_token=True,\n                                          add_bos_token=True,\n                                          use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token\n\n\n## Printing the GPU Util;\nprint_gpu_utilization()\n\n\n## Loading the tokenizer for inferencing a sample record;\neval_tokenizer = AutoTokenizer.from_pretrained(model_name, \n                                               add_bos_token=True, \n                                               trust_remote_code=True, \n                                               use_fast=False)\n\neval_tokenizer.pad_token = eval_tokenizer.eos_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:51:37.388388Z","iopub.execute_input":"2025-05-07T16:51:37.388705Z","iopub.status.idle":"2025-05-07T16:51:39.173256Z","shell.execute_reply.started":"2025-05-07T16:51:37.388673Z","shell.execute_reply":"2025-05-07T16:51:39.172655Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ff50b744af845e8ae453e18d456afc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b664f2c59fac49059272f38211ac79da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26f2c24c5bbf4ca787926545e0fa7c19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b351cfd4e1444dfc86365ea8ba362fdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7488cb0c32504394a53416dcc9e57893"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"984d73c1b73a4d6882d827bd6b0a8442"}},"metadata":{}},{"name":"stdout","text":"GPU memory occupied: 3104 MB.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Looking at the above code block, you might wonder why I have defined the tokenizer object twice. While you might think they are the same at first glance, but infact they are slightly different. \n\n- **tokenizer**: This tokenizer is used during **training** and **generation**. Padding on the left helps with alignment during causal attention, and setting the pad token to the EOS token avoids errors related to undefined tokens.\n\n- **eval_tokenizer**:  This is used for evaluation purposes only, hence we skip setting **padding_side** and **add_eos_token** since evaluation usually doesn’t involve padding or sequence truncation in the same way as training.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:51:39.173934Z","iopub.execute_input":"2025-05-07T16:51:39.174112Z","iopub.status.idle":"2025-05-07T16:51:40.411693Z","shell.execute_reply.started":"2025-05-07T16:51:39.174098Z","shell.execute_reply":"2025-05-07T16:51:40.411137Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"- To easily test the vanilla model, we define a simple function gen() that takes in a prompt, feeds it to the model, and returns the generated text.","metadata":{}},{"cell_type":"code","source":"def gen(model, p, maxlen=100, sample=True):\n    \n    toks = eval_tokenizer(p, return_tensors=\"pt\")  # Tokenize the prompt into input IDs\n    \n    res = model.generate(\n        **toks.to(\"cuda\"),                     # Move tokens to GPU\n        max_new_tokens=maxlen,                # Limit generation length\n        do_sample=sample,                     # Enable sampling if True (for diversity)\n        num_return_sequences=1,               # Generate a single sequence\n        temperature=0.1,                      # Low temperature = more deterministic output\n        num_beams=1,                          # No beam search (greedy or sampling instead)\n        top_p=0.95                            # Nucleus sampling: sample from top 95% of token probability mass\n    ).to('cpu')                               # Move result back to CPU\n    \n    return eval_tokenizer.batch_decode(res, skip_special_tokens=True)  # Decode tokens into readable text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:51:40.413709Z","iopub.execute_input":"2025-05-07T16:51:40.414101Z","iopub.status.idle":"2025-05-07T16:51:40.418127Z","shell.execute_reply.started":"2025-05-07T16:51:40.414082Z","shell.execute_reply":"2025-05-07T16:51:40.417524Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from transformers import set_seed\nseed = 42\nset_seed(seed)\n\nindex = 12\n\narticle = dataset['train']['article'][index]\nsummary = dataset['train']['highlights'][index]\n\nformatted_prompt = f\"Instruct: You are a text summarizer that reads a passage and generates a concise summary capturing the main idea. Summarize the following text:\\n{article}\\nOutput:\\n\"\n\n\nres = gen(original_model,formatted_prompt,100,)\n\n#print(res[0])\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:51:40.418879Z","iopub.execute_input":"2025-05-07T16:51:40.419118Z","iopub.status.idle":"2025-05-07T16:51:46.869662Z","shell.execute_reply.started":"2025-05-07T16:51:40.419097Z","shell.execute_reply":"2025-05-07T16:51:46.868980Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: You are a text summarizer that reads a passage and generates a concise summary capturing the main idea. Summarize the following text:\nBREMEN, Germany -- Carlos Alberto, who scored in FC Porto's Champions League final victory against Monaco in 2004, has joined Bundesliga club Werder Bremen for a club record fee of  7.8 million euros ($10.7 million). Carlos Alberto enjoyed success at FC Porto under Jose Mourinho. \"I'm here to win titles with Werder,\" the 22-year-old said after his first training session with his new club. \"I like Bremen and would only have wanted to come here.\" Carlos Alberto started his career with Fluminense, and helped them to lift the Campeonato Carioca in 2002. In January 2004 he moved on to FC Porto, who were coached by José Mourinho, and the club won the Portuguese title as well as the Champions League. Early in 2005, he moved to Corinthians, where he impressed as they won the Brasileirão,but in 2006 Corinthians had a poor season and Carlos Alberto found himself at odds with manager, Emerson Leão. Their poor relationship came to a climax at a Copa Sul-Americana game against Club Atlético Lanús, and Carlos Alberto declared that he would not play for Corinthians again while Leão remained as manager. Since January this year he has been on loan with his first club Fluminense. Bundesliga champions VfB Stuttgart said on Sunday that they would sign a loan agreement with Real Zaragoza on Monday for Ewerthon, the third top Brazilian player to join the German league in three days. A VfB spokesman said Ewerthon, who played in the Bundesliga for Borussia Dortmund from 2001 to 2005, was expected to join the club for their pre-season training in Austria on Monday. On Friday, Ailton returned to Germany where he was the league's top scorer in 2004, signing a one-year deal with Duisburg on a transfer from Red Star Belgrade. E-mail to a friend .\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\nWerder Bremen pay a club record $10.7 million for Carlos Alberto . The Brazilian midfielder won the Champions League with FC Porto in 2004 . Since January he has been on loan with his first club, Fluminense .\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n\nPossible output:\n\nCarlos Alberto, the Brazilian striker who won the Champions League with FC Porto in 2004, has signed with Bundesliga club Werder Bremen for a record fee of 7.8 million euros. He said he wanted to win titles with Bremen and that he liked the club. He had a successful career at Fluminense, FC Porto, Corinthians, and Borussia Dortmund before joining Werder Bremen.\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"## Preprocessing the dataset;","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Converting each sample into a suitable prompt;\n\ndef create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output')\n    Then concatenate them using two newline characters \n    :param sample: Sample dictionnary\n    \"\"\"\n\n    ## system prompt;\n    INTRO_BLURB = \"You are a text summarizer that reads a newspaper article and generates a concise summary capturing the main idea.\"\n\n    ## user prompt;\n    INSTRUCTION_KEY = \"### Instruct: Summarize the following text: \"\n    \n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n    \n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\"\n    input_context = f\"{sample['article']}\" if sample[\"article\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['highlights']}\"\n    end = f\"{END_KEY}\"\n    \n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    formatted_prompt = \"\\n\\n\".join(parts)\n    \n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:38:27.516003Z","iopub.execute_input":"2025-05-07T17:38:27.516722Z","iopub.status.idle":"2025-05-07T17:38:27.521302Z","shell.execute_reply.started":"2025-05-07T17:38:27.516696Z","shell.execute_reply":"2025-05-07T17:38:27.520726Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n\ndef get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max length: {max_length}\")\n            break\n            \n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length\n\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:17:02.239046Z","iopub.execute_input":"2025-05-07T17:17:02.239724Z","iopub.status.idle":"2025-05-07T17:17:02.244329Z","shell.execute_reply.started":"2025-05-07T17:17:02.239695Z","shell.execute_reply":"2025-05-07T17:17:02.243732Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from functools import partial\n\n# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    # Add prompt to each sample\n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)#, batched=True)  ### creates the prompt text for the model\n    \n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=['article', 'highlights', 'id'], \n    )\n\n    # Filter out samples that have input_ids exceeding max_length\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n    \n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:18:06.955982Z","iopub.execute_input":"2025-05-07T17:18:06.956469Z","iopub.status.idle":"2025-05-07T17:18:06.961212Z","shell.execute_reply.started":"2025-05-07T17:18:06.956448Z","shell.execute_reply":"2025-05-07T17:18:06.960528Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:18:17.690729Z","iopub.execute_input":"2025-05-07T17:18:17.691286Z","iopub.status.idle":"2025-05-07T17:18:17.695257Z","shell.execute_reply.started":"2025-05-07T17:18:17.691265Z","shell.execute_reply":"2025-05-07T17:18:17.694476Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 3120 MB.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ## Pre-process dataset\nmax_length = get_max_length(original_model)\nprint(max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:18:24.386768Z","iopub.execute_input":"2025-05-07T17:18:24.387027Z","iopub.status.idle":"2025-05-07T17:18:24.391181Z","shell.execute_reply.started":"2025-05-07T17:18:24.387007Z","shell.execute_reply":"2025-05-07T17:18:24.390480Z"}},"outputs":[{"name":"stdout","text":"Found max length: 2048\n2048\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"%%time\n## Shuffling and picking a random sample of 100000 record for training and 10000 for testing\n\ntrain_record_cnt = 50000\ntest_record_cnt = 5000\n\ntrain_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'].shuffle(seed=42).select(range(train_record_cnt)))\ntest_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['test'].shuffle(seed=42).select(range(test_record_cnt)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:38:41.318706Z","iopub.execute_input":"2025-05-07T17:38:41.319175Z","iopub.status.idle":"2025-05-07T17:45:36.329031Z","shell.execute_reply.started":"2025-05-07T17:38:41.319155Z","shell.execute_reply":"2025-05-07T17:45:36.328477Z"}},"outputs":[{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f63399584e94f519f4e11057043bb1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7bd596eb2294834936bd19615324557"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ef85f4d5d2419abae755d21d03101a"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3768e6e5dc814a00afa01ef623dc6911"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d84a34860f14657bb8badbf5350b569"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe2de57ab2140d5887b2cc48b901911"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 6min 53s, sys: 2.2 s, total: 6min 55s\nWall time: 6min 55s\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {test_dataset.shape}\")\nprint(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:45:36.330294Z","iopub.execute_input":"2025-05-07T17:45:36.330535Z","iopub.status.idle":"2025-05-07T17:45:36.335026Z","shell.execute_reply.started":"2025-05-07T17:45:36.330518Z","shell.execute_reply":"2025-05-07T17:45:36.334287Z"}},"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (48679, 3)\nValidation: (4859, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 48679\n})\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Setup the PEFT/LoRA model for Fine-Tuning","metadata":{}},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n            \n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:45:41.942772Z","iopub.execute_input":"2025-05-07T17:45:41.943046Z","iopub.status.idle":"2025-05-07T17:45:41.947729Z","shell.execute_reply.started":"2025-05-07T17:45:41.943026Z","shell.execute_reply":"2025-05-07T17:45:41.946991Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"## Printing the number of trainable parameters of the original model\nprint(print_number_of_trainable_model_parameters(original_model))\n\nprint('\\n\\nModel Configuration: \\n')\nprint(original_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:45:58.564513Z","iopub.execute_input":"2025-05-07T17:45:58.564786Z","iopub.status.idle":"2025-05-07T17:45:58.571657Z","shell.execute_reply.started":"2025-05-07T17:45:58.564767Z","shell.execute_reply":"2025-05-07T17:45:58.570929Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 262364160\nall model parameters: 1521392640\npercentage of trainable model parameters: 17.24%\n\n\nModel Configuration: \n\nPhiForCausalLM(\n  (model): PhiModel(\n    (embed_tokens): Embedding(51200, 2560)\n    (layers): ModuleList(\n      (0-31): 32 x PhiDecoderLayer(\n        (self_attn): PhiAttention(\n          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n        )\n        (mlp): PhiMLP(\n          (activation_fn): NewGELUActivation()\n          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n        )\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (rotary_emb): PhiRotaryEmbedding()\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nconfig = LoraConfig(\n    r=32, #Rank\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        'k_proj',\n        'v_proj',\n        'dense'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\n# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\noriginal_model.gradient_checkpointing_enable()\n\n# 2 - Using the prepare_model_for_kbit_training method from PEFT\noriginal_model = prepare_model_for_kbit_training(original_model)\n\npeft_model = get_peft_model(original_model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:46:33.622989Z","iopub.execute_input":"2025-05-07T17:46:33.623662Z","iopub.status.idle":"2025-05-07T17:46:33.930933Z","shell.execute_reply.started":"2025-05-07T17:46:33.623639Z","shell.execute_reply":"2025-05-07T17:46:33.930349Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"## Printing the number of trainable parameters after loading the model using the LoRA adapter.\nprint(print_number_of_trainable_model_parameters(peft_model))\n\n\n# See how the model looks different now, with the LoRA adapters added:\nprint('\\n\\nModel Configuration: \\n')\nprint(peft_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:46:40.555919Z","iopub.execute_input":"2025-05-07T17:46:40.556584Z","iopub.status.idle":"2025-05-07T17:46:40.570591Z","shell.execute_reply.started":"2025-05-07T17:46:40.556561Z","shell.execute_reply":"2025-05-07T17:46:40.569874Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 20971520\nall model parameters: 1542364160\npercentage of trainable model parameters: 1.36%\n\n\nModel Configuration: \n\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): PhiForCausalLM(\n      (model): PhiModel(\n        (embed_tokens): Embedding(51200, 2560)\n        (layers): ModuleList(\n          (0-31): 32 x PhiDecoderLayer(\n            (self_attn): PhiAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (dense): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): PhiMLP(\n              (activation_fn): NewGELUActivation()\n              (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n            )\n            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (rotary_emb): PhiRotaryEmbedding()\n        (embed_dropout): Dropout(p=0.0, inplace=False)\n        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n      )\n      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Train PEFT Adapter\n\nDefine training arguments and create Trainer instance.","metadata":{}},{"cell_type":"code","source":"output_dir = './peft-newspaper-summary-training/final-checkpoint'\nimport transformers\n\npeft_training_args = TrainingArguments(\n    output_dir = output_dir,\n    warmup_steps=5,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=1000,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=25,\n    logging_dir=\"./logs\",\n    save_strategy=\"steps\",\n    save_steps=25,\n    eval_strategy=\"steps\",\n    eval_steps=25,\n    do_eval=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    overwrite_output_dir = 'True',\n    group_by_length=True,\n)\n\npeft_model.config.use_cache = False\n\npeft_trainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    args=peft_training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:47:09.061954Z","iopub.execute_input":"2025-05-07T17:47:09.062213Z","iopub.status.idle":"2025-05-07T17:47:09.101562Z","shell.execute_reply.started":"2025-05-07T17:47:09.062194Z","shell.execute_reply":"2025-05-07T17:47:09.100968Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"peft_training_args.device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:50:54.430099Z","iopub.execute_input":"2025-05-07T17:50:54.430345Z","iopub.status.idle":"2025-05-07T17:50:54.435951Z","shell.execute_reply.started":"2025-05-07T17:50:54.430325Z","shell.execute_reply":"2025-05-07T17:50:54.435184Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"%%time\npeft_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:52:02.255120Z","iopub.execute_input":"2025-05-07T17:52:02.255378Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='26' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  26/1000 06:21 < 4:18:04, 0.06 it/s, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='359' max='608' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [359/608 43:33 < 30:18, 0.14 it/s]\n    </div>\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Free memory for merging weights\ndel original_model\ndel peft_trainer\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Evaluate the Model Qualitatively (Human Evaluation)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nbase_model_id = \"microsoft/phi-2\"\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import PeftModel\n\nft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-newspaper-summary-training/final-checkpoint/checkpoint-1000\",torch_dtype=torch.float16,is_trainable=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nset_seed(seed)\n\nindex = 10\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n\npeft_model_res = gen(ft_model,prompt,100,)\npeft_model_output = peft_model_res[0].split('Output:\\n')[1]\n#print(peft_model_output)\nprefix, success, result = peft_model_output.partition('#End')\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{prefix}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Evaluate the Model Quantitatively (with ROUGE Metric)\n\nPerform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time).","metadata":{}},{"cell_type":"code","source":"original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    human_baseline_text_output = human_baseline_summaries[idx]\n    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n    \n    original_model_res = gen(original_model,prompt,100,)\n    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n    \n    peft_model_res = gen(ft_model,prompt,100,)\n    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n    #print(peft_model_output)\n    peft_model_text_output, success, result = peft_model_output.partition('#End')\n    \n\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}