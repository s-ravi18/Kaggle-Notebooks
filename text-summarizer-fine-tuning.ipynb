{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 📚 Fine-Tuning Phi-2 for Text Summarization using PEFT\n\n\n- Large Language Models (LLMs) like Phi-2 have shown impressive capabilities in understanding and generating human-like text. However, fine-tuning these powerful models from scratch can be expensive and computationally intensive.\n\n\n- In this notebook, we'll explore how to fine-tune Phi-2 for text summarization using a lightweight and efficient technique called **PEFT** (**Parameter-Efficient Fine-Tuning**). Instead of updating all of a model’s parameters, PEFT allows us to adapt pre-trained models using a small number of additional parameters, significantly reducing training cost while achieving competitive performance.\n\n\n- We’ll walk through the full process—loading the model, preparing the dataset, applying PEFT via LoRA (Low-Rank Adaptation), training, and evaluating results. Whether you're new to LLM fine-tuning or looking for a practical example of PEFT in action, this notebook is designed to help you understand the how and why behind efficient fine-tuning.\n\n\nLet’s get started and make Phi-2 a summarizer!","metadata":{}},{"cell_type":"code","source":"## Installing the necessary dependencies;\n\n!pip install -qq -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:48:59.334985Z","iopub.execute_input":"2025-05-07T16:48:59.335506Z","iopub.status.idle":"2025-05-07T16:50:29.164155Z","shell.execute_reply.started":"2025-05-07T16:48:59.335483Z","shell.execute_reply":"2025-05-07T16:50:29.163465Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"### Importing the necessary libraries;\n\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nfrom huggingface_hub import login\n\nimport pandas as pd\nimport numpy as np\n\n\n## Using kaggle's feature to add the hugging face token so that it restricts public visibility. Please find\n## this feature available by clicking the \"Add-ons\" button on the menu bar of the editor.\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hf_token\")\n\n\n### This sort of mimics the .env file that we create during project creation.\nlogin(token=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:50:29.165811Z","iopub.execute_input":"2025-05-07T16:50:29.166069Z","iopub.status.idle":"2025-05-07T16:50:56.301885Z","shell.execute_reply.started":"2025-05-07T16:50:29.166049Z","shell.execute_reply":"2025-05-07T16:50:56.301301Z"}},"outputs":[{"name":"stderr","text":"2025-05-07 16:50:41.805674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746636641.992996      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746636642.043959      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"### Utility function to get the GPU memory used at any point of time\nfrom pynvml import *\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n\n\nprint_gpu_utilization()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:50:56.302576Z","iopub.execute_input":"2025-05-07T16:50:56.302797Z","iopub.status.idle":"2025-05-07T16:50:56.308227Z","shell.execute_reply.started":"2025-05-07T16:50:56.302779Z","shell.execute_reply":"2025-05-07T16:50:56.307716Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 116 MB.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# 📑 Dataset: CNN/DailyMail for Abstractive Summarization\n\n\n- To fine-tune our Phi-2 model for summarization, we're using the CNN/DailyMail dataset—one of the most popular benchmarks for training and evaluating summarization models.\n\n\n- This dataset consists of news articles from CNN and the Daily Mail, along with human-written highlights that serve as target summaries. It's well-suited for abstractive summarization, where the model learns to generate concise, paraphrased summaries instead of merely copying parts of the original text.\n\nWe'll use the **datasets** library to load the pre-processed version:","metadata":{}},{"cell_type":"code","source":"## Loading the dataset;\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"abisee/cnn_dailymail\", \"1.0.0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:50:56.310298Z","iopub.execute_input":"2025-05-07T16:50:56.310579Z","iopub.status.idle":"2025-05-07T16:51:06.590054Z","shell.execute_reply.started":"2025-05-07T16:50:56.310555Z","shell.execute_reply":"2025-05-07T16:51:06.589558Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1938e42ab8104c589c2200e754b1fa5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/256M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5bd5d46537349158a7ce51ebb612ae6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc644e2adda64b1785c2871f7243257e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e49cc072b354d1f9d3a6c6e1f727966"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1501979d881d42e8a76b5f2014e19696"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74800028c1a14d1bbb108d625463f050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"504645bd61954b78aee156ede146ce6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18bb01f470ab477a899983326d717d1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e13b006967244482a735fc69cf52ef09"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"## The dataset is already divided into three partitions, so we dont have to split the same for inferencing\n\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:51:06.590716Z","iopub.execute_input":"2025-05-07T16:51:06.590930Z","iopub.status.idle":"2025-05-07T16:51:06.595724Z","shell.execute_reply.started":"2025-05-07T16:51:06.590913Z","shell.execute_reply":"2025-05-07T16:51:06.595184Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 287113\n    })\n    validation: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 11490\n    })\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"## Viewing a sample observation;\n\ndataset['train']['article'][0], dataset['train']['highlights'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:51:06.596345Z","iopub.execute_input":"2025-05-07T16:51:06.596686Z","iopub.status.idle":"2025-05-07T16:51:09.304245Z","shell.execute_reply.started":"2025-05-07T16:51:06.596664Z","shell.execute_reply":"2025-05-07T16:51:09.303672Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.',\n \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday . Young actor says he has no plans to fritter his cash away . Radcliffe's earnings from first five Potter films have been held in trust fund .\")"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ⚙️ Loading Phi-2 with 4-bit Quantization (BitsAndBytes)\n\n- To **reduce memory usage** and make training feasible on limited hardware, we’re loading the Phi-2 model with **4-bit quantization** using the **bitsandbytes** library. This dramatically reduces the model's memory footprint while preserving much of its performance.","metadata":{}},{"cell_type":"code","source":"## Defining the bnb config/parameters, which defines the qunatisation settings for loading the model; \ncompute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\n\n\n## To place the model on the first available GPU\ndevice_map = {\"\": 0}\n\n\n## Loading the model;\nmodel_name='microsoft/phi-2'\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name, \n                                                      device_map=device_map,\n                                                      quantization_config=bnb_config,  ## the bnb config is passed as a parameter\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)\n\n\n## Printing the GPU Util;\nprint_gpu_utilization()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:51:09.305033Z","iopub.execute_input":"2025-05-07T16:51:09.305272Z","iopub.status.idle":"2025-05-07T16:51:37.387575Z","shell.execute_reply.started":"2025-05-07T16:51:09.305246Z","shell.execute_reply":"2025-05-07T16:51:37.386834Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"651e2085364f4ce797de354349a57c71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01b9b9370d5545e3be55476256fb27fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3cdfd3cc1ef478981825a80ef1fea62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0be2cbff1f2f4c2cbf3bee8658801257"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d93bdaf9f15448fa30c8a1aa915d4b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f67c8ac5e504e67bc151fb99fcee387"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea2edd6335314fb191f5973b48995fe7"}},"metadata":{}},{"name":"stdout","text":"GPU memory occupied: 3104 MB.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🔤 Tokenizer Setup\n\n\n- Before we can feed text into the Phi-2 model, we need to load its corresponding tokenizer. The tokenizer is responsible for converting raw text into input tokens that the model understands, and vice versa.\n\nWe’ll demonstrate two ways to load the tokenizer, depending on whether you’re training or evaluating.","metadata":{}},{"cell_type":"code","source":"### Tokeniser\n\nmodel_name='microsoft/phi-2'\n\n\n## Loading the tokenizer for trainer;\n\ntokenizer = AutoTokenizer.from_pretrained(model_name,\n                                          trust_remote_code=True,\n                                          padding_side=\"left\",\n                                          add_eos_token=True,\n                                          add_bos_token=True,\n                                          use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token\n\n\n## Printing the GPU Util;\nprint_gpu_utilization()\n\n\n## Loading the tokenizer for inferencing a sample record;\neval_tokenizer = AutoTokenizer.from_pretrained(model_name, \n                                               add_bos_token=True, \n                                               trust_remote_code=True, \n                                               use_fast=False)\n\neval_tokenizer.pad_token = eval_tokenizer.eos_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:51:37.388388Z","iopub.execute_input":"2025-05-07T16:51:37.388705Z","iopub.status.idle":"2025-05-07T16:51:39.173256Z","shell.execute_reply.started":"2025-05-07T16:51:37.388673Z","shell.execute_reply":"2025-05-07T16:51:39.172655Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ff50b744af845e8ae453e18d456afc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b664f2c59fac49059272f38211ac79da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26f2c24c5bbf4ca787926545e0fa7c19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b351cfd4e1444dfc86365ea8ba362fdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7488cb0c32504394a53416dcc9e57893"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"984d73c1b73a4d6882d827bd6b0a8442"}},"metadata":{}},{"name":"stdout","text":"GPU memory occupied: 3104 MB.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Looking at the above code block, you might wonder why I have defined the tokenizer object twice. While you might think they are the same at first glance, but infact they are slightly different. \n\n- **tokenizer**: This tokenizer is used during **training** and **generation**. Padding on the left helps with alignment during causal attention, and setting the pad token to the EOS token avoids errors related to undefined tokens.\n\n- **eval_tokenizer**:  This is used for evaluation purposes only, hence we skip setting **padding_side** and **add_eos_token** since evaluation usually doesn’t involve padding or sequence truncation in the same way as training.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:51:39.173934Z","iopub.execute_input":"2025-05-07T16:51:39.174112Z","iopub.status.idle":"2025-05-07T16:51:40.411693Z","shell.execute_reply.started":"2025-05-07T16:51:39.174098Z","shell.execute_reply":"2025-05-07T16:51:40.411137Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"- To easily test the vanilla model, we define a simple function gen() that takes in a prompt, feeds it to the model, and returns the generated text.","metadata":{}},{"cell_type":"code","source":"def gen(model, p, maxlen=100, sample=True):\n    \n    toks = eval_tokenizer(p, return_tensors=\"pt\")  # Tokenize the prompt into input IDs\n    \n    res = model.generate(\n        **toks.to(\"cuda\"),                     # Move tokens to GPU\n        max_new_tokens=maxlen,                # Limit generation length\n        do_sample=sample,                     # Enable sampling if True (for diversity)\n        num_return_sequences=1,               # Generate a single sequence\n        temperature=0.1,                      # Low temperature = more deterministic output\n        num_beams=1,                          # No beam search (greedy or sampling instead)\n        top_p=0.95                            # Nucleus sampling: sample from top 95% of token probability mass\n    ).to('cpu')                               # Move result back to CPU\n    \n    return eval_tokenizer.batch_decode(res, skip_special_tokens=True)  # Decode tokens into readable text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:51:40.413709Z","iopub.execute_input":"2025-05-07T16:51:40.414101Z","iopub.status.idle":"2025-05-07T16:51:40.418127Z","shell.execute_reply.started":"2025-05-07T16:51:40.414082Z","shell.execute_reply":"2025-05-07T16:51:40.417524Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Picking up a random sample and checking the prediction of the vanilla model.","metadata":{}},{"cell_type":"code","source":"from transformers import set_seed\nseed = 42\nset_seed(seed)\n\n\n## defining some random index;\nindex = 12\n\narticle = dataset['train']['article'][index]\nsummary = dataset['train']['highlights'][index]\n\nformatted_prompt = f\"Instruct: You are a text summarizer that reads a passage and generates a concise summary capturing the main idea. Summarize the following text:\\n{article}\\nOutput:\\n\"\n\n\nres = gen(original_model,formatted_prompt,100,)\n\n#print(res[0])\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:51:40.418879Z","iopub.execute_input":"2025-05-07T16:51:40.419118Z","iopub.status.idle":"2025-05-07T16:51:46.869662Z","shell.execute_reply.started":"2025-05-07T16:51:40.419097Z","shell.execute_reply":"2025-05-07T16:51:46.868980Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: You are a text summarizer that reads a passage and generates a concise summary capturing the main idea. Summarize the following text:\nBREMEN, Germany -- Carlos Alberto, who scored in FC Porto's Champions League final victory against Monaco in 2004, has joined Bundesliga club Werder Bremen for a club record fee of  7.8 million euros ($10.7 million). Carlos Alberto enjoyed success at FC Porto under Jose Mourinho. \"I'm here to win titles with Werder,\" the 22-year-old said after his first training session with his new club. \"I like Bremen and would only have wanted to come here.\" Carlos Alberto started his career with Fluminense, and helped them to lift the Campeonato Carioca in 2002. In January 2004 he moved on to FC Porto, who were coached by José Mourinho, and the club won the Portuguese title as well as the Champions League. Early in 2005, he moved to Corinthians, where he impressed as they won the Brasileirão,but in 2006 Corinthians had a poor season and Carlos Alberto found himself at odds with manager, Emerson Leão. Their poor relationship came to a climax at a Copa Sul-Americana game against Club Atlético Lanús, and Carlos Alberto declared that he would not play for Corinthians again while Leão remained as manager. Since January this year he has been on loan with his first club Fluminense. Bundesliga champions VfB Stuttgart said on Sunday that they would sign a loan agreement with Real Zaragoza on Monday for Ewerthon, the third top Brazilian player to join the German league in three days. A VfB spokesman said Ewerthon, who played in the Bundesliga for Borussia Dortmund from 2001 to 2005, was expected to join the club for their pre-season training in Austria on Monday. On Friday, Ailton returned to Germany where he was the league's top scorer in 2004, signing a one-year deal with Duisburg on a transfer from Red Star Belgrade. E-mail to a friend .\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\nWerder Bremen pay a club record $10.7 million for Carlos Alberto . The Brazilian midfielder won the Champions League with FC Porto in 2004 . Since January he has been on loan with his first club, Fluminense .\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n\nPossible output:\n\nCarlos Alberto, the Brazilian striker who won the Champions League with FC Porto in 2004, has signed with Bundesliga club Werder Bremen for a record fee of 7.8 million euros. He said he wanted to win titles with Bremen and that he liked the club. He had a successful career at Fluminense, FC Porto, Corinthians, and Borussia Dortmund before joining Werder Bremen.\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"## Preprocessing the dataset;","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🧾 Converting Samples into Prompt Format for Instruction Tuning\n\n\n- To fine-tune our model effectively, we need to structure the training data in a way that mimics instruction-based prompting. This helps the model learn not just from the raw article/summary pairs, but also from a consistent pattern of \"prompt → response.\"\n\n- The function below takes a single sample (an article and its human-written summary) and formats it into a structured prompt using natural language cues.\n\n- These parts are combined into a full prompt in the following order:\n\n1. System blurb\n\n2. Instruction header\n\n3. Article text (input)\n\n4. Output header + summary\n\n5. End token","metadata":{}},{"cell_type":"code","source":"## Converting each sample into a suitable prompt;\n\ndef create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output')\n    Then concatenate them using two newline characters \n    :param sample: Sample dictionnary\n    \"\"\"\n\n    ## system prompt;\n    INTRO_BLURB = \"You are a text summarizer that reads a newspaper article and generates a concise summary capturing the main idea.\"\n\n    ## user prompt;\n    INSTRUCTION_KEY = \"### Instruct: Summarize the following text: \"\n    \n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n    \n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\"\n    input_context = f\"{sample['article']}\" if sample[\"article\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['highlights']}\"\n    end = f\"{END_KEY}\"\n    \n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    formatted_prompt = \"\\n\\n\".join(parts)\n    \n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:38:27.516003Z","iopub.execute_input":"2025-05-07T17:38:27.516722Z","iopub.status.idle":"2025-05-07T17:38:27.521302Z","shell.execute_reply.started":"2025-05-07T17:38:27.516696Z","shell.execute_reply":"2025-05-07T17:38:27.520726Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"**📏 Determining the Model's Maximum Sequence Length**\n\n\n- When tokenizing text for training or inference, it's essential to know the maximum number of tokens your model can handle in a single input. Feeding in longer sequences than the model supports will result in errors or truncated outputs.\n\n- The function below attempts to dynamically fetch this limit from the model's configuration:","metadata":{}},{"cell_type":"code","source":"# SOURCE - https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n\ndef get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max length: {max_length}\")\n            break\n            \n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:17:02.239046Z","iopub.execute_input":"2025-05-07T17:17:02.239724Z","iopub.status.idle":"2025-05-07T17:17:02.244329Z","shell.execute_reply.started":"2025-05-07T17:17:02.239695Z","shell.execute_reply":"2025-05-07T17:17:02.243732Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"### Tokenising the input as batches;\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **🧹 Dataset Preprocessing for Fine-Tuning**\n\n\nBefore we can train our model, we need to **format**, **tokenize**, **filter**, and shuffle the dataset. This function prepares the dataset so it's ready to be fed into the model.\n\n```\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset):\n```\n\n#### **Step-by-step Explanation:**\n\n**1. Format the data as prompts:**\n\n- We use the previously defined create_prompt_formats() function to convert each article-summary pair into a well-structured instruction-style prompt.\n\n```\ndataset = dataset.map(create_prompt_formats)\n```\n\n**2. Tokenize the formatted text:**\n\n\nWe use functools.partial to create a preprocessing function that will tokenize each sample with a fixed max_length and tokenizer:\n\n```\n_preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n```\n\nThen, apply this to the dataset with batching enabled. We also remove the raw columns we no longer need:\n\n```\ndataset = dataset.map(\n    _preprocessing_function,\n    batched=True,\n    remove_columns=['article', 'highlights', 'id']\n)\n```\n\n**3. Filter out overly long samples:**\n\nSometimes, even after formatting, tokenized samples exceed the model's max length. We drop them to avoid runtime errors:\n\n```\ndataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n```\n\n\n✅ This function ensures our model gets clean, consistently formatted, and length-safe data, crucial for stable and effective training.","metadata":{}},{"cell_type":"code","source":"from functools import partial\n\n# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    # Add prompt to each sample\n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)#, batched=True)  ### creates the prompt text for the model\n    \n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=['article', 'highlights', 'id'], \n    )\n\n    # Filter out samples that have input_ids exceeding max_length\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n    \n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:18:06.955982Z","iopub.execute_input":"2025-05-07T17:18:06.956469Z","iopub.status.idle":"2025-05-07T17:18:06.961212Z","shell.execute_reply.started":"2025-05-07T17:18:06.956448Z","shell.execute_reply":"2025-05-07T17:18:06.960528Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"## Printing the GPU Util;\nprint_gpu_utilization()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:18:17.690729Z","iopub.execute_input":"2025-05-07T17:18:17.691286Z","iopub.status.idle":"2025-05-07T17:18:17.695257Z","shell.execute_reply.started":"2025-05-07T17:18:17.691265Z","shell.execute_reply":"2025-05-07T17:18:17.694476Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 3120 MB.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"## Getting the max sequence length;\nmax_length = get_max_length(original_model)\nprint(max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:18:24.386768Z","iopub.execute_input":"2025-05-07T17:18:24.387027Z","iopub.status.idle":"2025-05-07T17:18:24.391181Z","shell.execute_reply.started":"2025-05-07T17:18:24.387007Z","shell.execute_reply":"2025-05-07T17:18:24.390480Z"}},"outputs":[{"name":"stdout","text":"Found max length: 2048\n2048\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"%%time\n## Shuffling and picking a random sample of 100000 record for training and 10000 for testing\n\ntrain_record_cnt = 50000\ntest_record_cnt = 5000\n\ntrain_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'].shuffle(seed=42).select(range(train_record_cnt)))\ntest_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['test'].shuffle(seed=42).select(range(test_record_cnt)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:38:41.318706Z","iopub.execute_input":"2025-05-07T17:38:41.319175Z","iopub.status.idle":"2025-05-07T17:45:36.329031Z","shell.execute_reply.started":"2025-05-07T17:38:41.319155Z","shell.execute_reply":"2025-05-07T17:45:36.328477Z"}},"outputs":[{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f63399584e94f519f4e11057043bb1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7bd596eb2294834936bd19615324557"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ef85f4d5d2419abae755d21d03101a"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3768e6e5dc814a00afa01ef623dc6911"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d84a34860f14657bb8badbf5350b569"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe2de57ab2140d5887b2cc48b901911"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 6min 53s, sys: 2.2 s, total: 6min 55s\nWall time: 6min 55s\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {test_dataset.shape}\")\nprint(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:45:36.330294Z","iopub.execute_input":"2025-05-07T17:45:36.330535Z","iopub.status.idle":"2025-05-07T17:45:36.335026Z","shell.execute_reply.started":"2025-05-07T17:45:36.330518Z","shell.execute_reply":"2025-05-07T17:45:36.334287Z"}},"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (48679, 3)\nValidation: (4859, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 48679\n})\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setup the PEFT/LoRA model for Fine-Tuning","metadata":{}},{"cell_type":"markdown","source":"- When using Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA, we freeze most of the base model's weights and only train a small subset of parameters. This function helps us verify that setup by calculating how many model parameters are actually being trained.","metadata":{}},{"cell_type":"code","source":"# This function helps us verify that setup by calculating how many model parameters are actually being trained.\ndef print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n            \n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:45:41.942772Z","iopub.execute_input":"2025-05-07T17:45:41.943046Z","iopub.status.idle":"2025-05-07T17:45:41.947729Z","shell.execute_reply.started":"2025-05-07T17:45:41.943026Z","shell.execute_reply":"2025-05-07T17:45:41.946991Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"## Printing the number of trainable parameters of the original vanilla model\nprint(print_number_of_trainable_model_parameters(original_model))\n\nprint('\\n\\nModel Configuration: \\n')\nprint(original_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:45:58.564513Z","iopub.execute_input":"2025-05-07T17:45:58.564786Z","iopub.status.idle":"2025-05-07T17:45:58.571657Z","shell.execute_reply.started":"2025-05-07T17:45:58.564767Z","shell.execute_reply":"2025-05-07T17:45:58.570929Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 262364160\nall model parameters: 1521392640\npercentage of trainable model parameters: 17.24%\n\n\nModel Configuration: \n\nPhiForCausalLM(\n  (model): PhiModel(\n    (embed_tokens): Embedding(51200, 2560)\n    (layers): ModuleList(\n      (0-31): 32 x PhiDecoderLayer(\n        (self_attn): PhiAttention(\n          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n        )\n        (mlp): PhiMLP(\n          (activation_fn): NewGELUActivation()\n          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n        )\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (rotary_emb): PhiRotaryEmbedding()\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"**🛠️ Applying PEFT with LoRA for Efficient Fine-Tuning**\n\n- r=32: The rank of the low-rank decomposition. Controls how much of the original weights LoRA tries to approximate. Higher = more capacity (more weighta to train).\n\n- lora_alpha=32: A scaling factor applied to the LoRA updates. Think of it as a learning rate multiplier for the new trainable layers.\n\n- target_modules: The names of the layers to inject LoRA adapters into. In transformers, these are typically **q_proj**, **k_proj**, **v_proj**, and **dense layers** inside the attention mechanism.\n\n- bias=\"none\": Don't apply LoRA to any bias terms.\n\n- lora_dropout=0.05: Apply dropout within the LoRA layers for **regularization**.\n\n- task_type=\"CAUSAL_LM\": Specifies the task type, needed for adapter setup.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n\n## LoRA Configuration\nconfig = LoraConfig(\n    r=32, #Rank\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        'k_proj',\n        'v_proj',\n        'dense'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\n\n# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning:\n# This helps reduce memory usage during training by saving intermediate activations to recompute \n# them on the backward pass, rather than storing all of them. \n# It's essential when training large models on limited GPU memory.\noriginal_model.gradient_checkpointing_enable()\n\n\n\n# 2 - Using the prepare_model_for_kbit_training method from PEFT:\n# This function:\n# - Ensures the quantized model is ready for training.\n# - Casts certain layers to appropriate data types (e.g., normalization layers to float32).\n# - Ensures gradients behave properly in a 4-bit quantized environment.\n\noriginal_model = prepare_model_for_kbit_training(original_model)\n\n\n## Inject LoRA Adapters - making only a small subset of parameters trainable\npeft_model = get_peft_model(original_model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:46:33.622989Z","iopub.execute_input":"2025-05-07T17:46:33.623662Z","iopub.status.idle":"2025-05-07T17:46:33.930933Z","shell.execute_reply.started":"2025-05-07T17:46:33.623639Z","shell.execute_reply":"2025-05-07T17:46:33.930349Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"## Printing the number of trainable parameters after loading the model using the LoRA adapter.\nprint(print_number_of_trainable_model_parameters(peft_model))\n\n\n# See how the model looks different now, with the LoRA adapters added:\nprint('\\n\\nModel Configuration: \\n')\nprint(peft_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:46:40.555919Z","iopub.execute_input":"2025-05-07T17:46:40.556584Z","iopub.status.idle":"2025-05-07T17:46:40.570591Z","shell.execute_reply.started":"2025-05-07T17:46:40.556561Z","shell.execute_reply":"2025-05-07T17:46:40.569874Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 20971520\nall model parameters: 1542364160\npercentage of trainable model parameters: 1.36%\n\n\nModel Configuration: \n\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): PhiForCausalLM(\n      (model): PhiModel(\n        (embed_tokens): Embedding(51200, 2560)\n        (layers): ModuleList(\n          (0-31): 32 x PhiDecoderLayer(\n            (self_attn): PhiAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (dense): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): PhiMLP(\n              (activation_fn): NewGELUActivation()\n              (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n            )\n            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (rotary_emb): PhiRotaryEmbedding()\n        (embed_dropout): Dropout(p=0.0, inplace=False)\n        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n      )\n      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"**Now you can see that the number of trainable parameters reduced from 17.24% to 1.36%. This is the power of LoRA.**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🚀 Training the Model","metadata":{}},{"cell_type":"markdown","source":"**Define training arguments and create Trainer instance.**\n\n\nThis block defines the training configuration:\n\n- warmup_steps=5: Gradually increases the learning rate during the first 5 steps to stabilize training.\n\n- per_device_train_batch_size=1: We use a small batch size to reduce memory usage.\n\n- gradient_accumulation_steps=4: Gradients are accumulated over 4 steps before a backward pass—effectively simulating a larger batch size.\n\n- max_steps=1000: Total number of training steps.\n\n- learning_rate=2e-4: The learning rate used by the optimizer.\n\n- optim=\"paged_adamw_8bit\": Uses memory-efficient optimizer suited for large models.\n\n- logging_steps=25, save_steps=25, eval_steps=25: Logs, saves, and evaluates the model every 25 steps.\n\n- do_eval=True: Enables evaluation during training.\n\n- gradient_checkpointing=True: Reduces memory usage at the cost of extra computation by checkpointing intermediate activations.\n\n- overwrite_output_dir='True': Ensures that the output directory can be overwritten.\n\n\n**I would suggest looking up each parameter to understand them in a better fashion.**\n","metadata":{}},{"cell_type":"code","source":"# 🔧 Define Training Arguments for PEFT Model Fine-Tuning\noutput_dir = './peft-newspaper-summary-training/final-checkpoint'\nimport transformers\n\n# ⚙️ Configure TrainingArguments\npeft_training_args = TrainingArguments(\n    output_dir = output_dir,\n    warmup_steps=5,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=1000,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=25,\n    logging_dir=\"./logs\",\n    save_strategy=\"steps\",\n    save_steps=25,\n    eval_strategy=\"steps\",\n    eval_steps=25,\n    do_eval=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    overwrite_output_dir = 'True',\n    group_by_length=True,\n)\n\n# 🛠 Disable Cache for Training:\n# Disabling caching is necessary for models that use gradient_checkpointing, \n# as cached outputs interfere with memory optimization.\npeft_model.config.use_cache = False\n\n\n# 🧑‍🏫 Initialize the PEFT Trainer\npeft_trainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    args=peft_training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),  ### does dynamic padding\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:47:09.061954Z","iopub.execute_input":"2025-05-07T17:47:09.062213Z","iopub.status.idle":"2025-05-07T17:47:09.101562Z","shell.execute_reply.started":"2025-05-07T17:47:09.062194Z","shell.execute_reply":"2025-05-07T17:47:09.100968Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"## Ensures that our model is loaded up on the GPU\npeft_training_args.device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:50:54.430099Z","iopub.execute_input":"2025-05-07T17:50:54.430345Z","iopub.status.idle":"2025-05-07T17:50:54.435951Z","shell.execute_reply.started":"2025-05-07T17:50:54.430325Z","shell.execute_reply":"2025-05-07T17:50:54.435184Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"**The Training begins!**","metadata":{}},{"cell_type":"code","source":"%%time\npeft_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:52:02.255120Z","iopub.execute_input":"2025-05-07T17:52:02.255378Z","iopub.status.idle":"2025-05-07T19:02:43.991288Z","shell.execute_reply.started":"2025-05-07T17:52:02.255359Z","shell.execute_reply":"2025-05-07T19:02:43.989967Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='26' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  26/1000 06:21 < 4:18:04, 0.06 it/s, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='520' max='608' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [520/608 1:02:55 < 10:40, 0.14 it/s]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\", line 1327, in time\n    out = eval(code, glob, local_ns)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<timed eval>\", line 1, in <module>\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n    return inner_training_loop(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2627, in _inner_training_loop\n    self._maybe_log_save_evaluate(\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3096, in _maybe_log_save_evaluate\n    metrics = self._evaluate(trial, ignore_keys_for_eval)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3045, in _evaluate\n    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 4154, in evaluate\n    output = eval_loop(\n             ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 4338, in evaluation_loop\n    for step, inputs in enumerate(dataloader):\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\", line 575, in __iter__\n    current_batch = send_to_device(current_batch, self.device, non_blocking=self._non_blocking)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\", line 153, in send_to_device\n    return tensor.to(device, non_blocking=non_blocking)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 820, in to\n    self.data = {\n                ^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 821, in <dictcomp>\n    k: v.to(device=device, non_blocking=non_blocking) if isinstance(v, torch.Tensor) else v\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n    stb = value._render_traceback_()\n          ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n    traceback_info = getframeinfo(tb, context)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1684, in getframeinfo\n    filename = getsourcefile(frame) or getfile(frame)\n               ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n    module = getmodule(object, filename)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 988, in getmodule\n    if ismodule(module) and hasattr(module, '__file__'):\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1326\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2626\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2627\u001b[0;31m                         self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2628\u001b[0m                             \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3096\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3097\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3044\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3045\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3046\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4153\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4154\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4155\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4337\u001b[0m         \u001b[0;31m# Main evaluation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4338\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4339\u001b[0m             \u001b[0;31m# Update the observed num examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                     \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# .to() doesn't accept non_blocking as kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m             self.data = {\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    820\u001b[0m             self.data = {\n\u001b[0;32m--> 821\u001b[0;31m                 \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1301328836.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'peft_trainer.train()\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-54>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"],"ename":"TypeError","evalue":"object of type 'NoneType' has no len()","output_type":"error"}],"execution_count":43},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T19:02:43.991758Z","iopub.status.idle":"2025-05-07T19:02:43.991977Z","shell.execute_reply.started":"2025-05-07T19:02:43.991875Z","shell.execute_reply":"2025-05-07T19:02:43.991884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Free memory for merging weights\ndel original_model\ndel peft_trainer\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Printing the GPU Util;\nprint_gpu_utilization()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Once the training process is done, which took almost 4 hours for me, we can understanding the output of the model.","metadata":{}},{"cell_type":"markdown","source":"# 🧠 Qualitative Evaluation of Model Outputs","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n## Loading the vanilla model and tokeniser to compare its results with the fine tuned model;\nbase_model_id = \"microsoft/phi-2\"\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)\n\neval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Remember, when you fine-tune a model using PEFT (e.g. LoRA), **you are not training all the original model’s parameters**. Instead, you train a **small set of additional, trainable parameters** (called adapters or LoRA modules) that are layered on top of a frozen base model.\n\n\nHence when you load the fine-tuned model using PeftModel.from_pretrained, you need to pass the base model as an argument, along with other arguments.\n\n**Essentially, the PeftModel.from_pretrained method:**\n\n- Loads the adapter configuration.\n\n- Injects the fine-tuned adapter weights into the correct layers of base_model.\n\n- Returns a composite model (original + PEFT modules) ready for inference or further fine-tuning.\n\n\n","metadata":{}},{"cell_type":"code","source":"# 💾 Load the Final Model Checkpoint\n# we set is_trainable as False, as we are freezing the weights.\n\nfrom peft import PeftModel\n\nft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-newspaper-summary-training/final-checkpoint/checkpoint-1000\",torch_dtype=torch.float16,is_trainable=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Picking up a random index and comparing the outputs;**","metadata":{}},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nset_seed(seed)\n\n\nindex = 10\n\narticle = dataset['validation'][index]['article']\nsummary = dataset['validation'][index]['highlights']\n\nprompt = f\"Instruct: You are a text summarizer that reads a passage and generates a concise summary capturing the main idea. Summarize the following text:\\n{article}\\nOutput:\\n\"\n\npeft_model_res = gen(ft_model,prompt,100,)\npeft_model_output = peft_model_res[0].split('Output:\\n')[1]\n#print(peft_model_output)\nprefix, success, result = peft_model_output.partition('#End')\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{prefix}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluate the Model Quantitatively (with **ROUGE** Metric)\n\nPerform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time).","metadata":{}},{"cell_type":"code","source":"original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\narticles = dataset['validation'][0:10]['article']\nhuman_baseline_summaries = dataset['validation'][0:10]['highlights']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\n\nfor idx, article in enumerate(articles):\n    human_baseline_text_output = human_baseline_summaries[idx]\n    prompt = f\"Instruct: You are a text summarizer that reads a passage and generates a concise summary capturing the main idea. Summarize the following text:\\n{article}\\nOutput:\\n\"\n    \n    original_model_res = gen(original_model,prompt,100,)\n    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n    \n    peft_model_res = gen(ft_model,prompt,100,)\n    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n    #print(peft_model_output)\n    peft_model_text_output, success, result = peft_model_output.partition('#End')\n    \n\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## To compare the summaries;\ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) SCORE;**\n\n- Typically used for evaluating the produced text and the reference text - here it is for evaluating summarization quality by comparing model outputs to human-written reference summaries.\n\n- ROUGE evaluates the **overlap of n-grams** and **longest common subsequences** between predicted and reference summaries.\n\n\n#### **📏 How ROUGE is Calculated (Concise Explanation):**\n\nROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures the quality of a generated summary by comparing it to a reference summary, focusing on text overlap. It reports three main metrics:\n\n1. ROUGE-1: Measures the overlap of individual words (unigrams).\n\n2. ROUGE-2: Measures the overlap of word pairs (bigrams).\n\n3. ROUGE-L: Measures the longest common subsequence (LCS) of words in the same order.\n\n\nFor each summary pair (prediction vs. reference), ROUGE computes:\n\n1. Precision: How many words in the prediction are also in the reference.\n\n2. Recall: How many words in the reference are captured by the prediction.\n\n3. F1-Score: Harmonic mean of precision and recall.\n\nThe final ROUGE score is the average F1 score across all prediction–reference pairs, providing an overall measure of how well the model’s summaries align with human-written ones.\n\n\n#### **Example Explanation;**\n\n- Reference: \"The cat sat on the mat\"\n- Prediction: \"A cat sat on mat\"\n\n| **Metric**  | **Matching Units**                  | **Precision** | **Recall**   | **F1 Score** |\n| ----------- | ----------------------------------- | ------------- | ------------ | ------------ |\n| **ROUGE-1** | `[\"cat\", \"sat\", \"on\", \"mat\"]`       | 4 / 5 = 0.80  | 4 / 6 ≈ 0.67 | **0.73**     |\n| **ROUGE-2** | `[\"cat sat\", \"sat on\"]`             | 2 / 4 = 0.50  | 2 / 5 = 0.40 | **0.44**     |\n| **ROUGE-L** | `[\"cat\", \"sat\", \"on\", \"mat\"]` (LCS) | 4 / 5 = 0.80  | 4 / 6 ≈ 0.67 | **0.73**     |\n\n\n\n#### In general, ROUGE-1, ROUGE-2, ROUGE-L are preferred and reported, as it's harder for the model to match exact sequences of 3+ words.\n","metadata":{}},{"cell_type":"code","source":"!pip install rouge_score\nimport evaluate\n\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 📌 End Notes\n\n- In this notebook, we demonstrated how to fine-tune a large language model using Parameter-Efficient Fine-Tuning (PEFT) on a news summarization task. We also evaluated the model's performance using ROUGE metrics, and compared it against the base model both quantitatively and qualitatively.\n\n### **Key takeaways:**\n\n1. PEFT allows for efficient training with fewer parameters and lower compute costs.\n\n2. ROUGE provides a structured way to evaluate summarization quality through n-gram and sequence overlap.\n\n3. Human evaluation remains essential to capture fluency, relevance, and coherence.\n","metadata":{}},{"cell_type":"markdown","source":"## Have questions or suggestions? Feel free to post your doubts in the comments section below — I’ll be happy to help!\n\n## Found this notebook helpful? Please consider **upvoting** — it helps others discover this content and motivates me to create more educational notebooks!","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}