{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Hi Guys! Today, I have chosen an important problem statement to work upon. Let me break it down into simple words, and you will soon understand its importance.  ","metadata":{}},{"cell_type":"markdown","source":"### -  Financial institutions invest a ton of money for constructing credit risk analysis models to determine the probability of default of a potential borrower. The models provide information on the level of a borrower's credit risk at any particular time. \n\n### -  Some of you might be wondering what \"credit\" is. Well here's come the definition:","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://www.greenbiz.com/sites/default/files/2020-09/definition_conceptart.jpg\" alt=\"Heat beating\" style=\"height:300px;margin-top:3rem;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:red;\"> - \"Credit is the ability to borrow money or access goods or services with the understanding that you'll pay later.\" </span>\n\n### <span style=\"color:blue;\"> - \"Creditworthiness is how a lender determines that you will default on your debt obligations, or how worthy you are to receive new credit. Your creditworthiness is what creditors look at before they approve any new credit to you.\" </span>","metadata":{}},{"cell_type":"markdown","source":"### Credit risks are a commonly observed phenomenon in areas of finance that relate to mortgages, credit cards, and other kinds of loans. There is always a probability that the borrower may not get back with the amount.","metadata":{}},{"cell_type":"markdown","source":"So it is important that when a borrower applies for a loan, the lender or the issuer must establish and examine the borrower’s ability to repay the loan. So in this notebook, I will be doing the following stuff:\n\n1. Exploring the Dataset(**EDA**)\n2. Applying **Oversampling** Techniques\n3. Test all machine learning models with **Cross Validation**\n4. **Hyperparameter tune** the best model\n5. Analyse the best model with the help of relevant **Metrics** \n6. **Pickling** the best model \n7. Creating an **UI**(User Interface) with the help of **Streamlit** \n8. **Deployment** on **Heroku** platform\n9. A sample prediction will be made to test the application\n\nThe 7th and 8th steps will be shown and explained with the help of screenshots.","metadata":{}},{"cell_type":"markdown","source":"Importing the necessary header files:","metadata":{}},{"cell_type":"code","source":"## Basic Libraries:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline \n\nplt.style.use('fivethirtyeight') ## Setting the Style\n\n## For making sample data:\nfrom sklearn.datasets import make_classification\n\n## For Preprocessing: \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, RepeatedKFold\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n## Using imblearn library:\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\n\n## Using msno Library for Missing Value analysis:\nimport missingno as msno\n\n## For Metrics:\nfrom sklearn.metrics import plot_precision_recall_curve,accuracy_score \nfrom sklearn.metrics import roc_curve, roc_auc_score, auc\nfrom sklearn.metrics import plot_confusion_matrix, confusion_matrix, classification_report\nfrom sklearn.model_selection import learning_curve\n\n## For Machine Learning Models:\nfrom sklearn.linear_model import LogisticRegression,LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n## For Feature Importance:\n!pip install shap\nimport shap","metadata":{"execution":{"iopub.status.busy":"2023-02-26T14:51:44.471302Z","iopub.execute_input":"2023-02-26T14:51:44.471813Z","iopub.status.idle":"2023-02-26T14:51:55.545534Z","shell.execute_reply.started":"2023-02-26T14:51:44.471770Z","shell.execute_reply":"2023-02-26T14:51:55.544469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Setting the seed to allow reproducibility\nnp.random.seed(31415)","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:48.132804Z","iopub.execute_input":"2023-02-26T13:40:48.133580Z","iopub.status.idle":"2023-02-26T13:40:48.138788Z","shell.execute_reply.started":"2023-02-26T13:40:48.133542Z","shell.execute_reply":"2023-02-26T13:40:48.137768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Exploring the Dataset(EDA)","metadata":{}},{"cell_type":"markdown","source":"## About the Dataset:\n\n### The dataset consists of the following features:\n\n#### Independent Variables:\n\n- Name:\tDescription\n\n- person_age:\tAge of the person \n\n- person_income:\tAnnual Income\n\n- personhomeownership:\tHome ownership\n\n- personemplength:\tEmployment length (in years)\n\n- loan_intent:\tLoan intent\n\n- loan_grade:\tLoan grade\n\n- loan_amnt:\tLoan amount\n\n- loanintrate:\tInterest rate\n\n- loanpercentincome:\tPercent income\n\n- cbpersondefaultonfile:\tHistorical default\n\n- cbpresoncredhistlength:\tCredit history length\n\n#### Target Variable:\n\n- loan_status:\tLoan status (0 is non-default/1 is default)","metadata":{}},{"cell_type":"markdown","source":"### Reading the dataset:","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/credit-risk-dataset/credit_risk_dataset.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:48.140183Z","iopub.execute_input":"2023-02-26T13:40:48.140543Z","iopub.status.idle":"2023-02-26T13:40:48.269890Z","shell.execute_reply.started":"2023-02-26T13:40:48.140511Z","shell.execute_reply":"2023-02-26T13:40:48.268865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape[0],df.shape[1]","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:48.272434Z","iopub.execute_input":"2023-02-26T13:40:48.272793Z","iopub.status.idle":"2023-02-26T13:40:48.279429Z","shell.execute_reply.started":"2023-02-26T13:40:48.272762Z","shell.execute_reply":"2023-02-26T13:40:48.278419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:48.281232Z","iopub.execute_input":"2023-02-26T13:40:48.281615Z","iopub.status.idle":"2023-02-26T13:40:48.312962Z","shell.execute_reply.started":"2023-02-26T13:40:48.281573Z","shell.execute_reply":"2023-02-26T13:40:48.311852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:48.314370Z","iopub.execute_input":"2023-02-26T13:40:48.314699Z","iopub.status.idle":"2023-02-26T13:40:48.357849Z","shell.execute_reply.started":"2023-02-26T13:40:48.314668Z","shell.execute_reply":"2023-02-26T13:40:48.356982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Checking for Duplicates\ndups = df.duplicated()\ndups.value_counts() #There are 165 Duplicated rows","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:48.358874Z","iopub.execute_input":"2023-02-26T13:40:48.359175Z","iopub.status.idle":"2023-02-26T13:40:48.385896Z","shell.execute_reply.started":"2023-02-26T13:40:48.359148Z","shell.execute_reply":"2023-02-26T13:40:48.384810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Removing the Duplicates\nprint(f\"Shape of Data before removing duplicates -----> ({df.shape[0]},{df.shape[1]}) \\n\")\ndf.drop_duplicates(inplace=True)\nprint(f\"Shape of Data after removing duplicates -----> ({df.shape[0]},{df.shape[1]})\")","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:48.386956Z","iopub.execute_input":"2023-02-26T13:40:48.387264Z","iopub.status.idle":"2023-02-26T13:40:48.411832Z","shell.execute_reply.started":"2023-02-26T13:40:48.387237Z","shell.execute_reply":"2023-02-26T13:40:48.410759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ccol=df.select_dtypes(include=[\"object\"]).columns\nncol=df.select_dtypes(include=[\"int\",\"float\"]).columns\n\nprint(\"The number of Categorical columns are:\",len(ccol))\nprint(\"The number of Numerical columns are:\",len(ncol))","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:48.413161Z","iopub.execute_input":"2023-02-26T13:40:48.413557Z","iopub.status.idle":"2023-02-26T13:40:48.424454Z","shell.execute_reply.started":"2023-02-26T13:40:48.413526Z","shell.execute_reply":"2023-02-26T13:40:48.423265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing the different columns with their cardinality (number of unique elements in each column):","metadata":{}},{"cell_type":"code","source":"print(\"The NUMERICAL columns are:\\n\")\nfor i in ncol:\n    print(\"->\",i,\"-\",df[i].nunique())\n    \nprint(\"\\n---------------------------\\n\")\nprint(\"The CATEGORICAL columns are:\\n\")\nfor i in ccol:\n    print(\"->\",i,\"-\",df[i].nunique())","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:48.429082Z","iopub.execute_input":"2023-02-26T13:40:48.429403Z","iopub.status.idle":"2023-02-26T13:40:48.451490Z","shell.execute_reply.started":"2023-02-26T13:40:48.429373Z","shell.execute_reply":"2023-02-26T13:40:48.450469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Analysing the Target variable:","metadata":{}},{"cell_type":"code","source":"df[\"loan_status\"].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:48.452910Z","iopub.execute_input":"2023-02-26T13:40:48.453561Z","iopub.status.idle":"2023-02-26T13:40:48.461919Z","shell.execute_reply.started":"2023-02-26T13:40:48.453527Z","shell.execute_reply":"2023-02-26T13:40:48.461111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Data is highly **IMBALANCED**. We will deal with oversampling techniques like KNN-SMOTE to solve this issue.","metadata":{}},{"cell_type":"markdown","source":"- Checking for Missing Data:","metadata":{}},{"cell_type":"markdown","source":"Missing data, or missing values, occur when you don’t have data stored for certain variables or participants. Data can go missing due to incomplete data entry, equipment malfunctions, lost files, and many other reasons.\n\nThere are typically 3 types of missing values:\n\n1. Missing completely at random (MCAR)\n\n2. Missing at random (MAR)\n\n3. Missing not at random (MNAR)\n\nProblems:\n\nMissing data are problematic because, depending on the type, they can sometimes cause sampling bias. This means your results may not be generalizable outside of your study because your data come from an unrepresentative sample.","metadata":{}},{"cell_type":"code","source":"## Checking for Missing values:\ndf.isnull().any()","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:48.463394Z","iopub.execute_input":"2023-02-26T13:40:48.464085Z","iopub.status.idle":"2023-02-26T13:40:48.480694Z","shell.execute_reply.started":"2023-02-26T13:40:48.464052Z","shell.execute_reply":"2023-02-26T13:40:48.479556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:48.482136Z","iopub.execute_input":"2023-02-26T13:40:48.482622Z","iopub.status.idle":"2023-02-26T13:40:48.498434Z","shell.execute_reply.started":"2023-02-26T13:40:48.482589Z","shell.execute_reply":"2023-02-26T13:40:48.497441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mn=df.isna().sum()\nmn=pd.DataFrame(mn,columns=[\"missing_count\"]).reset_index()\nmn.columns=[\"columns_names\",\"missing_count\"]\nmn\n\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(15,10))\nplots=sns.barplot(x=mn.columns_names,y=mn.missing_count)\n# Iterating over the bars one-by-one\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.2f'),\n                   (bar.get_x() + bar.get_width() / 2,\n                    bar.get_height()), ha='center', va='center',\n                   size=12, xytext=(0, 8),\n                   textcoords='offset points')\nplt.xticks(rotation=45) #Rotating the Label\nplt.title(\"Missing values in every column.\",size=25,weight=\"bold\") #Title of the plot\nplt.xlabel(\"Columns\",fontsize=20) #Title of x-axis\nplt.ylabel(\"Missing_Count\",fontsize=20)\nplt.tick_params(labelsize=10) #Varying label size\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:48.499985Z","iopub.execute_input":"2023-02-26T13:40:48.500322Z","iopub.status.idle":"2023-02-26T13:40:49.057171Z","shell.execute_reply.started":"2023-02-26T13:40:48.500292Z","shell.execute_reply":"2023-02-26T13:40:49.056005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using missing number visualiser to review the missing data:\n\n- On the left side of the plot, the y-axis scale ranges from 0.0 to 1.0, where 1.0 represents 100% data completeness. If the bar is less than this, it indicates that we have missing values within that column.\n\n- On the right side of the plot, the scale is measured in index values. With the top right representing the maximum number of rows within the dataframe.\n\n- Along the top of the plot, there are a series of numbers that represent the total count of the non-null values within that column.","metadata":{}},{"cell_type":"code","source":"msno.bar(df)","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:49.058613Z","iopub.execute_input":"2023-02-26T13:40:49.059668Z","iopub.status.idle":"2023-02-26T13:40:50.071798Z","shell.execute_reply.started":"2023-02-26T13:40:49.059612Z","shell.execute_reply":"2023-02-26T13:40:50.070735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The matrix plot is a great tool if you are working with depth-related data or time-series data. It provides a colour fill for each column. When data is present, the plot is shaded in grey (or your colour of choice), and when it is absent the plot is displayed in white.","metadata":{}},{"cell_type":"code","source":"msno.matrix(df)","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:50.073069Z","iopub.execute_input":"2023-02-26T13:40:50.073399Z","iopub.status.idle":"2023-02-26T13:40:50.785539Z","shell.execute_reply.started":"2023-02-26T13:40:50.073368Z","shell.execute_reply":"2023-02-26T13:40:50.784485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The heatmap is used to identify correlations of the nullity between each of the different columns. \n\nHere no correlation is detected between columns having missing values.","metadata":{}},{"cell_type":"code","source":"msno.heatmap(df)","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:50.786974Z","iopub.execute_input":"2023-02-26T13:40:50.787311Z","iopub.status.idle":"2023-02-26T13:40:51.131562Z","shell.execute_reply.started":"2023-02-26T13:40:50.787279Z","shell.execute_reply":"2023-02-26T13:40:51.130439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Missing values in each target class:\nl=df.groupby([\"loan_status\"])[\"loan_int_rate\"].apply(lambda x:x.isnull().sum())\np=df.groupby([\"loan_status\"])[\"person_emp_length\"].apply(lambda x:x.isnull().sum())\n\nfig,ax=plt.subplots(1,2,figsize=(10,10))\nax[0].set_title(\"Missing values in loan_int_rate\")\nax[0].pie(l.values, labels=l.index, colors=sns.color_palette('bright'), autopct='%.0f%%')\n\nax[1].set_title(\"Missing values in person_emp_length\")\nax[1].pie(p.values, labels=p.index, colors=sns.color_palette('cool'), autopct='%.0f%%')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:51.132704Z","iopub.execute_input":"2023-02-26T13:40:51.133038Z","iopub.status.idle":"2023-02-26T13:40:51.497768Z","shell.execute_reply.started":"2023-02-26T13:40:51.133010Z","shell.execute_reply":"2023-02-26T13:40:51.496132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Still to do:\n\n1. Splitting\n2. Iterative Imputation   ## done after splitting\n3. Oversampling  ## done after splitting","metadata":{}},{"cell_type":"markdown","source":"NOTE: EVERY PREPROCESSING TECHNIQUE IS DONE ONLY ONE THE TRAIN SET. SO SPLITTING IS MANDATORY BEFORE OUTLIER REMOVAL, MISSING VALUES HANDLING, OVERSAMPLING, ETC...","metadata":{}},{"cell_type":"markdown","source":"Time to handle the missing values. First we will split the data:","metadata":{}},{"cell_type":"code","source":"# X and y will represent the entire training data\n# X_test and y_test will be the sample data for model evaluation\n\nX, X_test, y, y_test = train_test_split(df.drop('loan_status', axis=1), df['loan_status'],\n                                        random_state=0,  test_size=0.2, stratify=df['loan_status'],\n                                        shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:51.499569Z","iopub.execute_input":"2023-02-26T13:40:51.500094Z","iopub.status.idle":"2023-02-26T13:40:51.529973Z","shell.execute_reply.started":"2023-02-26T13:40:51.500035Z","shell.execute_reply":"2023-02-26T13:40:51.529161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have stratified sampled the data (checking the proportion of the target variable):","metadata":{}},{"cell_type":"code","source":"y.value_counts(normalize=True)   #Note that the proportion remains the same because of stratify.","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:51.531395Z","iopub.execute_input":"2023-02-26T13:40:51.532149Z","iopub.status.idle":"2023-02-26T13:40:51.542170Z","shell.execute_reply.started":"2023-02-26T13:40:51.532103Z","shell.execute_reply":"2023-02-26T13:40:51.540950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:51.543681Z","iopub.execute_input":"2023-02-26T13:40:51.544565Z","iopub.status.idle":"2023-02-26T13:40:51.554741Z","shell.execute_reply.started":"2023-02-26T13:40:51.544522Z","shell.execute_reply":"2023-02-26T13:40:51.553753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The proportion is preserved!","metadata":{}},{"cell_type":"code","source":"## Checking for missing values in train data:\n(X.shape[0]-X.dropna().shape[0])/X.shape[0]*100\n## There are 12% missing data in the train set","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:51.556124Z","iopub.execute_input":"2023-02-26T13:40:51.556519Z","iopub.status.idle":"2023-02-26T13:40:51.576370Z","shell.execute_reply.started":"2023-02-26T13:40:51.556479Z","shell.execute_reply":"2023-02-26T13:40:51.575323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To print the number of unique values:\nfor col in X:\n    print(col, '--->', X[col].nunique())\n    if X[col].nunique()<20:\n        print(X[col].value_counts(normalize=True)*100)\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:51.577552Z","iopub.execute_input":"2023-02-26T13:40:51.578223Z","iopub.status.idle":"2023-02-26T13:40:51.614397Z","shell.execute_reply.started":"2023-02-26T13:40:51.578178Z","shell.execute_reply":"2023-02-26T13:40:51.613589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing Outliers based on Domain Knowledge:","metadata":{}},{"cell_type":"markdown","source":"- Using common sense, we can exclude rows whose age is >80.","metadata":{}},{"cell_type":"code","source":"X.loc[X['person_age']>=80, :]  ","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:51.615769Z","iopub.execute_input":"2023-02-26T13:40:51.616094Z","iopub.status.idle":"2023-02-26T13:40:51.633612Z","shell.execute_reply.started":"2023-02-26T13:40:51.616063Z","shell.execute_reply":"2023-02-26T13:40:51.632585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape   ## shape before removal","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:51.635194Z","iopub.execute_input":"2023-02-26T13:40:51.636327Z","iopub.status.idle":"2023-02-26T13:40:51.644998Z","shell.execute_reply.started":"2023-02-26T13:40:51.636281Z","shell.execute_reply":"2023-02-26T13:40:51.644026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X.loc[X['person_age']<80, :]","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:51.646156Z","iopub.execute_input":"2023-02-26T13:40:51.646801Z","iopub.status.idle":"2023-02-26T13:40:51.657805Z","shell.execute_reply.started":"2023-02-26T13:40:51.646764Z","shell.execute_reply":"2023-02-26T13:40:51.656975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape  ## shape after removal","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:51.659128Z","iopub.execute_input":"2023-02-26T13:40:51.659455Z","iopub.status.idle":"2023-02-26T13:40:51.668836Z","shell.execute_reply.started":"2023-02-26T13:40:51.659424Z","shell.execute_reply":"2023-02-26T13:40:51.667836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can also exclude rows whose work experience is >60 (Assuming average Upper bound of employement).","metadata":{}},{"cell_type":"code","source":"X.loc[X['person_emp_length']>=60, :]","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:51.674770Z","iopub.execute_input":"2023-02-26T13:40:51.675460Z","iopub.status.idle":"2023-02-26T13:40:51.691685Z","shell.execute_reply.started":"2023-02-26T13:40:51.675427Z","shell.execute_reply":"2023-02-26T13:40:51.690622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X.loc[X['person_emp_length']<60, :]","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:51.693283Z","iopub.execute_input":"2023-02-26T13:40:51.693689Z","iopub.status.idle":"2023-02-26T13:40:51.702036Z","shell.execute_reply.started":"2023-02-26T13:40:51.693649Z","shell.execute_reply":"2023-02-26T13:40:51.701151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since we've removed some data from X, we need to pass on these updations to y as well,\n# as y doesn't know some of its corresponding X's have been deleted.\ny = y[X.index]","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:51.703331Z","iopub.execute_input":"2023-02-26T13:40:51.703656Z","iopub.status.idle":"2023-02-26T13:40:51.711592Z","shell.execute_reply.started":"2023-02-26T13:40:51.703611Z","shell.execute_reply":"2023-02-26T13:40:51.710528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking the distribution of the Target variable:","metadata":{}},{"cell_type":"code","source":"y.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:40:51.713286Z","iopub.execute_input":"2023-02-26T13:40:51.714128Z","iopub.status.idle":"2023-02-26T13:40:51.722198Z","shell.execute_reply.started":"2023-02-26T13:40:51.714094Z","shell.execute_reply":"2023-02-26T13:40:51.721269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part 1:\n\nUse Column Transformer and Pipeline to streamline process\n\nUse Randomized Search to find optimal set of parameters\n\nAutomate the procedure for multiple classifiers\n\nPlot Precision-Recall Curve\n\nPlot Learning Curve (for bias-variance tradeoff / check for overfitting-underfitting)\n\n## Part 2:\n\nRectify existing model based on inferences from the learning curve and make a better one","metadata":{}},{"cell_type":"markdown","source":"### Creating Pipelines:","metadata":{}},{"cell_type":"markdown","source":"The Main Pipeline will be made of **two** parts:\n\n- Preprocessing for **NUMERICAL VARIABLES**:\n\n1. **Iterative imputer** - To handle missing values    \n2. **SMOTE** - To handle imbalance in the dataset\n3. **Scaling** - To maintain the scale among features\n\n- Preprocessing for **CATEGORICAL VARIABLES**:\n\n1. **One Hot Encoder** - To encode each categoric for model interpretability","metadata":{}},{"cell_type":"markdown","source":"The bottom code snippet is a small picture for something big to come. \n\n**NOTE:** It is always important for any Data Scientist to try out their methods and solutions on smaller data(readily available data), before they can actually dwelve into their own dataset. This will make it easier to debug code.","metadata":{}},{"cell_type":"code","source":"## Making artificial data with the help of sklearn's make_classification method:\nX,y=make_classification(10000,20,n_classes=2,weights=[0.75])\n\n## Gives count of each target's class:\nuni,cnt=np.unique(y,return_counts=True)\nfor i,(j,k) in enumerate(zip(uni,cnt)):\n    print(j,\"--\",k)\n\nX=pd.DataFrame(X)\n\n## Function to select random indexes:\ndef create_index(nrows):\n    l=np.random.randint(0,nrows,50)\n    return l\n\n## Making some rows as Null.\nfor i in range(X.shape[1]):\n    X.iloc[create_index(X.shape[0]),i]=None\n    \n## To check the null values in every column:\nX.isnull().sum()\n\n## Splitting:\nX_train,X_test,y_train,y_test=train_test_split(X,y)\n\n## Making sample pipeline\nover = SMOTE() ## Object for smote #sampling_strategy=0.1\nmodel = DecisionTreeClassifier()\nsteps = [('impute', IterativeImputer()),('over', over),('model', model)]\npipeline = Pipeline(steps=steps)\n\npipeline.fit(X_train,y_train)\n\nres=pipeline.predict(X_test)\n\nprint(f\"The accuracy of the Decision Tree Model is:{accuracy_score(y_test,res)}\")","metadata":{"execution":{"iopub.status.busy":"2023-02-26T13:49:20.802023Z","iopub.execute_input":"2023-02-26T13:49:20.802917Z","iopub.status.idle":"2023-02-26T13:49:20.808815Z","shell.execute_reply.started":"2023-02-26T13:49:20.802880Z","shell.execute_reply":"2023-02-26T13:49:20.807864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Keeping a list of numeric and categorical columns:","metadata":{}},{"cell_type":"code","source":"num_cols= X.select_dtypes(include=[np.number]).columns\ncat_cols = X.select_dtypes(exclude=[np.number]).columns","metadata":{"execution":{"iopub.status.busy":"2023-02-26T14:48:39.801507Z","iopub.execute_input":"2023-02-26T14:48:39.801931Z","iopub.status.idle":"2023-02-26T14:48:39.810916Z","shell.execute_reply.started":"2023-02-26T14:48:39.801894Z","shell.execute_reply":"2023-02-26T14:48:39.810098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Original Pipeline:\n\n### 1. Creating a pipeline for numerical columns:\nnum_pipe = Pipeline([\n    ('impute', IterativeImputer()),     #MICE (Multivariate Imputation by Chained Equations)\n    ('scale', StandardScaler()),\n])\n\n### 2. Using the Column Transformer class for binding the above numeric pipeline and\n###    the preprocessing steps of categorical columns:\nct = ColumnTransformer([\n    ('num_pipe', num_pipe, num_cols),\n    ('cat_cols', OneHotEncoder(sparse=False, handle_unknown='ignore'), cat_cols)\n], remainder='passthrough')\n\n\n## Setting up the models to be tested upon and the parameters for their pipelines'::\ngrid = {\n    RandomForestClassifier(random_state=0, class_weight='balanced', n_jobs=-1):\n    {'model__n_estimators':[300,400,500],\n     'coltf__num_pipe__impute__estimator': [LinearRegression(), RandomForestRegressor(random_state=0), #coltf is the name of the final \n                                        KNeighborsRegressor()]},                                       #pipeline. The base estimator of \n                                                                                                       #iterative imputer are also considered\n    LGBMClassifier(class_weight='balanced', random_state=0, n_jobs=-1):                                #parameters\n    {'model__n_estimators':[300,400,500],\n     'model__learning_rate':[0.001,0.01,0.1,1,10],\n     'model__boosting_type': ['gbdt', 'goss', 'dart'],\n     'coltf__num_pipe__impute__estimator':[LinearRegression(), RandomForestRegressor(random_state=0),\n                                        KNeighborsRegressor()]},\n    KNeighborsClassifier(random_state=0, n_jobs=-1):\n    {'model__n_neighbors':[4,5,6,7,8,9],\n     'model__weights':['uniform', 'distance']        \n     'coltf__num_pipe__impute__estimator':[LinearRegression(), RandomForestRegressor(random_state=0),\n                                        KNeighborsRegressor()]},\n    \n    XGBClassifier()\n}","metadata":{"execution":{"iopub.status.busy":"2023-02-26T14:56:08.267771Z","iopub.execute_input":"2023-02-26T14:56:08.268759Z","iopub.status.idle":"2023-02-26T14:56:08.279537Z","shell.execute_reply.started":"2023-02-26T14:56:08.268707Z","shell.execute_reply":"2023-02-26T14:56:08.278398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing out the parameters:","metadata":{}},{"cell_type":"code","source":"for clf, param in grid.items():\n    print(clf)\n    print('-'*50)\n    print(param)\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-02-26T14:57:18.058259Z","iopub.execute_input":"2023-02-26T14:57:18.058680Z","iopub.status.idle":"2023-02-26T14:57:18.067631Z","shell.execute_reply.started":"2023-02-26T14:57:18.058642Z","shell.execute_reply":"2023-02-26T14:57:18.066534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding the optimal model and its best hyperparameters:","metadata":{}},{"cell_type":"code","source":"full_df = pd.DataFrame()\nbest_algos = {}\n\nfor model, param in grid.items():\n    pipe = Pipeline([\n    ('coltf', ct),       #ct for the column transformer for preprocessing\n    ('model', model)\n])\n    \n    ## Conducting a Randomized Search to find the best optimal hyperparamaters:\n    gs = RandomizedSearchCV(estimator=pipe, param_distributions=param, scoring='accuracy',\n                            n_jobs=-1, verbose=3, n_iter=4, random_state=0)\n    \n    gs.fit(X, y)\n    \n    all_res = pd.DataFrame(gs.cv_results_)\n\n    temp = all_res.loc[:, ['params', 'mean_test_score']]\n    algo_name = str(model).split('(')[0]\n    temp['algo'] = algo_name\n    \n    full_df = pd.concat([full_df, temp], ignore_index=True)\n    best_algos[algo_name] = gs.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2023-02-26T14:57:53.835972Z","iopub.execute_input":"2023-02-26T14:57:53.836623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Displaying the best performed model:","metadata":{}},{"cell_type":"code","source":"full_df.sort_values('mean_test_score', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T16:03:09.176638Z","iopub.execute_input":"2021-09-23T16:03:09.177602Z","iopub.status.idle":"2021-09-23T16:03:09.201226Z","shell.execute_reply.started":"2021-09-23T16:03:09.177555Z","shell.execute_reply":"2021-09-23T16:03:09.20005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_df.sort_values('mean_test_score', ascending=False).iloc[0, 0]","metadata":{"execution":{"iopub.status.busy":"2021-09-23T16:14:31.151348Z","iopub.execute_input":"2021-09-23T16:14:31.151791Z","iopub.status.idle":"2021-09-23T16:14:31.164752Z","shell.execute_reply.started":"2021-09-23T16:14:31.151754Z","shell.execute_reply":"2021-09-23T16:14:31.163815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To get the parameters of the best algorithm:\nbe = best_algos['RandomForestClassifier']\nbe","metadata":{"execution":{"iopub.status.busy":"2021-09-23T16:14:54.704364Z","iopub.execute_input":"2021-09-23T16:14:54.704841Z","iopub.status.idle":"2021-09-23T16:14:55.022174Z","shell.execute_reply.started":"2021-09-23T16:14:54.704792Z","shell.execute_reply":"2021-09-23T16:14:55.020906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Repeated k-Fold Cross-Validation** provides a way to improve the estimated performance of a machine learning model. This involves simply repeating the cross-validation procedure multiple times and reporting the mean result across all folds from all runs. This mean result is expected to be a more accurate estimate of the true unknown underlying mean performance of the model on the dataset, as calculated using the standard error.\n\nWe will use this technique here to test model performance once again before evaluating it.","metadata":{}},{"cell_type":"code","source":"## Placed best model into pipeline:\npipe = Pipeline([\n('coltf', ct),    \n('model', be)\n])\n\n# evaluate pipeline using k-fold cross validation:\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\nscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\nscores.mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Fitting:\npipeline.fit(X, y)\n## Getting predictions:\npreds = pipeline.predict(X_test)\n## Getting probabilities:\nprobs = pipeline.predict_proba(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T16:17:11.567223Z","iopub.execute_input":"2021-09-23T16:17:11.567592Z","iopub.status.idle":"2021-09-23T16:18:10.571931Z","shell.execute_reply.started":"2021-09-23T16:17:11.567558Z","shell.execute_reply":"2021-09-23T16:18:10.570437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Learning Curve:","metadata":{}},{"cell_type":"code","source":"def funct_lc(be,X_train,y_train):\n    a, b, c = learning_curve(be, X_train, y_train, n_jobs=-1, scoring='accuracy')\n    plt.plot(a, b.mean(axis=1), label='Training Accuracy')\n    plt.plot(a, c.mean(axis=1),  label='Validation Accuracy')\n    plt.xlabel('Training sample sizes')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n    \nfunct_lc(be,X,y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluating the results of the best model:","metadata":{}},{"cell_type":"code","source":"## Function for confusion matrix:\ndef funct_cm(be,X_test,y_test,preds):\n    ## Simple Confusion Matrix:\n    confusion_matrix(y_test, preds)\n    \n    ## Plotting the Confusion Matrix:\n    plot_confusion_matrix(be, X_test, y_test)\n    \n    ## Classification Report:\n    classification_report(y_test, preds)\n    \n    ## Accuracy Score\n    be.score(X_test, y_test)\n    \nfunct_cm(be,X_test,y_test,preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final Accuracy on test set is 92.27%","metadata":{}},{"cell_type":"markdown","source":"### Plotting the Precision-Recall Curve:","metadata":{}},{"cell_type":"code","source":"def funct_pr(be,X_test,y_test):\n    plot_precision_recall_curve(estimator=be, X=X_test, y=y_test, name='model AUC')\n    baseline = y_test.sum() / len(y_test)\n    plt.axhline(baseline, ls='--', color='r', label=f'Baseline model ({round(baseline,2)})')\n    plt.legend(loc='best')\n    plt.show()\n    \nfunct_pr(be,X_test,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the ROC Curve and calculating AUC:","metadata":{}},{"cell_type":"code","source":"def funct_roc(y_test,preds):\n    fpr, tpr, threshold = roc_curve(y_test, preds)\n    roc_auc = auc(fpr, tpr)\n\n    # method I: plt\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n\n#     # method II: ggplot\n#     from ggplot import *\n#     df = pd.DataFrame(dict(fpr = fpr, tpr = tpr))\n#     ggplot(df, aes(x = 'fpr', y = 'tpr')) + geom_line() + geom_abline(linetype = 'dashed')\n    \nfunct_roc(y_test,preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance:","metadata":{}},{"cell_type":"markdown","source":"**1. Using the embedded FI:**","metadata":{}},{"cell_type":"code","source":"def funct_fi(pipeline,X):\n    fi=pipeline.steps[1][1].feature_importances_  ## Returns the list of feature importance from model inside the pipeline\n    feat_importances = pd.Series(fi, index=X.columns)\n    feat_importances.nlargest(20).plot(kind='barh')\n    \nfunct_fi(pipeline,X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. Using SHAP Package:**","metadata":{}},{"cell_type":"code","source":"def shap_fe(X,y,pipeline):\n    # compute SHAP values\n\n    explainer = shap.TreeExplainer(pipeline.steps[1][1])\n    shap_values = explainer.shap_values(X)\n\n    # explainer = shap.Explainer(model)\n    # shap_values = explainer(X)\n\n    shap.summary_plot(shap_values, X, class_names= [0,1], feature_names = X.columns,show=False)\n\n    vals= np.abs(shap_values[1]).mean(0)\n    df_feature_importance=pd.DataFrame(np.concatenate([np.array(df.columns).reshape(-1,1),vals.reshape(-1,1)],axis=1),columns=[\"Feature\",\"Shap_Scores\"])\n    df_feature_importance = df_feature_importance.sort_values('Shap_Scores',ascending=False)\n    df_feature_importance.reset_index(drop=True,inplace=True)\n    return df_feature_importance\n\nshap_fi=shap_fe(X,y,pipeline)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Printing out the feature importance of each independent variable according to SHAP:\nshap_fi","metadata":{},"execution_count":null,"outputs":[]}]}