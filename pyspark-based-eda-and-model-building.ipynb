{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"font-family: Trebuchet MS; background-color: #58D68D; color: #000000; padding: 12px; line-height: 1.5; font-size:\"> Introduction ðŸŽ»</div>\n\n### <div style=\"font-family: Trebuchet MS; background-color: #F4D03F; color: #000000; padding: 12px; line-height: 1.5;\"> Hey Kagglers!! Today I am gonna share with you a simple tool that you can leverage to speeden up the big data processing involved in your own projects. For freshers/experienced practioners, I believe that it is important for y'all to get a basic understanding of the Spark ecosystem as many data-centric companies are continuing to adopt this technology.<br><br> In this notebook, I have tried to compile all the basic functionalities to get you started with Spark effortlessly.</div>\n\n<div style=\"font-family: Trebuchet MS; background-color: #F5B041; color: #000000; padding: 12px; line-height: 1;\"><h3> Some basic guidelines that I have followed to make this notebook look interactive:</h3><h4><ul style=â€œlist-style-type:squareâ€><li>Whenever there is a definition, I have highlighted it with a  <span style=\"background-color: #2E31FD;font-size: 25px\">ðŸ“£</span></li><br><li>Whenever there is a new function/method, I have highlighted it with a <span style=\"background-color: #2E31FD;font-size: 25px\">ðŸŒ¼</span></li><br><li>Whenever there is a suggestion from my side, I have highlighted it with a <span style=\"background-color: #2E31FD;font-size: 25px\">ðŸ“Œ</span></li></ul></h4></div> ","metadata":{}},{"cell_type":"markdown","source":"### So what are you waiting for! Let's get started with the basics:","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"padding: 12px\"><span style=\"background-color: #2E31FD;font-size: 35px\">ðŸ“£</span> What is Apache Spark in Technical terms.</div>\n\n- Apache Spark is an open-source, distributed data processing and analytics framework designed for large-scale data processing tasks. \n\n- It provides a unified and flexible platform for performing various data processing operations, including batch processing, interactive queries, real-time stream processing, machine learning, and graph processing.","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"padding: 12px\"><span style=\"background-color: #2E31FD;font-size: 35px\">ðŸ“£</span> What is this Apache Spark with a simple analogy? </div>\n\n- Apache Spark is like a supercharged engine for processing and analyzing really big piles of data. Imagine you have a massive amount of information, like a gigantic puzzle with millions of pieces. Trying to solve this puzzle on a single computer could take forever. But Spark lets you use many computers at once, like a team of puzzle solvers, to work on different parts of the puzzle together.\n\n- These \"puzzle solvers\" (computers) can talk to each other and share their findings, making the work faster and more efficient. Spark also keeps everything organized and makes sure that even if one of the \"puzzle solvers\" takes a break or has a problem, the others can still continue working without losing progress.\n\n- In simple words, Apache Spark helps you process huge amounts of data much faster by getting a bunch of computers to work together and collaborate on the job. It's like a team effort that makes solving big data problems much easier and quicker!","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"padding: 12px\"><span style=\"background-color: #2E31FD;font-size: 35px\">ðŸ“£</span> What is PySpark?</div>\n\n- PySpark is the Python API to use Spark, just like Pandas.\n\n- In simple words, PySpark is a special tool that combines the power of many computers with the simplicity of Python to help you handle really big piles of data without breaking a sweat!","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"padding: 12px\"><span style=\"background-color: #2E31FD;font-size: 35px\">ðŸ“£</span> Benefits of using PySpark over Pandas for Data Processing:</div>\n\n#### 1. Scalability and Distributed Computing:\n\n- PySpark is designed for processing large-scale data across clusters of machines. It can handle data sizes that may not fit in memory, as it utilizes distributed computing.\n- Pandas, on the other hand, is designed for single-machine data processing and may struggle with extremely large datasets that exceed available memory.\n\n#### 2. Performance:\n\n- PySpark's in-memory processing and distributed computing can lead to better performance for certain operations on large datasets compared to pandas.\n- While pandas is fast for single-machine operations, PySpark's parallel processing can provide significant performance gains for operations that can be parallelized across multiple nodes.","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"font-family: Trebuchet MS; background-color: #B0E0E6; color: #000000; padding: 12px; line-height: 1.5;\"> Importing Libraries ðŸ“š</div>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\nimport regex as re\nimport os\n\n## Supressing warnings:\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-17T19:47:12.123317Z","iopub.execute_input":"2023-08-17T19:47:12.123738Z","iopub.status.idle":"2023-08-17T19:47:12.130950Z","shell.execute_reply.started":"2023-08-17T19:47:12.123703Z","shell.execute_reply":"2023-08-17T19:47:12.129544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-08-17T19:47:12.133776Z","iopub.execute_input":"2023-08-17T19:47:12.134162Z","iopub.status.idle":"2023-08-17T19:48:05.453745Z","shell.execute_reply.started":"2023-08-17T19:47:12.134125Z","shell.execute_reply":"2023-08-17T19:48:05.452714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## importing essential spark libraries:\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, split, count, when, regexp_replace, isnan, udf\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:07:32.435153Z","iopub.execute_input":"2023-08-17T20:07:32.435580Z","iopub.status.idle":"2023-08-17T20:07:32.441275Z","shell.execute_reply.started":"2023-08-17T20:07:32.435548Z","shell.execute_reply":"2023-08-17T20:07:32.440184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"font-family: Trebuchet MS; background-color: #B0E0E6; color: #000000; padding: 12px; line-height: 1.5;\"> Getting Started with the Analysis ðŸ”¬</div>\n\n\n#### The first step towards your adventure in Spark is to create a Spark Session. It is the entry point to the Spark ecosystem. Once you<br><br>reach the Spark environment via the entry point, you can freely create and manipulate Spark RDDs, Dataframes and Datasets. ","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"background-color: #2E31FD;font-size: 35px\">ðŸ“£</span> What is a RDD?\n\nYou might be wondering what this new term is. Well RDD stands for **Resilient Distributed Dataset**. It is the fundamental data structure of Spark.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #2E31FD;font-size: 35px\">ðŸŒ¼</span> SparkSession.builder()\n\n#### SparkSession will be created using SparkSession.builder() builder patterns::","metadata":{}},{"cell_type":"code","source":"##  Creating a Spark session:\nspark = SparkSession.builder.appName('Sample').getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T19:48:05.552367Z","iopub.execute_input":"2023-08-17T19:48:05.553261Z","iopub.status.idle":"2023-08-17T19:48:11.595162Z","shell.execute_reply.started":"2023-08-17T19:48:05.553218Z","shell.execute_reply":"2023-08-17T19:48:11.593750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Quick glance at the object\nspark","metadata":{"execution":{"iopub.status.busy":"2023-08-17T19:48:11.599238Z","iopub.execute_input":"2023-08-17T19:48:11.602271Z","iopub.status.idle":"2023-08-17T19:48:12.908524Z","shell.execute_reply.started":"2023-08-17T19:48:11.602215Z","shell.execute_reply":"2023-08-17T19:48:12.907589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Here, the spark object acts as the gateway to the Spark ecosystem. \n\n### <span style=\"background-color: #2E31FD;font-size: 35px\">ðŸŒ¼</span> read.csv(), show()\n\n##### Next in order to read the CSV data, we use the **read.csv** functionality:","metadata":{}},{"cell_type":"code","source":"df=spark.read.csv(\"/kaggle/input/food-delivery-dataset/train.csv\",\n                  header=True,\n                  inferSchema=True)\n#  Parameters:\n## - inferSchema parameter ensures that the data formatting stays the same as the original dataframe. If False, then the \n##     columns will be of class string.\n## - header parameter tells that the columns names are provided along with the dataset.\n\n## Displaying the first 5 rows:\ndf.show(5)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:48.142790Z","iopub.execute_input":"2023-08-17T20:33:48.144000Z","iopub.status.idle":"2023-08-17T20:33:49.208803Z","shell.execute_reply.started":"2023-08-17T20:33:48.143947Z","shell.execute_reply":"2023-08-17T20:33:49.207642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #2E31FD;font-size: 35px\">ðŸŒ¼</span> toPandas(), head()","metadata":{}},{"cell_type":"code","source":"## To convert a spark dataframe into a pandas dataframe\ndf.toPandas().head()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:49.874203Z","iopub.execute_input":"2023-08-17T20:33:49.874581Z","iopub.status.idle":"2023-08-17T20:33:53.724123Z","shell.execute_reply.started":"2023-08-17T20:33:49.874550Z","shell.execute_reply":"2023-08-17T20:33:53.723011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As you can see above, Time_taken(min) is the target variable.","metadata":{}},{"cell_type":"markdown","source":"#### Now we have read the csv file into Spark. Lets view the dataframe:","metadata":{}},{"cell_type":"code","source":"## Viewing the type\ntype(df)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:53.725909Z","iopub.execute_input":"2023-08-17T20:33:53.726272Z","iopub.status.idle":"2023-08-17T20:33:53.733248Z","shell.execute_reply.started":"2023-08-17T20:33:53.726240Z","shell.execute_reply":"2023-08-17T20:33:53.732154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #2E31FD;font-size: 35px\">ðŸŒ¼</span> printSchema()\n\n#### Printing the schema of the dataframe","metadata":{}},{"cell_type":"code","source":"## Printing the attributes of the table:\ndf.printSchema()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:53.734767Z","iopub.execute_input":"2023-08-17T20:33:53.735167Z","iopub.status.idle":"2023-08-17T20:33:53.745698Z","shell.execute_reply.started":"2023-08-17T20:33:53.735128Z","shell.execute_reply":"2023-08-17T20:33:53.744466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Displaying the first 5 rows in the form of col-value pairs\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:53.748913Z","iopub.execute_input":"2023-08-17T20:33:53.750043Z","iopub.status.idle":"2023-08-17T20:33:53.818914Z","shell.execute_reply.started":"2023-08-17T20:33:53.749998Z","shell.execute_reply":"2023-08-17T20:33:53.817756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #2E31FD;font-size: 35px\">ðŸŒ¼</span> describe(), summary()","metadata":{}},{"cell_type":"code","source":"## Basic statistics of the data:\ndf.describe()    ### df.summary()\ndf.describe().show()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:53.823458Z","iopub.execute_input":"2023-08-17T20:33:53.823932Z","iopub.status.idle":"2023-08-17T20:33:58.079099Z","shell.execute_reply.started":"2023-08-17T20:33:53.823891Z","shell.execute_reply":"2023-08-17T20:33:58.077648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### NOTE: describe() represents the statistical summary of dataframe but it also uses the string variables","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #2E31FD;font-size: 35px\">ðŸŒ¼</span> count(), columns","metadata":{}},{"cell_type":"code","source":"## Shape of the dataframe is:\ndf.count(),len(df.columns)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:58.080442Z","iopub.execute_input":"2023-08-17T20:33:58.082765Z","iopub.status.idle":"2023-08-17T20:33:58.325478Z","shell.execute_reply.started":"2023-08-17T20:33:58.082720Z","shell.execute_reply":"2023-08-17T20:33:58.324391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #2E31FD;font-size: 35px\">ðŸŒ¼</span> col(), isNull()","metadata":{}},{"cell_type":"code","source":"## Checking for null values:\ndf.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:58.326679Z","iopub.execute_input":"2023-08-17T20:33:58.327190Z","iopub.status.idle":"2023-08-17T20:33:58.986396Z","shell.execute_reply.started":"2023-08-17T20:33:58.327038Z","shell.execute_reply":"2023-08-17T20:33:58.985313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Looks like there are no null values.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #2E31FD;font-size: 35px\">ðŸŒ¼</span> dtypes","metadata":{}},{"cell_type":"code","source":"## Checking the dtypes:\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:58.987625Z","iopub.execute_input":"2023-08-17T20:33:58.988016Z","iopub.status.idle":"2023-08-17T20:33:59.007943Z","shell.execute_reply.started":"2023-08-17T20:33:58.987978Z","shell.execute_reply":"2023-08-17T20:33:59.006451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #2E31FD;font-size: 35px\">ðŸŒ¼</span> select()","metadata":{}},{"cell_type":"code","source":"## To view a few selected columns:\ndf.select([\"ID\",\"Delivery_person_ID\"]).show()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:59.009972Z","iopub.execute_input":"2023-08-17T20:33:59.010813Z","iopub.status.idle":"2023-08-17T20:33:59.174716Z","shell.execute_reply.started":"2023-08-17T20:33:59.010768Z","shell.execute_reply":"2023-08-17T20:33:59.173561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:59.182319Z","iopub.execute_input":"2023-08-17T20:33:59.187529Z","iopub.status.idle":"2023-08-17T20:33:59.209423Z","shell.execute_reply.started":"2023-08-17T20:33:59.187449Z","shell.execute_reply":"2023-08-17T20:33:59.203089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #2E31FD;font-size: 35px\">ðŸŒ¼</span> cast()","metadata":{}},{"cell_type":"markdown","source":"#### The various datatypes that a column can take up are integers, string, double, float, timestamp, etc...\n\n#### To convert a column into:\n\n1. double ---> use DoubleType()\n\n2. int    ---> use IntegerType()\n\n3. float  ---> use FloatType()\n\n4. string ---> use StringType()\n\n5. long   ---> use LongType()\n\n#### all inside the cast() method.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #2E31FD;font-size: 35px\">ðŸŒ¼</span> withColumn()\n\n#### In PySpark, the withColumn() function is widely used and defined as the **transformation function** of the DataFrame\n\n#### which is further\n\n- used to change the value, \n\n- convert the datatype of an existing column, \n\n- create the new column etc...","metadata":{}},{"cell_type":"code","source":"## Have to correct the datatypes of some columns. Delivery_person_Age, Vehicle_condition, multiple_deliveries\ndf=df.withColumn('Delivery_person_Age',col('Delivery_person_Age').cast(IntegerType()))\\\n.withColumn('Vehicle_condition',col('Vehicle_condition').cast(IntegerType()))\\\n.withColumn('multiple_deliveries',col('multiple_deliveries').cast(IntegerType()))","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:59.212530Z","iopub.execute_input":"2023-08-17T20:33:59.213266Z","iopub.status.idle":"2023-08-17T20:33:59.270496Z","shell.execute_reply.started":"2023-08-17T20:33:59.213222Z","shell.execute_reply":"2023-08-17T20:33:59.269261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Checking after conversion:\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:59.271757Z","iopub.execute_input":"2023-08-17T20:33:59.272189Z","iopub.status.idle":"2023-08-17T20:33:59.308377Z","shell.execute_reply.started":"2023-08-17T20:33:59.272146Z","shell.execute_reply":"2023-08-17T20:33:59.306180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.select(['Delivery_person_Age','Vehicle_condition','multiple_deliveries']).dtypes","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:59.312601Z","iopub.execute_input":"2023-08-17T20:33:59.313532Z","iopub.status.idle":"2023-08-17T20:33:59.336941Z","shell.execute_reply.started":"2023-08-17T20:33:59.313466Z","shell.execute_reply":"2023-08-17T20:33:59.335840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To display the PySpark dataframe as a pandas dataframe:\ndf.toPandas().head()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:33:59.342262Z","iopub.execute_input":"2023-08-17T20:33:59.345108Z","iopub.status.idle":"2023-08-17T20:34:02.763130Z","shell.execute_reply.started":"2023-08-17T20:33:59.345059Z","shell.execute_reply":"2023-08-17T20:34:02.762028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Checking the numeric columns:\ndef num_cols(dataframe):\n    num_cols = [col for col in dataframe.columns if dataframe.select(col).dtypes[0][1] in ['double', 'int']]\n    return num_cols\n\nnum_cols = num_cols(df)  ### list of numeric columns\n    \ndf.describe(num_cols).show()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:34:02.764328Z","iopub.execute_input":"2023-08-17T20:34:02.765122Z","iopub.status.idle":"2023-08-17T20:34:03.574784Z","shell.execute_reply.started":"2023-08-17T20:34:02.765085Z","shell.execute_reply":"2023-08-17T20:34:03.573578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #2E31FD;font-size: 35px\">ðŸŒ¼</span> distinct()","metadata":{}},{"cell_type":"code","source":"### There are 1320 unique IDs\ndf.select('Delivery_person_ID').distinct().count()  ","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:34:03.576059Z","iopub.execute_input":"2023-08-17T20:34:03.577243Z","iopub.status.idle":"2023-08-17T20:34:04.163632Z","shell.execute_reply.started":"2023-08-17T20:34:03.577188Z","shell.execute_reply":"2023-08-17T20:34:04.162215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #2E31FD;font-size: 35px\">ðŸŒ¼</span> orderBy()","metadata":{}},{"cell_type":"code","source":"### Counts of unique delivery person ids::\ndf.select('Delivery_person_ID').distinct().show()  ### 20 \ndf.groupBy('Delivery_person_ID').count().orderBy('count').show()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:34:04.169370Z","iopub.execute_input":"2023-08-17T20:34:04.170828Z","iopub.status.idle":"2023-08-17T20:34:05.330692Z","shell.execute_reply.started":"2023-08-17T20:34:04.170777Z","shell.execute_reply":"2023-08-17T20:34:05.329549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 10px;\n              color:white;\">\n            Feature Engineering Overview\n        </p>\n    </div></h2>","metadata":{}},{"cell_type":"markdown","source":"As observed from the above dataset, we can extract the following:\n\n1. City from Delivery_person_ID ----> city\n\n2. Bucket cities into Zones - North, South, East, West  ----> city_zone\n\n3. Time taken to pick up delivery using Time_Orderd and Time_Order_picked ----> pickup_time\n\n4. Time of the day - Morning, Lunch, Evening, Night, Midnight ----> day_zone\n\n5. To clean up target variable - Time_taken(min)\n\n6. Bucket Age - Delivery_person_Age ----> life_stage\n\n7. Features using Latitude and Longitude ----> geosidic","metadata":{}},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">1. City from delivery id:</p></blockquote>\n\n#### In order to apply a function into a particular column, we have create the function and register it as a UDF(User Defined Function) on Spark","metadata":{}},{"cell_type":"code","source":"# Create custom function\ndef city_extract(x):\n    return re.findall(\"(\\S+)RES\\S+\",x)[0]\n\n# Convert the function as a UDF using the udf function:\ncity_extract_UDF = udf(lambda x:city_extract(x),StringType()) \n\n# Apply the function on the desired column:\ndf=df.withColumn(\"City\",city_extract_UDF(col(\"Delivery_person_ID\")))\n\n## Having a glance at the new column:\ndf.select(['Delivery_person_ID','City']).show()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:34:05.331909Z","iopub.execute_input":"2023-08-17T20:34:05.332297Z","iopub.status.idle":"2023-08-17T20:34:05.827459Z","shell.execute_reply.started":"2023-08-17T20:34:05.332260Z","shell.execute_reply":"2023-08-17T20:34:05.826318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.select(\"City\").distinct().show(22)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:37:38.706076Z","iopub.execute_input":"2023-08-17T20:37:38.706498Z","iopub.status.idle":"2023-08-17T20:37:39.928777Z","shell.execute_reply.started":"2023-08-17T20:37:38.706452Z","shell.execute_reply":"2023-08-17T20:37:39.927702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">3. Getting Pickup time:</p></blockquote>","metadata":{}},{"cell_type":"code","source":"## equivalent value counts in python:\n## Looks like there are ~1700 rows of null values in this column.\ndf.groupBy('Time_Orderd').count().sort(col(\"count\").desc()).show(10)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:46:01.701587Z","iopub.execute_input":"2023-08-17T20:46:01.702075Z","iopub.status.idle":"2023-08-17T20:46:02.081649Z","shell.execute_reply.started":"2023-08-17T20:46:01.702041Z","shell.execute_reply":"2023-08-17T20:46:02.080545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupBy('Time_Order_picked').count().sort(col(\"count\").desc()).show(10)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:45:14.327425Z","iopub.execute_input":"2023-08-17T20:45:14.327877Z","iopub.status.idle":"2023-08-17T20:45:14.705409Z","shell.execute_reply.started":"2023-08-17T20:45:14.327844Z","shell.execute_reply":"2023-08-17T20:45:14.704240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    ðŸ“Œ &nbsp; A go-to approach will be to calculate average pickup time using other non null rows and then imputing the null rows with the average obtained.\n</div>","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">5. Cleaning the target variable:</p></blockquote>\n\n#### Use withColumnRenamed method to rename a column.","metadata":{}},{"cell_type":"code","source":"## Before transformation:\ndf.select(\"Time_taken(min)\").show(5)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:34:36.429790Z","iopub.execute_input":"2023-08-17T20:34:36.430275Z","iopub.status.idle":"2023-08-17T20:34:36.518410Z","shell.execute_reply.started":"2023-08-17T20:34:36.430238Z","shell.execute_reply":"2023-08-17T20:34:36.517106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Renaming the column name::\ndf=df.withColumnRenamed('Time_taken(min)','time_taken')\n\n## Removing the preffix (i.e. '(min)') in the column values with the help of a UDF:\ndef target_clean(x):\n    return x[-2:]\n\ntarget_clean_udf=udf(lambda x:target_clean(x),StringType())\ndf=df.withColumn(\"time_taken\",target_clean_udf(col(\"time_taken\")))\n## Converting type:\ndf=df.withColumn(\"time_taken\",col(\"time_taken\").cast(IntegerType()))","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:34:39.455770Z","iopub.execute_input":"2023-08-17T20:34:39.456197Z","iopub.status.idle":"2023-08-17T20:34:39.507656Z","shell.execute_reply.started":"2023-08-17T20:34:39.456162Z","shell.execute_reply":"2023-08-17T20:34:39.506693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## As you can see, the values have been cleaned and the type has been changed:\ndf.select(\"time_taken\").show(5),df.select(\"time_taken\").dtypes","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:35:26.383056Z","iopub.execute_input":"2023-08-17T20:35:26.383461Z","iopub.status.idle":"2023-08-17T20:35:26.754811Z","shell.execute_reply.started":"2023-08-17T20:35:26.383432Z","shell.execute_reply":"2023-08-17T20:35:26.753783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"7. \n\n# from geopy.distance import geodesic \n\n# train['distance_diff_KM']=np.zeros(len(train))\n# restaurant_cordinates_train=train[['Restaurant_latitude','Restaurant_longitude']].to_numpy()\n# delivery_location_cordinates_train=train[['Delivery_location_latitude','Delivery_location_longitude']].to_numpy()\n\n# for i in range(len(train)):\n#     train['distance_diff_KM'].loc[i]=geodesic(restaurant_cordinates_train[i],delivery_location_cordinates_train[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}