{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"font-family: Trebuchet MS; background-color: #58D68D; color: #000000; padding: 12px; line-height: 1.5; font-size:\"> Introduction 🎻</div>\n\n### <div style=\"font-family: Trebuchet MS; background-color: #F4D03F; color: #000000; padding: 12px; line-height: 1.5;\"> Hey Kagglers!! Today I am gonna share with you a simple tool that you can leverage to speeden up the big data processing involved in your own projects. For freshers/experienced practioners, I believe that it is important for y'all to get a basic understanding of the Spark ecosystem as many data-centric companies are continuing to adopt this technology.<br><br> In this notebook, I have tried to compile all the basic functionalities to get you started with Spark effortlessly.</div>\n\n<div style=\"font-family: Trebuchet MS; background-color: #EAECEE; color: #000000; padding: 12px; line-height: 1;\"><h3> Some basic guidelines that I have followed to make this notebook look interactive:</h3><h4><ul style=“list-style-type:square”><li>Whenever there is a definition, I have highlighted it with a  <span style=\"background-color: #2E31FD;font-size: 25px\">📣</span></li><br><li>Whenever there is a new function/method, I have highlighted it with a <span style=\"background-color: #00FF00;font-size: 25px\">🌼</span></li><br><li>Whenever there is a suggestion from my side, I have highlighted it with a <span style=\"background-color: #F3FF00;font-size: 25px\">📌</span></li></ul></h4></div> ","metadata":{}},{"cell_type":"markdown","source":"### So what are you waiting for! Let's get started with the basics:","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"padding: 12px\"><span style=\"background-color: #2E31FD;font-size: 35px\">📣</span> What is Apache Spark in Technical terms.</div>\n\n- Apache Spark is an open-source, distributed data processing and analytics framework designed for large-scale data processing tasks. \n\n- It provides a unified and flexible platform for performing various data processing operations, including batch processing, interactive queries, real-time stream processing, machine learning, and graph processing.","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"padding: 12px\"><span style=\"background-color: #2E31FD;font-size: 35px\">📣</span> What is this Apache Spark with a simple analogy? </div>\n\n- Apache Spark is like a supercharged engine for processing and analyzing really big piles of data. Imagine you have a massive amount of information, like a gigantic puzzle with millions of pieces. Trying to solve this puzzle on a single computer could take forever. But Spark lets you use many computers at once, like a team of puzzle solvers, to work on different parts of the puzzle together.\n\n- These \"puzzle solvers\" (computers) can talk to each other and share their findings, making the work faster and more efficient. Spark also keeps everything organized and makes sure that even if one of the \"puzzle solvers\" takes a break or has a problem, the others can still continue working without losing progress.\n\n- In simple words, Apache Spark helps you process huge amounts of data much faster by getting a bunch of computers to work together and collaborate on the job. It's like a team effort that makes solving big data problems much easier and quicker!","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"padding: 12px\"><span style=\"background-color: #2E31FD;font-size: 35px\">📣</span> What is PySpark?</div>\n\n- PySpark is the Python API to use Spark, just like Pandas.\n\n- In simple words, PySpark is a special tool that combines the power of many computers with the simplicity of Python to help you handle really big piles of data without breaking a sweat!","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"padding: 12px\"><span style=\"background-color: #2E31FD;font-size: 35px\">📣</span> Benefits of using PySpark over Pandas for Data Processing:</div>\n\n#### 1. Scalability and Distributed Computing:\n\n- PySpark is designed for processing large-scale data across clusters of machines. It can handle data sizes that may not fit in memory, as it utilizes distributed computing.\n- Pandas, on the other hand, is designed for single-machine data processing and may struggle with extremely large datasets that exceed available memory.\n\n#### 2. Performance:\n\n- PySpark's in-memory processing and distributed computing can lead to better performance for certain operations on large datasets compared to pandas.\n- While pandas is fast for single-machine operations, PySpark's parallel processing can provide significant performance gains for operations that can be parallelized across multiple nodes.","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"font-family: Trebuchet MS; background-color: #B0E0E6; color: #000000; padding: 12px; line-height: 1.5;\"> Importing Libraries 📚</div>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\nimport regex as re\nimport os\nfrom IPython.display import Image,display\n\n## Supressing warnings:\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyspark","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## importing essential spark libraries:\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, split, count, when, regexp_replace, udf, struct, lit, isnull, trim, asc, desc, round, mean\nfrom pyspark.sql.functions import to_timestamp, to_date, unix_timestamp, date_format, hour   ### --> Date manipulation\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"font-family: Trebuchet MS; background-color: #B0E0E6; color: #000000; padding: 12px; line-height: 1.5;\"> Getting Started with the Analysis 🔬</div>\n\n\n#### The first step towards your adventure in Spark is to create a Spark Session. It is the entry point to the Spark ecosystem. Once you<br><br>reach the Spark environment via the entry point, you can freely create and manipulate Spark RDDs, Dataframes and Datasets. ","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"background-color: #2E31FD;font-size: 35px\">📣</span> What is a RDD?\n\nYou might be wondering what this new term is. Well RDD stands for **Resilient Distributed Dataset**. It is the fundamental data structure of Spark.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> SparkSession.builder()\n\n#### SparkSession will be created using SparkSession.builder() builder patterns::","metadata":{}},{"cell_type":"code","source":"##  Creating a Spark session:\nspark = SparkSession.builder.appName('Sample').getOrCreate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Quick glance at the object\nspark","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Here, the spark object acts as the gateway to the Spark ecosystem. \n\n### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> read.csv(), show()\n\nTo read a csv file.","metadata":{}},{"cell_type":"code","source":"df=spark.read.csv(\"/kaggle/input/food-delivery-dataset/train.csv\",\n                  header=True,\n                  inferSchema=True)\n#  Parameters:\n## - inferSchema parameter ensures that the data formatting stays the same as the original dataframe. If False, then the \n##     columns will be of class string.\n## - header parameter tells that the columns names are provided along with the dataset.\n\n## Displaying the first 5 rows:\ndf.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> first()\n\nTo view the data points of the first row.","metadata":{}},{"cell_type":"code","source":"df.first()   ### returns a Row object","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> toPandas(), head()\n\nTo view the dataframe in the form of Pandas dataframe fashion.","metadata":{}},{"cell_type":"code","source":"## To convert a spark dataframe into a pandas dataframe\ndf.toPandas().head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As you can see above, Time_taken(min) is the target variable.","metadata":{}},{"cell_type":"markdown","source":"#### Now we have read the csv file into Spark. Lets view the dataframe:","metadata":{}},{"cell_type":"code","source":"## Viewing the type\ntype(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> printSchema()\n\nTo print the schema of the dataset.","metadata":{}},{"cell_type":"code","source":"## Printing the attributes of the table:\ndf.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Displaying the first 5 rows in the form of col-value pairs\ndf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> describe(), summary()\n\nTo view the basic statistics of the dataset.","metadata":{}},{"cell_type":"code","source":"## Basic statistics of the data:\ndf.describe()    ### df.summary()\ndf.describe().show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### NOTE: describe() represents the statistical summary of dataframe but it also uses the string variables","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> count(), columns\n\nTo count the number of rows present, To display the various columns present in the dataframe.","metadata":{}},{"cell_type":"code","source":"## Shape of the dataframe is:\ndf.count(),len(df.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> col(), isNull(), alias()\n\nTo select a particular column for applying a transformation, To check whether a column has Null values, To rename a column after a transformation.","metadata":{}},{"cell_type":"code","source":"## Checking for null values:\ndf.select([count(when(df[c].isNull(), c)).alias(c) for c in df.columns]).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #F3FF00;font-size: 35px\">📌</span>Breaking down the above query by taking one sample column:","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> when()-otherwise()\n\nTo fill in values of a column based on a condition.","metadata":{}},{"cell_type":"code","source":"## Then when-otherwise pair effectively works as the CASE WHEN THEN ELSE END expression of SQL:\ndf.select((when(df['Weatherconditions'].isNull(),\"None\").otherwise(df['Weatherconditions']))).show(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Placing the alias changes the column name:\ndf.select((when(df['Weatherconditions'].isNull(),\"None\").otherwise(df['Weatherconditions'])).alias('Weatherconditions')).show(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Placing a count() function returns the number of empty/None/Null rows:\ndf.select(count(when(df['Weatherconditions'].isNull(),\"None\")).alias(\"Count_Null_Weather\")).show(2)\n\n## Automating this expression for multiple columns using the list comprehension will yield the desired output.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Looks like there are no null values.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> dtypes\n\nTo view the datatypes of a column(s).","metadata":{}},{"cell_type":"code","source":"## Checking the dtypes:\ndf.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> select()\n\nTo select columns for display.","metadata":{}},{"cell_type":"code","source":"## To view a few selected columns:\ndf.select([\"ID\",\"Delivery_person_ID\"]).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> cast()\n\nTo change the datatype of a column(s).","metadata":{}},{"cell_type":"markdown","source":"#### The various datatypes that a column can take up are integers, string, double, float, timestamp, etc...\n\n#### To convert a column into:\n\n1. double ---> use DoubleType()\n\n2. int    ---> use IntegerType()\n\n3. float  ---> use FloatType()\n\n4. string ---> use StringType()\n\n5. long   ---> use LongType()\n\n#### all inside the cast() method.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> withColumn()\n\n#### In PySpark, the withColumn() function is widely used and defined as the **transformation function** of the DataFrame\n\n#### which is further\n\n- used to change the value, \n\n- convert the datatype of an existing column, \n\n- create the new column etc...","metadata":{}},{"cell_type":"code","source":"## Have to correct the datatypes of some columns. Delivery_person_Age, Vehicle_condition, multiple_deliveries\ndf=df.withColumn('Delivery_person_Age',col('Delivery_person_Age').cast(IntegerType()))\\\n.withColumn('Vehicle_condition',col('Vehicle_condition').cast(IntegerType()))\\\n.withColumn('multiple_deliveries',col('multiple_deliveries').cast(IntegerType()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Checking after conversion:\ndf.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.select(['Delivery_person_Age','Vehicle_condition','multiple_deliveries']).dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To display the PySpark dataframe as a pandas dataframe:\ndf.toPandas().head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Checking the numeric columns:\ndef num_cols(dataframe):\n    num_cols = [col for col in dataframe.columns if dataframe.select(col).dtypes[0][1] in ['double', 'int']]\n    return num_cols\n\nnum_cols = num_cols(df)  ### list of numeric columns\n    \ndf.describe(num_cols).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> distinct()\n\nTo get the unique values.","metadata":{}},{"cell_type":"code","source":"### There are 1320 unique IDs\ndf.select('Delivery_person_ID').distinct().count()  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> orderBy()\n\nTo sort a column(s).","metadata":{}},{"cell_type":"code","source":"### Counts of unique delivery person ids::\ndf.select('Delivery_person_ID').distinct().show(5)  ### 20 \ndf.groupBy('Delivery_person_ID').count().orderBy('count').show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#DE3163;font-size:110%;font-family:Verdana;letter-spacing:0.5px\"><p style=\"padding: 10px;color:white;\">Visualising the Distributions</p></div>","metadata":{}},{"cell_type":"code","source":"#Visualisating the distribution of the categorical variables:\ncols = ['Delivery_person_Age','Delivery_person_Ratings','Weatherconditions','Road_traffic_density','multiple_deliveries','Festival','City']\nnum_plots = len(cols)\nnum_rows = (num_plots // 2) + (num_plots % 2)\n\nfig, axes = plt.subplots(num_rows, 2, figsize=(20,15))\n\nfor i, column_name in enumerate(cols):\n    row = i // 2\n    col = i % 2\n\n    ax = axes[row, col]\n    sns.countplot(data=df.toPandas(), x=column_name, order=df.toPandas()[column_name].value_counts().sort_index().index, ax=ax)\n\n    ax.set_xlabel(column_name)\n    ax.set_ylabel('No. of Orders')\n    ax.set_title(column_name)\n    ax.tick_params(axis='x', rotation=45)\n    \nif num_plots % 2 != 0:\n    fig.delaxes(axes[-1, -1])\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The Delivery_person_Age column contains Age=0 values which are clearly outliers.","metadata":{}},{"cell_type":"markdown","source":"### Looks like we have to handle all the missing values. We can notice that although the data showed no missing values initially, it seems that the NaN values have been represented as a string, hence the isNa() function wasnt able to detect it earlier. \n\n### We can also bin categories as well!","metadata":{}},{"cell_type":"code","source":"spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\naddress = [(1,\"14851 Jeffrey Rd\",\"DE\"),\n    (2,\"Address nan \",\"NY\"),\n    (3,\"13111 Siemon Ave\",\"CA\"),\n    (4,\"bougain nan \",\"WA\")]\nsample =spark.createDataFrame(address,[\"id\",\"address\",\"state\"])\nsample.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> trim()\n\nTo remove trailing spaces.","metadata":{}},{"cell_type":"code","source":"### Stripping all the white space present in categorical columns:\ncat=[i for i in df.columns if df.select(i).dtypes[0][1] in ('string')]\nfor i in cat:\n    df=df.withColumn(i,trim(df[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## checking whether rows are trimmed:\ndf.show(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> regexp_replace()\n\nTo replace characters of a string.","metadata":{}},{"cell_type":"markdown","source":"#### Here we are replacing the \"NaN\" string values with \"None\"","metadata":{}},{"cell_type":"code","source":"cols = ['Delivery_person_Age','Delivery_person_Ratings','Weatherconditions','Road_traffic_density','multiple_deliveries','Festival','City']\nfor i in cols:\n    ## Displaying the changes realtime\n    ## NOTE: THE PARAMETER OF THE FUNCTION DOES NOT SUPPORT SQL-LIKE STRING MATCHING LIKE %,_,etc...\n    df.withColumn(f\"{i}_new\",regexp_replace(i,\"^(.*?)NaN\",\"None\")).select(f\"{i}_new\").distinct().show(5)\n    ## Replacing the dataframe:\n    df=df.withColumn(i,regexp_replace(i,\"^(.*?)NaN\",\"None\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now that we have temporarily replaced the NaN values with \"None\", we will treat them later one-by-one in the upcoming sections.","metadata":{}},{"cell_type":"code","source":"df.select(df.Weatherconditions).distinct().show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#5642C5;font-size:110%;font-family:Verdana;letter-spacing:0.5px\"><p style=\"padding: 10px;color:white;\">Feature Engineering Overview</p></div>\n    \nAs observed from the above dataset, we can extract the following:\n\n1. City from Delivery_person_ID ----> city  ✅\n\n2. Bucket cities into Zones - North, South, East, West  ----> city_zone  ✅\n\n3. Cleaning the Weatherconditions column ✅\n\n4. Removing Zero-Aged delivery agents ✅\n\n5. Time taken to process and package the delivery using Time_Orderd and Time_Order_picked ----> processing_time ✅\n\n6. Time of the day - Morning, Lunch, Evening, Night, Midnight ----> day_zone ✅\n\n7. To clean up target variable - Time_taken(min)  ✅\n\n8. Bucket Age - Delivery_person_Age ----> life_stage ✅\n\n9. Features using Latitude and Longitude ----> geosidic\n\n10. Handle NaN values in all other column","metadata":{}},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">1. City from delivery id:</p></blockquote>\n\n### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> udf()\n\n#### In order to apply a function into a particular column, we have create the function and register it as a UDF(User Defined Function) on Spark.\n\n#### It is imported from the pyspark.sql.functions module.","metadata":{}},{"cell_type":"code","source":"# Create custom function\ndef city_extract(x):\n    return re.findall(\"(\\S+)RES\\S+\",x)[0]\n\n# Convert the function as a UDF using the udf function:\ncity_extract_UDF = udf(lambda x:city_extract(x),StringType()) \n\n# Apply the function on the desired column:\ndf=df.withColumn(\"City_code\",city_extract_UDF(df[\"Delivery_person_ID\"]))\n\n## Having a glance at the new column:\ndf.select(['Delivery_person_ID','City_code']).show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## There are 22 unique city codes in our data:\ndf.select(\"City_code\").distinct().show(22)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #F3FF00;font-size: 35px\">📌</span> There are three ways by which you can apply a custom function to rows of a spark dataframe:\n\n- User Defined Functions\n- Map functions\n- Custom Spark-native functions","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> groupBy(), sort()--asc()/desc()\n\nTo group by the data based on column(s), To sort based on column(s) in ascending/descending fashion.","metadata":{}},{"cell_type":"code","source":"## To get count of the distinct cities:: (equivalent to value_counts() method in pandas)\ndf.groupBy(\"City_code\").count().sort(desc(\"count\")).show(22)  ### orderBy(desc(col(\"count\")))  ## orderBy(desc(\"count\")) ## orderBy('count', ascending=False) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> rdd.flatMap().collect()\n\nMethods to convert a pyspark column into a list/array:","metadata":{}},{"cell_type":"code","source":"df.select(\"City_code\").distinct().rdd.flatMap(lambda x: x).collect()  ### to convert a column into a list\ndf.select(\"City_code\").distinct().toPandas().values.flatten()  ### to convert a column into a numpy array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> withColumnRenamed()\n\nTo rename a column.","metadata":{}},{"cell_type":"code","source":"## Renaming the column to avoid name clash:\ndf=df.withColumnRenamed(\"City_code\",\"City_short_form\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Checking to see if change has reflected:\ndf.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Created a manual list of the full form of the city encodings:\ndic_city={\"LUDH\":\"Ludhiana\",\n\"CHEN\":\"Chennai\",\n\"KOC\":\"Kochi\",\n\"GOA\":\"Goa\",\n\"AURG\":\"Aurangabad\",\n\"JAP\":\"Jaipur\",\n\"DEH\":\"Delhi\",\n\"MUM\":\"Mumbai\",\n\"AGR\":\"Agra\",\n\"SUR\":\"Surat\",\n\"INDO\":\"Indore\",\n\"PUNE\":\"Pune\",\n\"ALH\":\"Allahabad\",\n\"MYS\":\"Mysore\",\n\"COIMB\":\"Coimbatore\",\n\"HYD\":\"Hyderabad\",\n\"VAD\":\"Vadodara\",\n\"RANCHI\":\"Ranchi\",\n\"BHP\":\"Bhopal\",\n\"KOL\":\"Kolkatta\",\n\"KNP\":\"Kanpur\",\n\"BANG\":\"Bangalore\"}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NOTE: You cannot pass dictionaries as a parameter for a UDF! Hence the below code cell will raise an error:","metadata":{}},{"cell_type":"code","source":"## Creating a udf to map the encodings with their original names:\ndef city_map(x,dic):\n    return dic[x['City_short_form']]\nudf_city_map=udf(lambda x:city_map(x,dic),StringType())\ndf=df.withColumn(\"City\",udf_city_map(\"City_short_form\"))\n\n## Raises Error\n# df.City.show(12)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hence you have to make a small change in the way you define the function by creating a nested function for indirectly passing the dictionary as a parameter to the UDF:","metadata":{}},{"cell_type":"code","source":"def get_city(mapping):\n    def f(x):\n        return mapping.get(x)\n    return udf(f)\n\ndf=df.withColumn('City', get_city(dic_city)('City_short_form'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Checking the dataset for the new column:\ndf.select(\"City\").show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">2. Bucketing cities into various Zones - North, South, East, West:</p></blockquote>","metadata":{}},{"cell_type":"code","source":"## NOTE: THIS IS COMPLETELY BASED ON MY INTUTION. IF YOU FEEL LIKE SOMETHING IS OUT OF PLACE, PLZ CORRECT THIS IN YOUR OWN ANALYSIS:\ndic_zones={\"Ludhiana\":\"North\",\n\"Chennai\":\"South\",\n\"Kochi\":\"South\",\n\"Goa\":\"West\",\n\"Aurangabad\":\"West\",\n\"Jaipur\":\"North\",\n\"Delhi\":\"North\",\n\"Mumbai\":\"West\",\n\"Agra\":\"North\",\n\"Surat\":\"East\",\n\"Indore\":\"Central\",\n\"Pune\":\"West\",\n\"Allahabad\":\"North\",\n\"Mysore\":\"South\",\n\"Coimbatore\":\"South\",\n\"Hyderabad\":\"South\",\n\"Vadodara\":\"West\",\n\"Ranchi\":\"North\",\n\"Bhopal\":\"North\",\n\"Kolkatta\":\"East\",\n\"Kanpur\":\"North\",\n\"Bangalore\":\"South\"}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_zone(mapping):\n    def f(x):\n        return mapping.get(x)\n    return udf(f)\n\ndf=df.withColumn('city_zone', get_zone(dic_zones)('City'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Checking the new columns:\ndf.select([\"City\",\"city_zone\"]).show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">3. Cleaning the Weatherconditions column:</p></blockquote>","metadata":{}},{"cell_type":"code","source":"df.select(\"Weatherconditions\").show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupBy(\"Weatherconditions\").count().sort(desc(\"count\")).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like there are None values as well. We will have to clean those **616** data points.\n\n### To clean this, we will just randomly fill it with any weather as the distributions for all weather conditions are uniform.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> filter(), sample()\n\nIn order to get a sample, we use the sample() function:","metadata":{}},{"cell_type":"markdown","source":"#### collect() method returns a list of row objects. In order to get an attribute from a row object, we use the index of the row, followed by the \\_\\_getitem\\_\\_(<col_name>) magic method.","metadata":{}},{"cell_type":"code","source":"### list of unique weathers\nweather=[i.__getitem__('Weatherconditions') for i in df.filter(df['Weatherconditions']!=\"None\").select('Weatherconditions').distinct().collect()]\nweather","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.select('Weatherconditions').show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> alias()\n\nTo create a copy of a table.","metadata":{}},{"cell_type":"code","source":"## creating a copy:  ### For testing.\ndf2=df.alias('df2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Checking the working of the sample() method:\ndef random_weather(x):\n    ind=np.random.randint(6)\n    k=weather[ind]\n    return k\n\n## Creating a udf:\nudf_random_weather=udf(lambda x:random_weather(x),StringType())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.withColumn('Weatherconditions',when(df['Weatherconditions']==\"None\",udf_random_weather(df['Weatherconditions'])).otherwise(df['Weatherconditions']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Now we have successfully replaced the None with random values\ndf.groupBy('Weatherconditions').count().orderBy(desc('count')).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now it seems that the None values have been replaced.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">4. Removing Zero Aged delivery agents:</p></blockquote>","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Around 4% of the delivery agents dont have an age:\ndf.filter(df[\"Delivery_person_Age\"]==0).count()/df.count() * 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Seems to be that there is no correlation between 'Delivery_person_Age' and any other columns, \n## apart from 'Time_Orderd', as viewed through naked eyes\ndf.filter(df[\"Delivery_person_Age\"]==0).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Removing them from the dataset:\ndf=df.filter(df[\"Delivery_person_Age\"]!=0)\n\n## Verifying counts:\ndf.count()\ndf.filter(df[\"Delivery_person_Age\"]==0).count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## There are order dates which are NaNs:\ndf.filter(df[\"Time_Orderd\"]==np.NaN).count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### As you can see, there are \ndf.filter(df[\"Time_Orderd\"]==np.NaN).select(['Delivery_person_Age','Time_Orderd']).show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.filter(df[\"Time_Orderd\"]==np.NaN).select(['Delivery_person_Age','Time_Orderd']).groupby(df['Delivery_person_Age']).count().orderBy(desc('count')).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">5. Getting Processing time:</p></blockquote>\n\nThe important variables involved in the calculation of the processing time is **Time_Orderd**, **Order_Date** and **Time_Order_picked**.","metadata":{}},{"cell_type":"markdown","source":"#### - Analysing the **Time_Orderd** variable:","metadata":{}},{"cell_type":"code","source":"## Looks like there are ~1700 rows of null values in this column.\ndf.groupBy('Time_Orderd').count().sort(desc(\"count\")).show(10)\n# df.groupBy('Time_Orderd').count().sort(col(\"count\").desc()).select(\"Time_Orderd\").show(1)  ### To view the NaN string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## In this dataset, as we have seen earlier, the NaNs are encoded in string, hence this will show error:\n## df.filter(df[\"Time_Orderd\"].isNaN()).select(\"Time_Orderd\").show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Using normal string NaN in the filter function:\ndf.filter(df['Time_Orderd']==\"NaN\").select('Time_Orderd').show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.filter(df['Time_Orderd']==\"NaN\").count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - Analysing the **Order_Date** variable:","metadata":{}},{"cell_type":"code","source":"## There no null values here:\ndf.groupBy('Order_Date').count().sort(desc(\"count\")).show(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## No null values as below line returns error:\n## df.filter(df[\"Order_Date\"].isNaN()).select(\"Order_Date\").show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.filter(df['Order_Date']==\"NaN\").count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.select('Order_Date').show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We are combining the date and time of ordering together into a single timestamp.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> struct()\n\nWe use struct to pass multiple columns as an argument to a udf.","metadata":{}},{"cell_type":"code","source":"df.select(['Order_Date','Time_Orderd']).show(2)\n\n## Creating a udf without an explicit function, and using multiple columns with the help of struct function::\norder_timestamp_udf = udf(lambda x: x[0]+\" \"+x[1], StringType())\ndf=df.withColumn(\"order_time_timestamp\",order_timestamp_udf(struct('Order_Date','Time_Orderd')))#.select(\"order_time_timestamp\")#.show(5)\n## Viewing the created column:\ndf.select(\"order_time_timestamp\").show(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### -  Analysing the **Time_Order_picked** variable:","metadata":{}},{"cell_type":"code","source":"df.groupBy('Time_Order_picked').count().sort(desc(\"count\")).show(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As you can see, there are NaN values present in the 'Time_Orderd' attribute only. We cannot calculate processing time with NaNs in this column. How do we tackle this. ANY IDEAS?? Let me know your ideas in the comments.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #F3FF00;font-size: 35px\">📌</span>A go-to approach will be to calculate average pickup time using other non null rows and then imputing the null rows with the average obtained.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> to_timestamp()\n\nTo convert a string to timestamp\n\nHere's the list of all the metacharacters and its corresponding meaning:","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimg=Image.open('/kaggle/input/pyspark/Metacharacters Pyspark.png')\nimg=img.resize((700,500))\nimg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen from the above table, we can conclude that the metacharacter format for the final order placed timestamp can be represented as:\n\n'**dd-MM-yyyy HH:mm:ss**'","metadata":{}},{"cell_type":"code","source":"## selecting the Non-null rows\ntemp=df.filter(df['Time_Orderd']!=\"NaN\").select(['Order_Date','Time_Orderd','order_time_timestamp'])\\\n            .withColumn('order_time_timestamp',to_timestamp('order_time_timestamp','dd-MM-yyyy HH:mm:ss'))\ntemp.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #F3FF00;font-size: 35px\">📌</span>NOTE: There is **to_date()** function also available which lets you convert a string date column into date format.","metadata":{}},{"cell_type":"code","source":"### Now we have converted a string to a timestep\ntemp.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupBy('order_time_timestamp').count().sort(desc(\"count\")).show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Getting first row from the above table:\ndf.groupBy('order_time_timestamp').count().sort(desc(\"count\")).collect()[:1][0].__getitem__('order_time_timestamp')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - Looks like NaN has been appended along with the date. Now we will calculate the difference between all the non-NaN containing timestamps of the ordered timestamp with their corresponding delivered timestamp to get the processing time in  minutes.\n\n#### - Then we will impute the NaN containing timestamps with the mean of the processing time.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> contains(), like()\n\nTo extract text containing a certain characters/sequence.","metadata":{}},{"cell_type":"code","source":"## We remove the NaN filled order time from the existing data and treat them separately:\n## To get the non-NaN containing timestamps:(There are two ways to do so.)\n# df.filter(~df[\"order_time_timestamp\"].like(\"%NaN%\")).show(2)\ntemp=df.filter(~df[\"order_time_timestamp\"].contains(\"NaN\")).select(['order_time_timestamp','Time_Order_picked'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Notice the above table, the date components of both the timestamps dont match, and ideally they have to match. Hence they seem misleading. So we will use the time components alone to calculate the difference.","metadata":{}},{"cell_type":"code","source":"temp.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Converting datatype into timestamp:\ntemp=temp.withColumn('order_time_timestamp',to_timestamp(temp['order_time_timestamp'],'dd-MM-yyyy HH:mm:ss'))\ntemp.show(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> date_format()\n\nTo extract parts of a timestamp.","metadata":{}},{"cell_type":"code","source":"temp = temp.withColumn('order_time', date_format('order_time_timestamp', 'HH:mm:ss'))\\\n           .withColumn('Time_picked', date_format('Time_Order_picked', 'HH:mm:ss'))\n\ntemp.show(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> unix_timestamp()\n\nTo convert Date and Timestamp Column to Unix Time.","metadata":{}},{"cell_type":"code","source":"### Calculating the difference in seconds, then dividing by 60 to get minutes\ntemp=temp.withColumn('order_time',to_timestamp(temp['order_time']))\\\n         .withColumn('Time_picked', to_timestamp(temp['Time_picked']))\\\n         .withColumn('processing_time',round((unix_timestamp(\"Time_picked\") - unix_timestamp('order_time'))/60))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Got the desired processing_time in minutes:\ntemp.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Replicating the above steps onto the actual dataframe:\ndf=df.withColumn('order_time_timestamp',to_timestamp(df['order_time_timestamp'],'dd-MM-yyyy HH:mm:ss'))\\\n     .withColumn('order_time', date_format('order_time_timestamp', 'HH:mm:ss'))\\\n     .withColumn('Time_picked', date_format('Time_Order_picked', 'HH:mm:ss'))\\\n     \ndf=df.withColumn('order_time',to_timestamp(df['order_time']))\\\n     .withColumn('Time_picked', to_timestamp(df['Time_picked']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Seems to be right so far\ndf.show(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Creating a column called processing time:\ndf=df.withColumn('processing_time',when(df['Time_Orderd']!=\"NaN\",round((unix_timestamp(\"Time_picked\") - unix_timestamp('order_time'))/60))\\\n                                   .otherwise(lit(np.NaN)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking whether columns are created properly:","metadata":{}},{"cell_type":"code","source":"df.filter(df['Time_Orderd']==\"NaN\").show(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.filter(df['Time_Orderd']!=\"NaN\").show(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like we have done a good job. Now we will replace these NaN values with the mean of the corresponding column:","metadata":{}},{"cell_type":"code","source":"df.filter(df['processing_time']<0).select(['order_time','Time_picked','processing_time']).show()\n## Looks like we have negative values. This can be because:\n## 00:05 - 23:50 = -1425. Hence to correct these negative values, we add 1440(24*60):\ndf=df.withColumn('processing_time',when(df['processing_time']<0,df['processing_time']+1440).otherwise(df['processing_time']))\ndf.filter(df['processing_time']<0).select(['order_time','Time_picked','processing_time']).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like we have made the correction.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> na.fill()\n\nTo fill the NaN values of a specified column with a given value.(Missing value imputation) ","metadata":{}},{"cell_type":"code","source":"## The processing time taken by the restaurant is::\nmean_processing=df.filter(df['Time_Orderd']!=\"NaN\").select(mean(\"processing_time\")).collect()[0].__getitem__('avg(processing_time)')\nmean_processing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Takes column name as parameter:\ndf=df.na.fill(mean_processing,'processing_time')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As we can see from below, we have got our final output:","metadata":{}},{"cell_type":"code","source":"df.filter(df['Time_Orderd']==\"NaN\").select(['order_time','Time_picked','processing_time']).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">6. Splitting the time of ordering into zones of a day - Morning, Lunch, Evening, Night, Midnight:</p></blockquote>","metadata":{}},{"cell_type":"code","source":"## All the available Datetime columns:\ndf.select(['Order_Date','Time_Orderd','order_time','order_time_timestamp','Time_picked','Time_Order_picked']).show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We will use the Time_picked column for the preprocessing as Time_Orderd contains Null Values:","metadata":{}},{"cell_type":"code","source":"## No null values:\ndf.filter(df['Time_picked']==None).show(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## The Time_picked is a timestamp:\ndf.select('Time_picked').show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### So from the above timestamp column, we try to isolate hour:","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> hour()\n\nTo get hour component from a timestamp. We can use other methods like **minute**, **second**, **year**, **month**, **dayofmonth**, **dayofweek**,  **dayofyear**, **weekofyear**.","metadata":{}},{"cell_type":"code","source":"df=df.withColumn('hour_picked',hour(df['Time_picked']))\ndf.select(['Time_picked','hour_picked']).show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Below, we define a function to classify the hours into day-zones. The ranges of hour are custom and can be tweaked by you.","metadata":{}},{"cell_type":"code","source":"def time_of_day(x):\n    if x in [4,5,6,7,8,9,10]:\n        return \"Morning\"\n    elif x in [11,12,13,14,15]:\n        return \"Afternoon\"\n    elif x in [16,17,18,19]:\n        return \"Evening\"\n    elif x in [20,21,22,23]:\n        return \"Night\"\n    else:\n        return \"Midnight\"\n    \ntime_day_udf=udf(time_of_day,StringType())\ndf=df.withColumn('day_zone',time_day_udf(df[\"hour_picked\"]))\ndf.select(['hour_picked','day_zone']).show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">7. Cleaning the target variable:</p></blockquote>","metadata":{}},{"cell_type":"code","source":"## Before transformation:\ndf.select(\"Time_taken(min)\").show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Renaming the column name::\ndf=df.withColumnRenamed('Time_taken(min)','time_taken')\n\n## Removing the preffix (i.e. '(min)') in the column values with the help of a UDF:\ndef target_clean(x):\n    return int(x[-2:])\n\ntarget_clean_udf=udf(lambda x:target_clean(x),IntegerType())\n## Cleaning and Converting type to integer:\ndf=df.withColumn(\"time_taken\",target_clean_udf(df[\"time_taken\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.select('time_taken').show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">8. Bucketing age of delivery agents:</p></blockquote>","metadata":{}},{"cell_type":"code","source":"df.select(['Delivery_person_Age']).distinct().show(30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupBy('Delivery_person_Age').count().orderBy(df['Delivery_person_Age'].asc()).show(25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Deliver Age of 15 and 50 seems to be suspicious. They seem to behave like outliers. Lets keep them though.","metadata":{}},{"cell_type":"code","source":"## Defining a udf:\ndef age_bucket(x):\n    if x in range(15,25):\n        return \"Youth\"\n    else:\n        return \"Adult\"\n    \nage_bucket_udf=udf(age_bucket,StringType())\ndf=df.withColumn('age_bucket',age_bucket_udf(df[\"Delivery_person_Age\"]))\ndf.select(['Delivery_person_Age','age_bucket']).show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">8. Handling the Geo Data:</p></blockquote>","metadata":{}},{"cell_type":"markdown","source":"### **GeoPy** is a Python library that makes geographical calculations easier for the users. There are two ways to calculate distance between two points:\n\n#### 1. Assuming that the Earth is a flat land and calculating distance.\n\n#### 2. Assuming that the Earth is spherical and calculating distance.","metadata":{}},{"cell_type":"markdown","source":"### **Geodesic** measure is used to determine the shortest path between any two points on the globe.\n\nHave a look at this simple article for enahnced understanding: https://www.section.io/engineering-education/using-geopy-to-calculate-the-distance-between-two-points/\n\n#### So we will be calculting geosidic distance between the given source and destination coordinates.","metadata":{}},{"cell_type":"code","source":"from geopy.distance import geodesic \n\n## We define this function:\n\ndef geo_distance(src_lat,src_long,des_lat,des_long):\n    ## initialising the source and destination coordinates:\n    src_coordinates=(src_lat,src_long)\n    des_coordinates=(des_lat,des_long)\n    return geodesic(src_coordinates, des_coordinates).km   ### km method converts the distance into kms\n\n## Creating a udf:\ngeosidic_distance_udf=udf(lambda x:geo_distance(x[0],x[1],x[2],x[3]),FloatType())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.withColumn(\"src_des_distance\",geosidic_distance_udf(df['Restaurant_latitude'],df['Restaurant_longitude'],\\\n                                                       df['Delivery_location_latitude'],df['Delivery_location_longitude'])).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}