{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"font-family: Trebuchet MS; background-color: #58D68D; color: #000000; padding: 12px; line-height: 1.5; font-size:\"> Introduction 🎻</div>\n\n### <div style=\"font-family: Trebuchet MS; background-color: #F4D03F; color: #000000; padding: 12px; line-height: 1.5;\"> Hey Kagglers!! Today I am gonna share with you a simple tool that you can leverage to speeden up the big data processing involved in your own projects. For freshers/experienced practioners, I believe that it is important for y'all to get a basic understanding of the Spark ecosystem as many data-centric companies are continuing to adopt this technology.<br><br> In this notebook, I have tried to compile all the basic functionalities to get you started with Spark effortlessly.</div>\n\n<div style=\"font-family: Trebuchet MS; background-color: #EAECEE; color: #000000; padding: 12px; line-height: 1;\"><h3> Some basic guidelines that I have followed to make this notebook look interactive:</h3><h4><ul style=“list-style-type:square”><li>Whenever there is a definition, I have highlighted it with a  <span style=\"background-color: #2E31FD;font-size: 25px\">📣</span></li><br><li>Whenever there is a new function/method, I have highlighted it with a <span style=\"background-color: #00FF00;font-size: 25px\">🌼</span></li><br><li>Whenever there is a suggestion from my side, I have highlighted it with a <span style=\"background-color: #F3FF00;font-size: 25px\">📌</span></li></ul></h4></div> ","metadata":{}},{"cell_type":"markdown","source":"### So what are you waiting for! Let's get started with the basics:","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"padding: 12px\"><span style=\"background-color: #2E31FD;font-size: 35px\">📣</span> What is Apache Spark in Technical terms.</div>\n\n- Apache Spark is an open-source, distributed data processing and analytics framework designed for large-scale data processing tasks. \n\n- It provides a unified and flexible platform for performing various data processing operations, including batch processing, interactive queries, real-time stream processing, machine learning, and graph processing.","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"padding: 12px\"><span style=\"background-color: #2E31FD;font-size: 35px\">📣</span> What is this Apache Spark with a simple analogy? </div>\n\n- Apache Spark is like a supercharged engine for processing and analyzing really big piles of data. Imagine you have a massive amount of information, like a gigantic puzzle with millions of pieces. Trying to solve this puzzle on a single computer could take forever. But Spark lets you use many computers at once, like a team of puzzle solvers, to work on different parts of the puzzle together.\n\n- These \"puzzle solvers\" (computers) can talk to each other and share their findings, making the work faster and more efficient. Spark also keeps everything organized and makes sure that even if one of the \"puzzle solvers\" takes a break or has a problem, the others can still continue working without losing progress.\n\n- In simple words, Apache Spark helps you process huge amounts of data much faster by getting a bunch of computers to work together and collaborate on the job. It's like a team effort that makes solving big data problems much easier and quicker!","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"padding: 12px\"><span style=\"background-color: #2E31FD;font-size: 35px\">📣</span> What is PySpark?</div>\n\n- PySpark is the Python API to use Spark, just like Pandas.\n\n- In simple words, PySpark is a special tool that combines the power of many computers with the simplicity of Python to help you handle really big piles of data without breaking a sweat!","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"padding: 12px\"><span style=\"background-color: #2E31FD;font-size: 35px\">📣</span> Benefits of using PySpark over Pandas for Data Processing:</div>\n\n#### 1. Scalability and Distributed Computing:\n\n- PySpark is designed for processing large-scale data across clusters of machines. It can handle data sizes that may not fit in memory, as it utilizes distributed computing.\n- Pandas, on the other hand, is designed for single-machine data processing and may struggle with extremely large datasets that exceed available memory.\n\n#### 2. Performance:\n\n- PySpark's in-memory processing and distributed computing can lead to better performance for certain operations on large datasets compared to pandas.\n- While pandas is fast for single-machine operations, PySpark's parallel processing can provide significant performance gains for operations that can be parallelized across multiple nodes.","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"font-family: Trebuchet MS; background-color: #B0E0E6; color: #000000; padding: 12px; line-height: 1.5;\"> Importing Libraries 📚</div>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\nimport regex as re\nimport os\n\n## Supressing warnings:\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-19T10:39:39.589105Z","iopub.execute_input":"2023-08-19T10:39:39.589604Z","iopub.status.idle":"2023-08-19T10:39:40.554911Z","shell.execute_reply.started":"2023-08-19T10:39:39.589558Z","shell.execute_reply":"2023-08-19T10:39:40.553597Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:39:40.557045Z","iopub.execute_input":"2023-08-19T10:39:40.557374Z","iopub.status.idle":"2023-08-19T10:40:34.820207Z","shell.execute_reply.started":"2023-08-19T10:39:40.557346Z","shell.execute_reply":"2023-08-19T10:40:34.818175Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285413 sha256=6403b7fe9b63a84924c12a53be0d8f5867c50cc1910655a672518b47bb50ce0c\n  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.4.1\n","output_type":"stream"}]},{"cell_type":"code","source":"## importing essential spark libraries:\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, split, count, when, regexp_replace, isnan, udf\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:40:34.821838Z","iopub.execute_input":"2023-08-19T10:40:34.822339Z","iopub.status.idle":"2023-08-19T10:40:34.930200Z","shell.execute_reply.started":"2023-08-19T10:40:34.822293Z","shell.execute_reply":"2023-08-19T10:40:34.928953Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"font-family: Trebuchet MS; background-color: #B0E0E6; color: #000000; padding: 12px; line-height: 1.5;\"> Getting Started with the Analysis 🔬</div>\n\n\n#### The first step towards your adventure in Spark is to create a Spark Session. It is the entry point to the Spark ecosystem. Once you<br><br>reach the Spark environment via the entry point, you can freely create and manipulate Spark RDDs, Dataframes and Datasets. ","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"background-color: #2E31FD;font-size: 35px\">📣</span> What is a RDD?\n\nYou might be wondering what this new term is. Well RDD stands for **Resilient Distributed Dataset**. It is the fundamental data structure of Spark.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> SparkSession.builder()\n\n#### SparkSession will be created using SparkSession.builder() builder patterns::","metadata":{}},{"cell_type":"code","source":"##  Creating a Spark session:\nspark = SparkSession.builder.appName('Sample').getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:40:34.931857Z","iopub.execute_input":"2023-08-19T10:40:34.932816Z","iopub.status.idle":"2023-08-19T10:40:41.267352Z","shell.execute_reply.started":"2023-08-19T10:40:34.932772Z","shell.execute_reply":"2023-08-19T10:40:41.266165Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/08/19 10:40:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"## Quick glance at the object\nspark","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:40:41.273722Z","iopub.execute_input":"2023-08-19T10:40:41.274179Z","iopub.status.idle":"2023-08-19T10:40:42.664894Z","shell.execute_reply.started":"2023-08-19T10:40:41.274135Z","shell.execute_reply":"2023-08-19T10:40:42.663750Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<pyspark.sql.session.SparkSession at 0x7a150beb59c0>","text/html":"\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://de1b9d93bd62:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.4.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Sample</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "},"metadata":{}}]},{"cell_type":"markdown","source":"##### Here, the spark object acts as the gateway to the Spark ecosystem. \n\n### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> read.csv(), show()\n\n##### Next in order to read the CSV data, we use the **read.csv** functionality:","metadata":{}},{"cell_type":"code","source":"df=spark.read.csv(\"/kaggle/input/food-delivery-dataset/train.csv\",\n                  header=True,\n                  inferSchema=True)\n#  Parameters:\n## - inferSchema parameter ensures that the data formatting stays the same as the original dataframe. If False, then the \n##     columns will be of class string.\n## - header parameter tells that the columns names are provided along with the dataset.\n\n## Displaying the first 5 rows:\ndf.show(5)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:40:42.666382Z","iopub.execute_input":"2023-08-19T10:40:42.667159Z","iopub.status.idle":"2023-08-19T10:40:51.234666Z","shell.execute_reply.started":"2023-08-19T10:40:42.667109Z","shell.execute_reply":"2023-08-19T10:40:51.233481Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-------------------+--------------------+--------------------+-----------------+-------------+---------------+-------------------+--------+--------------+---------------+\n|     ID|Delivery_person_ID|Delivery_person_Age|Delivery_person_Ratings|Restaurant_latitude|Restaurant_longitude|Delivery_location_latitude|Delivery_location_longitude|Order_Date|Time_Orderd|  Time_Order_picked|   Weatherconditions|Road_traffic_density|Vehicle_condition|Type_of_order|Type_of_vehicle|multiple_deliveries|Festival|          City|Time_taken(min)|\n+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-------------------+--------------------+--------------------+-----------------+-------------+---------------+-------------------+--------+--------------+---------------+\n|0x4607 |   INDORES13DEL02 |               37.0|                    4.9|          22.745049|           75.892471|                 22.765049|                  75.912471|19-03-2022|   11:30:00|2023-08-19 11:45:00|    conditions Sunny|               High |                2|       Snack |    motorcycle |                0.0|     No |        Urban |       (min) 24|\n|0xb379 |   BANGRES18DEL02 |               34.0|                    4.5|          12.913041|           77.683237|                 13.043041|                  77.813237|25-03-2022|   19:45:00|2023-08-19 19:50:00|   conditions Stormy|                Jam |                2|       Snack |       scooter |                1.0|     No |Metropolitian |       (min) 33|\n|0x5d6d |   BANGRES19DEL01 |               23.0|                    4.4|          12.914264|             77.6784|                 12.924264|                    77.6884|19-03-2022|   08:30:00|2023-08-19 08:45:00|conditions Sandst...|                Low |                0|      Drinks |    motorcycle |                1.0|     No |        Urban |       (min) 26|\n|0x7a6a |  COIMBRES13DEL02 |               38.0|                    4.7|          11.003669|           76.976494|                 11.053669|                  77.026494|05-04-2022|   18:00:00|2023-08-19 18:10:00|    conditions Sunny|             Medium |                0|      Buffet |    motorcycle |                1.0|     No |Metropolitian |       (min) 21|\n|0x70a2 |   CHENRES12DEL01 |               32.0|                    4.6|          12.972793|           80.249982|                 13.012793|                  80.289982|26-03-2022|   13:30:00|2023-08-19 13:45:00|   conditions Cloudy|               High |                1|       Snack |       scooter |                1.0|     No |Metropolitian |       (min) 30|\n+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-------------------+--------------------+--------------------+-----------------+-------------+---------------+-------------------+--------+--------------+---------------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> toPandas(), head()","metadata":{}},{"cell_type":"code","source":"## To convert a spark dataframe into a pandas dataframe\ndf.toPandas().head()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:40:51.235789Z","iopub.execute_input":"2023-08-19T10:40:51.236167Z","iopub.status.idle":"2023-08-19T10:40:55.605452Z","shell.execute_reply.started":"2023-08-19T10:40:51.236133Z","shell.execute_reply":"2023-08-19T10:40:55.604032Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"        ID Delivery_person_ID  Delivery_person_Age  Delivery_person_Ratings  \\\n0  0x4607     INDORES13DEL02                  37.0                      4.9   \n1  0xb379     BANGRES18DEL02                  34.0                      4.5   \n2  0x5d6d     BANGRES19DEL01                  23.0                      4.4   \n3  0x7a6a    COIMBRES13DEL02                  38.0                      4.7   \n4  0x70a2     CHENRES12DEL01                  32.0                      4.6   \n\n   Restaurant_latitude  Restaurant_longitude  Delivery_location_latitude  \\\n0            22.745049             75.892471                   22.765049   \n1            12.913041             77.683237                   13.043041   \n2            12.914264             77.678400                   12.924264   \n3            11.003669             76.976494                   11.053669   \n4            12.972793             80.249982                   13.012793   \n\n   Delivery_location_longitude  Order_Date Time_Orderd   Time_Order_picked  \\\n0                    75.912471  19-03-2022    11:30:00 2023-08-19 11:45:00   \n1                    77.813237  25-03-2022    19:45:00 2023-08-19 19:50:00   \n2                    77.688400  19-03-2022    08:30:00 2023-08-19 08:45:00   \n3                    77.026494  05-04-2022    18:00:00 2023-08-19 18:10:00   \n4                    80.289982  26-03-2022    13:30:00 2023-08-19 13:45:00   \n\n       Weatherconditions Road_traffic_density  Vehicle_condition  \\\n0       conditions Sunny                High                   2   \n1      conditions Stormy                 Jam                   2   \n2  conditions Sandstorms                 Low                   0   \n3       conditions Sunny              Medium                   0   \n4      conditions Cloudy                High                   1   \n\n  Type_of_order Type_of_vehicle  multiple_deliveries Festival            City  \\\n0        Snack      motorcycle                   0.0      No           Urban    \n1        Snack         scooter                   1.0      No   Metropolitian    \n2       Drinks      motorcycle                   1.0      No           Urban    \n3       Buffet      motorcycle                   1.0      No   Metropolitian    \n4        Snack         scooter                   1.0      No   Metropolitian    \n\n  Time_taken(min)  \n0        (min) 24  \n1        (min) 33  \n2        (min) 26  \n3        (min) 21  \n4        (min) 30  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Delivery_person_ID</th>\n      <th>Delivery_person_Age</th>\n      <th>Delivery_person_Ratings</th>\n      <th>Restaurant_latitude</th>\n      <th>Restaurant_longitude</th>\n      <th>Delivery_location_latitude</th>\n      <th>Delivery_location_longitude</th>\n      <th>Order_Date</th>\n      <th>Time_Orderd</th>\n      <th>Time_Order_picked</th>\n      <th>Weatherconditions</th>\n      <th>Road_traffic_density</th>\n      <th>Vehicle_condition</th>\n      <th>Type_of_order</th>\n      <th>Type_of_vehicle</th>\n      <th>multiple_deliveries</th>\n      <th>Festival</th>\n      <th>City</th>\n      <th>Time_taken(min)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x4607</td>\n      <td>INDORES13DEL02</td>\n      <td>37.0</td>\n      <td>4.9</td>\n      <td>22.745049</td>\n      <td>75.892471</td>\n      <td>22.765049</td>\n      <td>75.912471</td>\n      <td>19-03-2022</td>\n      <td>11:30:00</td>\n      <td>2023-08-19 11:45:00</td>\n      <td>conditions Sunny</td>\n      <td>High</td>\n      <td>2</td>\n      <td>Snack</td>\n      <td>motorcycle</td>\n      <td>0.0</td>\n      <td>No</td>\n      <td>Urban</td>\n      <td>(min) 24</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0xb379</td>\n      <td>BANGRES18DEL02</td>\n      <td>34.0</td>\n      <td>4.5</td>\n      <td>12.913041</td>\n      <td>77.683237</td>\n      <td>13.043041</td>\n      <td>77.813237</td>\n      <td>25-03-2022</td>\n      <td>19:45:00</td>\n      <td>2023-08-19 19:50:00</td>\n      <td>conditions Stormy</td>\n      <td>Jam</td>\n      <td>2</td>\n      <td>Snack</td>\n      <td>scooter</td>\n      <td>1.0</td>\n      <td>No</td>\n      <td>Metropolitian</td>\n      <td>(min) 33</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x5d6d</td>\n      <td>BANGRES19DEL01</td>\n      <td>23.0</td>\n      <td>4.4</td>\n      <td>12.914264</td>\n      <td>77.678400</td>\n      <td>12.924264</td>\n      <td>77.688400</td>\n      <td>19-03-2022</td>\n      <td>08:30:00</td>\n      <td>2023-08-19 08:45:00</td>\n      <td>conditions Sandstorms</td>\n      <td>Low</td>\n      <td>0</td>\n      <td>Drinks</td>\n      <td>motorcycle</td>\n      <td>1.0</td>\n      <td>No</td>\n      <td>Urban</td>\n      <td>(min) 26</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x7a6a</td>\n      <td>COIMBRES13DEL02</td>\n      <td>38.0</td>\n      <td>4.7</td>\n      <td>11.003669</td>\n      <td>76.976494</td>\n      <td>11.053669</td>\n      <td>77.026494</td>\n      <td>05-04-2022</td>\n      <td>18:00:00</td>\n      <td>2023-08-19 18:10:00</td>\n      <td>conditions Sunny</td>\n      <td>Medium</td>\n      <td>0</td>\n      <td>Buffet</td>\n      <td>motorcycle</td>\n      <td>1.0</td>\n      <td>No</td>\n      <td>Metropolitian</td>\n      <td>(min) 21</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x70a2</td>\n      <td>CHENRES12DEL01</td>\n      <td>32.0</td>\n      <td>4.6</td>\n      <td>12.972793</td>\n      <td>80.249982</td>\n      <td>13.012793</td>\n      <td>80.289982</td>\n      <td>26-03-2022</td>\n      <td>13:30:00</td>\n      <td>2023-08-19 13:45:00</td>\n      <td>conditions Cloudy</td>\n      <td>High</td>\n      <td>1</td>\n      <td>Snack</td>\n      <td>scooter</td>\n      <td>1.0</td>\n      <td>No</td>\n      <td>Metropolitian</td>\n      <td>(min) 30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### As you can see above, Time_taken(min) is the target variable.","metadata":{}},{"cell_type":"markdown","source":"#### Now we have read the csv file into Spark. Lets view the dataframe:","metadata":{}},{"cell_type":"code","source":"## Viewing the type\ntype(df)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:40:55.607580Z","iopub.execute_input":"2023-08-19T10:40:55.608086Z","iopub.status.idle":"2023-08-19T10:40:55.616267Z","shell.execute_reply.started":"2023-08-19T10:40:55.608047Z","shell.execute_reply":"2023-08-19T10:40:55.614901Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"pyspark.sql.dataframe.DataFrame"},"metadata":{}}]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> printSchema()\n\n#### Printing the schema of the dataframe","metadata":{}},{"cell_type":"code","source":"## Printing the attributes of the table:\ndf.printSchema()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:40:55.618741Z","iopub.execute_input":"2023-08-19T10:40:55.619259Z","iopub.status.idle":"2023-08-19T10:40:55.632400Z","shell.execute_reply.started":"2023-08-19T10:40:55.619214Z","shell.execute_reply":"2023-08-19T10:40:55.630740Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"root\n |-- ID: string (nullable = true)\n |-- Delivery_person_ID: string (nullable = true)\n |-- Delivery_person_Age: double (nullable = true)\n |-- Delivery_person_Ratings: double (nullable = true)\n |-- Restaurant_latitude: double (nullable = true)\n |-- Restaurant_longitude: double (nullable = true)\n |-- Delivery_location_latitude: double (nullable = true)\n |-- Delivery_location_longitude: double (nullable = true)\n |-- Order_Date: string (nullable = true)\n |-- Time_Orderd: string (nullable = true)\n |-- Time_Order_picked: timestamp (nullable = true)\n |-- Weatherconditions: string (nullable = true)\n |-- Road_traffic_density: string (nullable = true)\n |-- Vehicle_condition: integer (nullable = true)\n |-- Type_of_order: string (nullable = true)\n |-- Type_of_vehicle: string (nullable = true)\n |-- multiple_deliveries: double (nullable = true)\n |-- Festival: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Time_taken(min): string (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"## Displaying the first 5 rows in the form of col-value pairs\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:40:55.634510Z","iopub.execute_input":"2023-08-19T10:40:55.635099Z","iopub.status.idle":"2023-08-19T10:40:55.826734Z","shell.execute_reply.started":"2023-08-19T10:40:55.635049Z","shell.execute_reply":"2023-08-19T10:40:55.825405Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[Row(ID='0x4607 ', Delivery_person_ID='INDORES13DEL02 ', Delivery_person_Age=37.0, Delivery_person_Ratings=4.9, Restaurant_latitude=22.745049, Restaurant_longitude=75.892471, Delivery_location_latitude=22.765049, Delivery_location_longitude=75.912471, Order_Date='19-03-2022', Time_Orderd='11:30:00', Time_Order_picked=datetime.datetime(2023, 8, 19, 11, 45), Weatherconditions='conditions Sunny', Road_traffic_density='High ', Vehicle_condition=2, Type_of_order='Snack ', Type_of_vehicle='motorcycle ', multiple_deliveries=0.0, Festival='No ', City='Urban ', Time_taken(min)='(min) 24'),\n Row(ID='0xb379 ', Delivery_person_ID='BANGRES18DEL02 ', Delivery_person_Age=34.0, Delivery_person_Ratings=4.5, Restaurant_latitude=12.913041, Restaurant_longitude=77.683237, Delivery_location_latitude=13.043041, Delivery_location_longitude=77.813237, Order_Date='25-03-2022', Time_Orderd='19:45:00', Time_Order_picked=datetime.datetime(2023, 8, 19, 19, 50), Weatherconditions='conditions Stormy', Road_traffic_density='Jam ', Vehicle_condition=2, Type_of_order='Snack ', Type_of_vehicle='scooter ', multiple_deliveries=1.0, Festival='No ', City='Metropolitian ', Time_taken(min)='(min) 33'),\n Row(ID='0x5d6d ', Delivery_person_ID='BANGRES19DEL01 ', Delivery_person_Age=23.0, Delivery_person_Ratings=4.4, Restaurant_latitude=12.914264, Restaurant_longitude=77.6784, Delivery_location_latitude=12.924264, Delivery_location_longitude=77.6884, Order_Date='19-03-2022', Time_Orderd='08:30:00', Time_Order_picked=datetime.datetime(2023, 8, 19, 8, 45), Weatherconditions='conditions Sandstorms', Road_traffic_density='Low ', Vehicle_condition=0, Type_of_order='Drinks ', Type_of_vehicle='motorcycle ', multiple_deliveries=1.0, Festival='No ', City='Urban ', Time_taken(min)='(min) 26'),\n Row(ID='0x7a6a ', Delivery_person_ID='COIMBRES13DEL02 ', Delivery_person_Age=38.0, Delivery_person_Ratings=4.7, Restaurant_latitude=11.003669, Restaurant_longitude=76.976494, Delivery_location_latitude=11.053669, Delivery_location_longitude=77.026494, Order_Date='05-04-2022', Time_Orderd='18:00:00', Time_Order_picked=datetime.datetime(2023, 8, 19, 18, 10), Weatherconditions='conditions Sunny', Road_traffic_density='Medium ', Vehicle_condition=0, Type_of_order='Buffet ', Type_of_vehicle='motorcycle ', multiple_deliveries=1.0, Festival='No ', City='Metropolitian ', Time_taken(min)='(min) 21'),\n Row(ID='0x70a2 ', Delivery_person_ID='CHENRES12DEL01 ', Delivery_person_Age=32.0, Delivery_person_Ratings=4.6, Restaurant_latitude=12.972793, Restaurant_longitude=80.249982, Delivery_location_latitude=13.012793, Delivery_location_longitude=80.289982, Order_Date='26-03-2022', Time_Orderd='13:30:00', Time_Order_picked=datetime.datetime(2023, 8, 19, 13, 45), Weatherconditions='conditions Cloudy', Road_traffic_density='High ', Vehicle_condition=1, Type_of_order='Snack ', Type_of_vehicle='scooter ', multiple_deliveries=1.0, Festival='No ', City='Metropolitian ', Time_taken(min)='(min) 30')]"},"metadata":{}}]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> describe(), summary()","metadata":{}},{"cell_type":"code","source":"## Basic statistics of the data:\ndf.describe()    ### df.summary()\ndf.describe().show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:40:55.833526Z","iopub.execute_input":"2023-08-19T10:40:55.835490Z","iopub.status.idle":"2023-08-19T10:41:02.553401Z","shell.execute_reply.started":"2023-08-19T10:40:55.835382Z","shell.execute_reply":"2023-08-19T10:41:02.551887Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"23/08/19 10:40:56 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n[Stage 7:>                                                          (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+-------+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-----------------+--------------------+------------------+-------------+---------------+-------------------+--------+--------------+---------------+\n|summary|     ID|Delivery_person_ID|Delivery_person_Age|Delivery_person_Ratings|Restaurant_latitude|Restaurant_longitude|Delivery_location_latitude|Delivery_location_longitude|Order_Date|Time_Orderd|Weatherconditions|Road_traffic_density| Vehicle_condition|Type_of_order|Type_of_vehicle|multiple_deliveries|Festival|          City|Time_taken(min)|\n+-------+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-----------------+--------------------+------------------+-------------+---------------+-------------------+--------+--------------+---------------+\n|  count|  45593|             45593|              45593|                  45593|              45593|               45593|                     45593|                      45593|     45593|      45593|            45593|               45593|             45593|        45593|          45593|              45593|   45593|         45593|          45593|\n|   mean|   null|              null|                NaN|                    NaN| 17.017728506525582|   70.23133233807862|        17.465185865088966|          70.84570225567651|      null|        NaN|             null|                 NaN|  1.02335884894611|         null|           null|                NaN|     NaN|           NaN|           null|\n| stddev|   null|              null|                NaN|                    NaN|  8.185108965214452|   22.88364722309305|        7.3351219945143376|         21.118811879085857|      null|        NaN|             null|                 NaN|0.8390647867161859|         null|           null|                NaN|     NaN|           NaN|           null|\n|    min|0x1000 |   AGRRES010DEL01 |               15.0|                    1.0|         -30.905562|          -88.366217|                      0.01|                       0.01|01-03-2022|   00:00:00|conditions Cloudy|               High |                 0|      Buffet |       bicycle |                0.0|    NaN |Metropolitian |       (min) 10|\n|    max| 0xffe |    VADRES20DEL03 |                NaN|                    NaN|          30.914057|           88.433452|                 31.054057|                  88.563452|31-03-2022|       NaN | conditions Windy|                NaN |                 3|       Snack |       scooter |                NaN|    Yes |        Urban |       (min) 54|\n+-------+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-----------------+--------------------+------------------+-------------+---------------+-------------------+--------+--------------+---------------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"#### NOTE: describe() represents the statistical summary of dataframe but it also uses the string variables","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> count(), columns","metadata":{}},{"cell_type":"code","source":"## Shape of the dataframe is:\ndf.count(),len(df.columns)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:41:02.554778Z","iopub.execute_input":"2023-08-19T10:41:02.555487Z","iopub.status.idle":"2023-08-19T10:41:02.995149Z","shell.execute_reply.started":"2023-08-19T10:41:02.555448Z","shell.execute_reply":"2023-08-19T10:41:02.994033Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(45593, 20)"},"metadata":{}}]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> col(), isNull()","metadata":{}},{"cell_type":"code","source":"## Checking for null values:\ndf.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:41:02.997128Z","iopub.execute_input":"2023-08-19T10:41:02.997562Z","iopub.status.idle":"2023-08-19T10:41:04.281246Z","shell.execute_reply.started":"2023-08-19T10:41:02.997522Z","shell.execute_reply":"2023-08-19T10:41:04.280002Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"+---+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-----------------+-----------------+--------------------+-----------------+-------------+---------------+-------------------+--------+----+---------------+\n| ID|Delivery_person_ID|Delivery_person_Age|Delivery_person_Ratings|Restaurant_latitude|Restaurant_longitude|Delivery_location_latitude|Delivery_location_longitude|Order_Date|Time_Orderd|Time_Order_picked|Weatherconditions|Road_traffic_density|Vehicle_condition|Type_of_order|Type_of_vehicle|multiple_deliveries|Festival|City|Time_taken(min)|\n+---+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-----------------+-----------------+--------------------+-----------------+-------------+---------------+-------------------+--------+----+---------------+\n|  0|                 0|                  0|                      0|                  0|                   0|                         0|                          0|         0|          0|                0|                0|                   0|                0|            0|              0|                  0|       0|   0|              0|\n+---+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-----------------+-----------------+--------------------+-----------------+-------------+---------------+-------------------+--------+----+---------------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Looks like there are no null values.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> dtypes","metadata":{}},{"cell_type":"code","source":"## Checking the dtypes:\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:41:04.287115Z","iopub.execute_input":"2023-08-19T10:41:04.287518Z","iopub.status.idle":"2023-08-19T10:41:04.297518Z","shell.execute_reply.started":"2023-08-19T10:41:04.287482Z","shell.execute_reply":"2023-08-19T10:41:04.296292Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[('ID', 'string'),\n ('Delivery_person_ID', 'string'),\n ('Delivery_person_Age', 'double'),\n ('Delivery_person_Ratings', 'double'),\n ('Restaurant_latitude', 'double'),\n ('Restaurant_longitude', 'double'),\n ('Delivery_location_latitude', 'double'),\n ('Delivery_location_longitude', 'double'),\n ('Order_Date', 'string'),\n ('Time_Orderd', 'string'),\n ('Time_Order_picked', 'timestamp'),\n ('Weatherconditions', 'string'),\n ('Road_traffic_density', 'string'),\n ('Vehicle_condition', 'int'),\n ('Type_of_order', 'string'),\n ('Type_of_vehicle', 'string'),\n ('multiple_deliveries', 'double'),\n ('Festival', 'string'),\n ('City', 'string'),\n ('Time_taken(min)', 'string')]"},"metadata":{}}]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> select()","metadata":{}},{"cell_type":"code","source":"## To view a few selected columns:\ndf.select([\"ID\",\"Delivery_person_ID\"]).show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:41:04.299890Z","iopub.execute_input":"2023-08-19T10:41:04.301215Z","iopub.status.idle":"2023-08-19T10:41:04.489891Z","shell.execute_reply.started":"2023-08-19T10:41:04.301162Z","shell.execute_reply":"2023-08-19T10:41:04.488693Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"+-------+------------------+\n|     ID|Delivery_person_ID|\n+-------+------------------+\n|0x4607 |   INDORES13DEL02 |\n|0xb379 |   BANGRES18DEL02 |\n|0x5d6d |   BANGRES19DEL01 |\n|0x7a6a |  COIMBRES13DEL02 |\n|0x70a2 |   CHENRES12DEL01 |\n|0x9bb4 |    HYDRES09DEL03 |\n|0x95b4 | RANCHIRES15DEL01 |\n|0x9eb2 |    MYSRES15DEL02 |\n|0x1102 |    HYDRES05DEL02 |\n|0xcdcd |    DEHRES17DEL01 |\n|0xd987 |    KOCRES16DEL01 |\n|0x2784 |   PUNERES13DEL03 |\n|0xc8b6 |   LUDHRES15DEL02 |\n|0xdb64 |    KNPRES14DEL02 |\n|0x3af3 |    MUMRES15DEL03 |\n|0x3aab |    MYSRES01DEL01 |\n|0x689b |   PUNERES20DEL01 |\n|0x6f67 |    HYDRES14DEL01 |\n|0xc9cf |    KOLRES15DEL03 |\n|0x36b8 |   PUNERES19DEL02 |\n+-------+------------------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:41:04.491188Z","iopub.execute_input":"2023-08-19T10:41:04.493240Z","iopub.status.idle":"2023-08-19T10:41:04.512538Z","shell.execute_reply.started":"2023-08-19T10:41:04.493192Z","shell.execute_reply":"2023-08-19T10:41:04.511282Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"root\n |-- ID: string (nullable = true)\n |-- Delivery_person_ID: string (nullable = true)\n |-- Delivery_person_Age: double (nullable = true)\n |-- Delivery_person_Ratings: double (nullable = true)\n |-- Restaurant_latitude: double (nullable = true)\n |-- Restaurant_longitude: double (nullable = true)\n |-- Delivery_location_latitude: double (nullable = true)\n |-- Delivery_location_longitude: double (nullable = true)\n |-- Order_Date: string (nullable = true)\n |-- Time_Orderd: string (nullable = true)\n |-- Time_Order_picked: timestamp (nullable = true)\n |-- Weatherconditions: string (nullable = true)\n |-- Road_traffic_density: string (nullable = true)\n |-- Vehicle_condition: integer (nullable = true)\n |-- Type_of_order: string (nullable = true)\n |-- Type_of_vehicle: string (nullable = true)\n |-- multiple_deliveries: double (nullable = true)\n |-- Festival: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Time_taken(min): string (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> cast()","metadata":{}},{"cell_type":"markdown","source":"#### The various datatypes that a column can take up are integers, string, double, float, timestamp, etc...\n\n#### To convert a column into:\n\n1. double ---> use DoubleType()\n\n2. int    ---> use IntegerType()\n\n3. float  ---> use FloatType()\n\n4. string ---> use StringType()\n\n5. long   ---> use LongType()\n\n#### all inside the cast() method.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> withColumn()\n\n#### In PySpark, the withColumn() function is widely used and defined as the **transformation function** of the DataFrame\n\n#### which is further\n\n- used to change the value, \n\n- convert the datatype of an existing column, \n\n- create the new column etc...","metadata":{}},{"cell_type":"code","source":"## Have to correct the datatypes of some columns. Delivery_person_Age, Vehicle_condition, multiple_deliveries\ndf=df.withColumn('Delivery_person_Age',col('Delivery_person_Age').cast(IntegerType()))\\\n.withColumn('Vehicle_condition',col('Vehicle_condition').cast(IntegerType()))\\\n.withColumn('multiple_deliveries',col('multiple_deliveries').cast(IntegerType()))","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:41:04.514877Z","iopub.execute_input":"2023-08-19T10:41:04.515822Z","iopub.status.idle":"2023-08-19T10:41:04.656902Z","shell.execute_reply.started":"2023-08-19T10:41:04.515768Z","shell.execute_reply":"2023-08-19T10:41:04.654404Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"## Checking after conversion:\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:41:04.661065Z","iopub.execute_input":"2023-08-19T10:41:04.663065Z","iopub.status.idle":"2023-08-19T10:41:04.729548Z","shell.execute_reply.started":"2023-08-19T10:41:04.662921Z","shell.execute_reply":"2023-08-19T10:41:04.728455Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[('ID', 'string'),\n ('Delivery_person_ID', 'string'),\n ('Delivery_person_Age', 'int'),\n ('Delivery_person_Ratings', 'double'),\n ('Restaurant_latitude', 'double'),\n ('Restaurant_longitude', 'double'),\n ('Delivery_location_latitude', 'double'),\n ('Delivery_location_longitude', 'double'),\n ('Order_Date', 'string'),\n ('Time_Orderd', 'string'),\n ('Time_Order_picked', 'timestamp'),\n ('Weatherconditions', 'string'),\n ('Road_traffic_density', 'string'),\n ('Vehicle_condition', 'int'),\n ('Type_of_order', 'string'),\n ('Type_of_vehicle', 'string'),\n ('multiple_deliveries', 'int'),\n ('Festival', 'string'),\n ('City', 'string'),\n ('Time_taken(min)', 'string')]"},"metadata":{}}]},{"cell_type":"code","source":"df.select(['Delivery_person_Age','Vehicle_condition','multiple_deliveries']).dtypes","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:41:04.731429Z","iopub.execute_input":"2023-08-19T10:41:04.733303Z","iopub.status.idle":"2023-08-19T10:41:04.879096Z","shell.execute_reply.started":"2023-08-19T10:41:04.733250Z","shell.execute_reply":"2023-08-19T10:41:04.877278Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[('Delivery_person_Age', 'int'),\n ('Vehicle_condition', 'int'),\n ('multiple_deliveries', 'int')]"},"metadata":{}}]},{"cell_type":"code","source":"## To display the PySpark dataframe as a pandas dataframe:\ndf.toPandas().head()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:41:04.882229Z","iopub.execute_input":"2023-08-19T10:41:04.883174Z","iopub.status.idle":"2023-08-19T10:41:08.536368Z","shell.execute_reply.started":"2023-08-19T10:41:04.883125Z","shell.execute_reply":"2023-08-19T10:41:08.535290Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"        ID Delivery_person_ID  Delivery_person_Age  Delivery_person_Ratings  \\\n0  0x4607     INDORES13DEL02                    37                      4.9   \n1  0xb379     BANGRES18DEL02                    34                      4.5   \n2  0x5d6d     BANGRES19DEL01                    23                      4.4   \n3  0x7a6a    COIMBRES13DEL02                    38                      4.7   \n4  0x70a2     CHENRES12DEL01                    32                      4.6   \n\n   Restaurant_latitude  Restaurant_longitude  Delivery_location_latitude  \\\n0            22.745049             75.892471                   22.765049   \n1            12.913041             77.683237                   13.043041   \n2            12.914264             77.678400                   12.924264   \n3            11.003669             76.976494                   11.053669   \n4            12.972793             80.249982                   13.012793   \n\n   Delivery_location_longitude  Order_Date Time_Orderd   Time_Order_picked  \\\n0                    75.912471  19-03-2022    11:30:00 2023-08-19 11:45:00   \n1                    77.813237  25-03-2022    19:45:00 2023-08-19 19:50:00   \n2                    77.688400  19-03-2022    08:30:00 2023-08-19 08:45:00   \n3                    77.026494  05-04-2022    18:00:00 2023-08-19 18:10:00   \n4                    80.289982  26-03-2022    13:30:00 2023-08-19 13:45:00   \n\n       Weatherconditions Road_traffic_density  Vehicle_condition  \\\n0       conditions Sunny                High                   2   \n1      conditions Stormy                 Jam                   2   \n2  conditions Sandstorms                 Low                   0   \n3       conditions Sunny              Medium                   0   \n4      conditions Cloudy                High                   1   \n\n  Type_of_order Type_of_vehicle  multiple_deliveries Festival            City  \\\n0        Snack      motorcycle                     0      No           Urban    \n1        Snack         scooter                     1      No   Metropolitian    \n2       Drinks      motorcycle                     1      No           Urban    \n3       Buffet      motorcycle                     1      No   Metropolitian    \n4        Snack         scooter                     1      No   Metropolitian    \n\n  Time_taken(min)  \n0        (min) 24  \n1        (min) 33  \n2        (min) 26  \n3        (min) 21  \n4        (min) 30  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Delivery_person_ID</th>\n      <th>Delivery_person_Age</th>\n      <th>Delivery_person_Ratings</th>\n      <th>Restaurant_latitude</th>\n      <th>Restaurant_longitude</th>\n      <th>Delivery_location_latitude</th>\n      <th>Delivery_location_longitude</th>\n      <th>Order_Date</th>\n      <th>Time_Orderd</th>\n      <th>Time_Order_picked</th>\n      <th>Weatherconditions</th>\n      <th>Road_traffic_density</th>\n      <th>Vehicle_condition</th>\n      <th>Type_of_order</th>\n      <th>Type_of_vehicle</th>\n      <th>multiple_deliveries</th>\n      <th>Festival</th>\n      <th>City</th>\n      <th>Time_taken(min)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x4607</td>\n      <td>INDORES13DEL02</td>\n      <td>37</td>\n      <td>4.9</td>\n      <td>22.745049</td>\n      <td>75.892471</td>\n      <td>22.765049</td>\n      <td>75.912471</td>\n      <td>19-03-2022</td>\n      <td>11:30:00</td>\n      <td>2023-08-19 11:45:00</td>\n      <td>conditions Sunny</td>\n      <td>High</td>\n      <td>2</td>\n      <td>Snack</td>\n      <td>motorcycle</td>\n      <td>0</td>\n      <td>No</td>\n      <td>Urban</td>\n      <td>(min) 24</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0xb379</td>\n      <td>BANGRES18DEL02</td>\n      <td>34</td>\n      <td>4.5</td>\n      <td>12.913041</td>\n      <td>77.683237</td>\n      <td>13.043041</td>\n      <td>77.813237</td>\n      <td>25-03-2022</td>\n      <td>19:45:00</td>\n      <td>2023-08-19 19:50:00</td>\n      <td>conditions Stormy</td>\n      <td>Jam</td>\n      <td>2</td>\n      <td>Snack</td>\n      <td>scooter</td>\n      <td>1</td>\n      <td>No</td>\n      <td>Metropolitian</td>\n      <td>(min) 33</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x5d6d</td>\n      <td>BANGRES19DEL01</td>\n      <td>23</td>\n      <td>4.4</td>\n      <td>12.914264</td>\n      <td>77.678400</td>\n      <td>12.924264</td>\n      <td>77.688400</td>\n      <td>19-03-2022</td>\n      <td>08:30:00</td>\n      <td>2023-08-19 08:45:00</td>\n      <td>conditions Sandstorms</td>\n      <td>Low</td>\n      <td>0</td>\n      <td>Drinks</td>\n      <td>motorcycle</td>\n      <td>1</td>\n      <td>No</td>\n      <td>Urban</td>\n      <td>(min) 26</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x7a6a</td>\n      <td>COIMBRES13DEL02</td>\n      <td>38</td>\n      <td>4.7</td>\n      <td>11.003669</td>\n      <td>76.976494</td>\n      <td>11.053669</td>\n      <td>77.026494</td>\n      <td>05-04-2022</td>\n      <td>18:00:00</td>\n      <td>2023-08-19 18:10:00</td>\n      <td>conditions Sunny</td>\n      <td>Medium</td>\n      <td>0</td>\n      <td>Buffet</td>\n      <td>motorcycle</td>\n      <td>1</td>\n      <td>No</td>\n      <td>Metropolitian</td>\n      <td>(min) 21</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x70a2</td>\n      <td>CHENRES12DEL01</td>\n      <td>32</td>\n      <td>4.6</td>\n      <td>12.972793</td>\n      <td>80.249982</td>\n      <td>13.012793</td>\n      <td>80.289982</td>\n      <td>26-03-2022</td>\n      <td>13:30:00</td>\n      <td>2023-08-19 13:45:00</td>\n      <td>conditions Cloudy</td>\n      <td>High</td>\n      <td>1</td>\n      <td>Snack</td>\n      <td>scooter</td>\n      <td>1</td>\n      <td>No</td>\n      <td>Metropolitian</td>\n      <td>(min) 30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## Checking the numeric columns:\ndef num_cols(dataframe):\n    num_cols = [col for col in dataframe.columns if dataframe.select(col).dtypes[0][1] in ['double', 'int']]\n    return num_cols\n\nnum_cols = num_cols(df)  ### list of numeric columns\n    \ndf.describe(num_cols).show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:41:08.537872Z","iopub.execute_input":"2023-08-19T10:41:08.538223Z","iopub.status.idle":"2023-08-19T10:41:09.902513Z","shell.execute_reply.started":"2023-08-19T10:41:08.538193Z","shell.execute_reply":"2023-08-19T10:41:09.901240Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"+-------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+------------------+-------------------+\n|summary|Delivery_person_Age|Delivery_person_Ratings|Restaurant_latitude|Restaurant_longitude|Delivery_location_latitude|Delivery_location_longitude| Vehicle_condition|multiple_deliveries|\n+-------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+------------------+-------------------+\n|  count|              45593|                  45593|              45593|               45593|                     45593|                      45593|             45593|              45593|\n|   mean| 28.364814774197793|                    NaN| 17.017728506525582|   70.23133233807862|        17.465185865088966|          70.84570225567651|  1.02335884894611| 0.7284451560546575|\n| stddev|  8.157529884739837|                    NaN|  8.185108965214452|   22.88364722309305|        7.3351219945143376|         21.118811879085857|0.8390647867161859| 0.5765432892259538|\n|    min|                  0|                    1.0|         -30.905562|          -88.366217|                      0.01|                       0.01|                 0|                  0|\n|    max|                 50|                    NaN|          30.914057|           88.433452|                 31.054057|                  88.563452|                 3|                  3|\n+-------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+------------------+-------------------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> distinct()","metadata":{}},{"cell_type":"code","source":"### There are 1320 unique IDs\ndf.select('Delivery_person_ID').distinct().count()  ","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:41:09.903886Z","iopub.execute_input":"2023-08-19T10:41:09.907713Z","iopub.status.idle":"2023-08-19T10:41:10.802855Z","shell.execute_reply.started":"2023-08-19T10:41:09.907656Z","shell.execute_reply":"2023-08-19T10:41:10.801689Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"1320"},"metadata":{}}]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> orderBy()","metadata":{}},{"cell_type":"code","source":"### Counts of unique delivery person ids::\ndf.select('Delivery_person_ID').distinct().show()  ### 20 \ndf.groupBy('Delivery_person_ID').count().orderBy('count').show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:41:10.804621Z","iopub.execute_input":"2023-08-19T10:41:10.805109Z","iopub.status.idle":"2023-08-19T10:41:12.226596Z","shell.execute_reply.started":"2023-08-19T10:41:10.805068Z","shell.execute_reply":"2023-08-19T10:41:12.225479Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"+------------------+\n|Delivery_person_ID|\n+------------------+\n|    SURRES11DEL01 |\n|    GOARES02DEL01 |\n|    KNPRES09DEL03 |\n|    KOCRES02DEL01 |\n|    KOLRES08DEL01 |\n|    BHPRES13DEL03 |\n|    ALHRES06DEL02 |\n|    BHPRES05DEL02 |\n|    GOARES03DEL03 |\n|    VADRES16DEL02 |\n|    VADRES04DEL02 |\n|  COIMBRES07DEL01 |\n|    KNPRES08DEL03 |\n|   LUDHRES09DEL01 |\n|    KOCRES09DEL01 |\n|   MUMRES010DEL02 |\n| RANCHIRES11DEL01 |\n|    HYDRES09DEL02 |\n|    DEHRES06DEL02 |\n|    BHPRES09DEL03 |\n+------------------+\nonly showing top 20 rows\n\n+------------------+-----+\n|Delivery_person_ID|count|\n+------------------+-----+\n|   BHPRES010DEL03 |    5|\n|    KOLRES09DEL03 |    6|\n|    KOCRES16DEL03 |    6|\n|    BHPRES15DEL03 |    7|\n|   AURGRES13DEL03 |    7|\n|    GOARES01DEL03 |    7|\n|   AURGRES11DEL03 |    7|\n|    DEHRES18DEL03 |    7|\n|   LUDHRES01DEL03 |    8|\n|    GOARES11DEL01 |    8|\n|    KOLRES08DEL03 |    8|\n|    BHPRES06DEL03 |    8|\n|   AURGRES06DEL03 |    8|\n|   GOARES010DEL03 |    8|\n|    BHPRES11DEL03 |    8|\n|    GOARES19DEL03 |    8|\n|    AGRRES11DEL02 |    8|\n|    GOARES13DEL03 |    8|\n|    GOARES05DEL03 |    8|\n|    BHPRES09DEL03 |    8|\n+------------------+-----+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h2><div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 10px;\n              color:white;\">\n            Feature Engineering Overview\n        </p>\n    </div></h2>","metadata":{}},{"cell_type":"markdown","source":"As observed from the above dataset, we can extract the following:\n\n1. City from Delivery_person_ID ----> city\n\n2. Bucket cities into Zones - North, South, East, West  ----> city_zone\n\n3. Cleaning the Weatherconditions column\n\n4. Time taken to pick up delivery using Time_Orderd and Time_Order_picked ----> pickup_time\n\n5. Time of the day - Morning, Lunch, Evening, Night, Midnight ----> day_zone\n\n6. To clean up target variable - Time_taken(min)\n\n7. Bucket Age - Delivery_person_Age ----> life_stage\n\n8. Features using Latitude and Longitude ----> geosidic\n\n9. \n\n10. Handle NaN values in all column\n\n11. ","metadata":{}},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">1. City from delivery id:</p></blockquote>\n\n### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> udf()\n\n#### In order to apply a function into a particular column, we have create the function and register it as a UDF(User Defined Function) on Spark","metadata":{}},{"cell_type":"code","source":"# Create custom function\ndef city_extract(x):\n    return re.findall(\"(\\S+)RES\\S+\",x)[0]\n\n# Convert the function as a UDF using the udf function:\ncity_extract_UDF = udf(lambda x:city_extract(x),StringType()) \n\n# Apply the function on the desired column:\ndf=df.withColumn(\"City\",city_extract_UDF(col(\"Delivery_person_ID\")))\n\n## Having a glance at the new column:\ndf.select(['Delivery_person_ID','City']).show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:51:38.083306Z","iopub.execute_input":"2023-08-19T11:51:38.083765Z","iopub.status.idle":"2023-08-19T11:51:38.448310Z","shell.execute_reply.started":"2023-08-19T11:51:38.083731Z","shell.execute_reply":"2023-08-19T11:51:38.447013Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"+------------------+------+\n|Delivery_person_ID|  City|\n+------------------+------+\n|   INDORES13DEL02 |  INDO|\n|   BANGRES18DEL02 |  BANG|\n|   BANGRES19DEL01 |  BANG|\n|  COIMBRES13DEL02 | COIMB|\n|   CHENRES12DEL01 |  CHEN|\n|    HYDRES09DEL03 |   HYD|\n| RANCHIRES15DEL01 |RANCHI|\n|    MYSRES15DEL02 |   MYS|\n|    HYDRES05DEL02 |   HYD|\n|    DEHRES17DEL01 |   DEH|\n|    KOCRES16DEL01 |   KOC|\n|   PUNERES13DEL03 |  PUNE|\n|   LUDHRES15DEL02 |  LUDH|\n|    KNPRES14DEL02 |   KNP|\n|    MUMRES15DEL03 |   MUM|\n|    MYSRES01DEL01 |   MYS|\n|   PUNERES20DEL01 |  PUNE|\n|    HYDRES14DEL01 |   HYD|\n|    KOLRES15DEL03 |   KOL|\n|   PUNERES19DEL02 |  PUNE|\n+------------------+------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df.select(\"City\").distinct().show(22)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:18:15.056760Z","iopub.execute_input":"2023-08-19T11:18:15.057222Z","iopub.status.idle":"2023-08-19T11:18:16.078394Z","shell.execute_reply.started":"2023-08-19T11:18:15.057184Z","shell.execute_reply":"2023-08-19T11:18:16.076921Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stderr","text":"[Stage 88:=============================>                            (1 + 1) / 2]\r","output_type":"stream"},{"name":"stdout","text":"+------+\n|  City|\n+------+\n|  LUDH|\n|  CHEN|\n|   KOC|\n|   GOA|\n|  AURG|\n|   JAP|\n|   DEH|\n|   MUM|\n|   AGR|\n|   SUR|\n|  INDO|\n|  PUNE|\n|   ALH|\n|   MYS|\n| COIMB|\n|   HYD|\n|   VAD|\n|RANCHI|\n|   BHP|\n|   KOL|\n|   KNP|\n|  BANG|\n+------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> groupBy(), sort()--asc()/desc()","metadata":{}},{"cell_type":"code","source":"## To get count of the distinct cities:: (equivalent to value_counts() method in pandas)\ndf.groupBy(\"City\").count().sort(col(\"count\").desc()).show(22)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:40:15.449430Z","iopub.execute_input":"2023-08-19T11:40:15.450026Z","iopub.status.idle":"2023-08-19T11:40:16.687531Z","shell.execute_reply.started":"2023-08-19T11:40:15.449984Z","shell.execute_reply":"2023-08-19T11:40:16.686063Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stderr","text":"[Stage 110:>                                                        (0 + 2) / 2]\r","output_type":"stream"},{"name":"stdout","text":"+------+-----+\n|  City|count|\n+------+-----+\n|   JAP| 3443|\n|RANCHI| 3229|\n|  BANG| 3195|\n|   SUR| 3187|\n|   HYD| 3181|\n|   MUM| 3173|\n|   MYS| 3171|\n| COIMB| 3170|\n|   VAD| 3166|\n|  INDO| 3159|\n|  CHEN| 3145|\n|  PUNE| 3132|\n|   AGR|  763|\n|  LUDH|  758|\n|   ALH|  740|\n|   KNP|  740|\n|   DEH|  737|\n|   GOA|  709|\n|  AURG|  703|\n|   KOC|  701|\n|   KOL|  700|\n|   BHP|  691|\n+------+-----+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> rdd.flatMap().collect()\n\nMethods to convert a pyspark column into a list/array:","metadata":{}},{"cell_type":"code","source":"df.select(\"City\").distinct().rdd.flatMap(lambda x: x).collect()  ### to convert a column into a list\ndf.select(\"City\").distinct().toPandas().values.flatten()  ### to convert a column into a numpy array","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:37:53.941945Z","iopub.execute_input":"2023-08-19T11:37:53.942523Z","iopub.status.idle":"2023-08-19T11:37:56.462343Z","shell.execute_reply.started":"2023-08-19T11:37:53.942483Z","shell.execute_reply":"2023-08-19T11:37:56.461154Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"array(['LUDH', 'CHEN', 'KOC', 'GOA', 'AURG', 'JAP', 'DEH', 'MUM', 'AGR',\n       'SUR', 'INDO', 'PUNE', 'ALH', 'MYS', 'COIMB', 'HYD', 'VAD',\n       'RANCHI', 'BHP', 'KOL', 'KNP', 'BANG'], dtype=object)"},"metadata":{}}]},{"cell_type":"markdown","source":"### <span style=\"background-color: #00FF00;font-size: 35px\">🌼</span> withColumnRenamed()\n\n#### Use withColumnRenamed method to rename a column.","metadata":{}},{"cell_type":"code","source":"## Renaming the column to avoid name clash:\ndf=df.withColumnRenamed(\"City\",\"City_encodings\")","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:40:56.098164Z","iopub.execute_input":"2023-08-19T11:40:56.098607Z","iopub.status.idle":"2023-08-19T11:40:56.111534Z","shell.execute_reply.started":"2023-08-19T11:40:56.098570Z","shell.execute_reply":"2023-08-19T11:40:56.110245Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"## Checking to see if change has reflected:\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:41:03.465339Z","iopub.execute_input":"2023-08-19T11:41:03.465786Z","iopub.status.idle":"2023-08-19T11:41:03.478539Z","shell.execute_reply.started":"2023-08-19T11:41:03.465752Z","shell.execute_reply":"2023-08-19T11:41:03.476853Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"['ID',\n 'Delivery_person_ID',\n 'Delivery_person_Age',\n 'Delivery_person_Ratings',\n 'Restaurant_latitude',\n 'Restaurant_longitude',\n 'Delivery_location_latitude',\n 'Delivery_location_longitude',\n 'Order_Date',\n 'Time_Orderd',\n 'Time_Order_picked',\n 'Weatherconditions',\n 'Road_traffic_density',\n 'Vehicle_condition',\n 'Type_of_order',\n 'Type_of_vehicle',\n 'multiple_deliveries',\n 'Festival',\n 'City_encodings',\n 'Time_taken(min)']"},"metadata":{}}]},{"cell_type":"code","source":"### Created a manual list of the full form of the city encodings:\ndic_city={\"LUDH\":\"Ludhiana\",\n\"CHEN\":\"Chennai\",\n\"KOC\":\"Kochi\",\n\"GOA\":\"Goa\",\n\"AURG\":\"Aurangabad\",\n\"JAP\":\"Jaipur\",\n\"DEH\":\"Delhi\",\n\"MUM\":\"Mumbai\",\n\"AGR\":\"Agra\",\n\"SUR\":\"Surat\",\n\"INDO\":\"Indore\",\n\"PUNE\":\"Pune\",\n\"ALH\":\"Allahabad\",\n\"MYS\":\"Mysore\",\n\"COIMB\":\"Coimbatore\",\n\"HYD\":\"Hyderabad\",\n\"VAD\":\"Vadodara\",\n\"RANCHI\":\"Ranchi\",\n\"BHP\":\"Bhopal\",\n\"KOL\":\"Kolkatta\",\n\"KNP\":\"Kanpur\",\n\"BANG\":\"Bangalore\"}\n\n# ## Creating a udf to map the encodings with their original names:\n# def city(x,dic):\n#     return dic[str(x['City_encodings'])]\n# udf_city=udf(lambda x:city(x,dic),StringType())\n# df=df.withColumn(\"City\",udf_city(col(\"City_encodings\")))","metadata":{"execution":{"iopub.status.busy":"2023-08-19T12:08:32.429038Z","iopub.execute_input":"2023-08-19T12:08:32.429532Z","iopub.status.idle":"2023-08-19T12:08:32.437666Z","shell.execute_reply.started":"2023-08-19T12:08:32.429494Z","shell.execute_reply":"2023-08-19T12:08:32.436432Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"## Creating a udf to map the encodings with their original names:\ndef city_map(x,dic):\n    return dic[x['City_encodings']]\nudf_city_map=udf(lambda x:city_map(x,dic),StringType())\ndf=df.withColumn(\"City\",udf_city_map(col(\"City_encodings\")))","metadata":{"execution":{"iopub.status.busy":"2023-08-19T12:08:51.157057Z","iopub.execute_input":"2023-08-19T12:08:51.157453Z","iopub.status.idle":"2023-08-19T12:08:51.193723Z","shell.execute_reply.started":"2023-08-19T12:08:51.157422Z","shell.execute_reply":"2023-08-19T12:08:51.192453Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"df.select(\"City\").show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T12:08:51.498798Z","iopub.execute_input":"2023-08-19T12:08:51.499297Z","iopub.status.idle":"2023-08-19T12:08:51.997056Z","shell.execute_reply.started":"2023-08-19T12:08:51.499261Z","shell.execute_reply":"2023-08-19T12:08:51.995387Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stderr","text":"23/08/19 12:08:51 ERROR Executor: Exception in task 0.0 in stage 128.0 (TID 130)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_32/887845058.py\", line 4, in <lambda>\n  File \"/tmp/ipykernel_32/887845058.py\", line 3, in city_map\nTypeError: string indices must be integers\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/08/19 12:08:51 WARN TaskSetManager: Lost task 0.0 in stage 128.0 (TID 130) (de1b9d93bd62 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_32/887845058.py\", line 4, in <lambda>\n  File \"/tmp/ipykernel_32/887845058.py\", line 3, in city_map\nTypeError: string indices must be integers\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n23/08/19 12:08:51 ERROR TaskSetManager: Task 0 in stage 128.0 failed 1 times; aborting job\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[123], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_32/887845058.py\", line 4, in <lambda>\n  File \"/tmp/ipykernel_32/887845058.py\", line 3, in city_map\nTypeError: string indices must be integers\n"],"ename":"PythonException","evalue":"\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_32/887845058.py\", line 4, in <lambda>\n  File \"/tmp/ipykernel_32/887845058.py\", line 3, in city_map\nTypeError: string indices must be integers\n","output_type":"error"}]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-08-19T12:05:39.092210Z","iopub.execute_input":"2023-08-19T12:05:39.093563Z","iopub.status.idle":"2023-08-19T12:05:39.103920Z","shell.execute_reply.started":"2023-08-19T12:05:39.093501Z","shell.execute_reply":"2023-08-19T12:05:39.103017Z"},"trusted":true},"execution_count":120,"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"[('ID', 'string'),\n ('Delivery_person_ID', 'string'),\n ('Delivery_person_Age', 'int'),\n ('Delivery_person_Ratings', 'double'),\n ('Restaurant_latitude', 'double'),\n ('Restaurant_longitude', 'double'),\n ('Delivery_location_latitude', 'double'),\n ('Delivery_location_longitude', 'double'),\n ('Order_Date', 'string'),\n ('Time_Orderd', 'string'),\n ('Time_Order_picked', 'timestamp'),\n ('Weatherconditions', 'string'),\n ('Road_traffic_density', 'string'),\n ('Vehicle_condition', 'int'),\n ('Type_of_order', 'string'),\n ('Type_of_vehicle', 'string'),\n ('multiple_deliveries', 'int'),\n ('Festival', 'string'),\n ('City_encodings', 'string'),\n ('Time_taken(min)', 'string'),\n ('City', 'string')]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-19T12:05:20.886299Z","iopub.execute_input":"2023-08-19T12:05:20.886804Z","iopub.status.idle":"2023-08-19T12:05:20.922434Z","shell.execute_reply.started":"2023-08-19T12:05:20.886768Z","shell.execute_reply":"2023-08-19T12:05:20.921424Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-19T12:05:22.373018Z","iopub.execute_input":"2023-08-19T12:05:22.373453Z","iopub.status.idle":"2023-08-19T12:05:22.906274Z","shell.execute_reply.started":"2023-08-19T12:05:22.373419Z","shell.execute_reply":"2023-08-19T12:05:22.903035Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stderr","text":"23/08/19 12:05:22 ERROR Executor: Exception in task 0.0 in stage 127.0 (TID 129)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_32/887845058.py\", line 4, in <lambda>\n  File \"/tmp/ipykernel_32/887845058.py\", line 3, in city_map\nTypeError: string indices must be integers\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/08/19 12:05:22 WARN TaskSetManager: Lost task 0.0 in stage 127.0 (TID 129) (de1b9d93bd62 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_32/887845058.py\", line 4, in <lambda>\n  File \"/tmp/ipykernel_32/887845058.py\", line 3, in city_map\nTypeError: string indices must be integers\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n23/08/19 12:05:22 ERROR TaskSetManager: Task 0 in stage 127.0 failed 1 times; aborting job\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_32/887845058.py\", line 4, in <lambda>\n  File \"/tmp/ipykernel_32/887845058.py\", line 3, in city_map\nTypeError: string indices must be integers\n"],"ename":"PythonException","evalue":"\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_32/887845058.py\", line 4, in <lambda>\n  File \"/tmp/ipykernel_32/887845058.py\", line 3, in city_map\nTypeError: string indices must be integers\n","output_type":"error"}]},{"cell_type":"code","source":"df=df.drop(\"City\")","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:57:28.846425Z","iopub.execute_input":"2023-08-19T11:57:28.846914Z","iopub.status.idle":"2023-08-19T11:57:28.865602Z","shell.execute_reply.started":"2023-08-19T11:57:28.846877Z","shell.execute_reply":"2023-08-19T11:57:28.864577Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"df.withColumn(\"City\",udf_city(col(\"City_encodings\"))).show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:55:09.922000Z","iopub.execute_input":"2023-08-19T11:55:09.922378Z","iopub.status.idle":"2023-08-19T11:55:10.462593Z","shell.execute_reply.started":"2023-08-19T11:55:09.922349Z","shell.execute_reply":"2023-08-19T11:55:10.460792Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stderr","text":"23/08/19 11:55:10 ERROR Executor: Exception in task 0.0 in stage 120.0 (TID 122)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_32/752156141.py\", line 28, in <lambda>\n  File \"/tmp/ipykernel_32/752156141.py\", line 27, in city\nTypeError: string indices must be integers\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/08/19 11:55:10 WARN TaskSetManager: Lost task 0.0 in stage 120.0 (TID 122) (de1b9d93bd62 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_32/752156141.py\", line 28, in <lambda>\n  File \"/tmp/ipykernel_32/752156141.py\", line 27, in city\nTypeError: string indices must be integers\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n23/08/19 11:55:10 ERROR TaskSetManager: Task 0 in stage 120.0 failed 1 times; aborting job\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mudf_city\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCity_encodings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_32/752156141.py\", line 28, in <lambda>\n  File \"/tmp/ipykernel_32/752156141.py\", line 27, in city\nTypeError: string indices must be integers\n"],"ename":"PythonException","evalue":"\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_32/752156141.py\", line 28, in <lambda>\n  File \"/tmp/ipykernel_32/752156141.py\", line 27, in city\nTypeError: string indices must be integers\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:57:39.659310Z","iopub.execute_input":"2023-08-19T11:57:39.659756Z","iopub.status.idle":"2023-08-19T11:57:40.054864Z","shell.execute_reply.started":"2023-08-19T11:57:39.659723Z","shell.execute_reply":"2023-08-19T11:57:40.053389Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-------------------+--------------------+--------------------+-----------------+-------------+-----------------+-------------------+--------+--------------+---------------+\n|     ID|Delivery_person_ID|Delivery_person_Age|Delivery_person_Ratings|Restaurant_latitude|Restaurant_longitude|Delivery_location_latitude|Delivery_location_longitude|Order_Date|Time_Orderd|  Time_Order_picked|   Weatherconditions|Road_traffic_density|Vehicle_condition|Type_of_order|  Type_of_vehicle|multiple_deliveries|Festival|City_encodings|Time_taken(min)|\n+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-------------------+--------------------+--------------------+-----------------+-------------+-----------------+-------------------+--------+--------------+---------------+\n|0x4607 |   INDORES13DEL02 |                 37|                    4.9|          22.745049|           75.892471|                 22.765049|                  75.912471|19-03-2022|   11:30:00|2023-08-19 11:45:00|    conditions Sunny|               High |                2|       Snack |      motorcycle |                  0|     No |          INDO|       (min) 24|\n|0xb379 |   BANGRES18DEL02 |                 34|                    4.5|          12.913041|           77.683237|                 13.043041|                  77.813237|25-03-2022|   19:45:00|2023-08-19 19:50:00|   conditions Stormy|                Jam |                2|       Snack |         scooter |                  1|     No |          BANG|       (min) 33|\n|0x5d6d |   BANGRES19DEL01 |                 23|                    4.4|          12.914264|             77.6784|                 12.924264|                    77.6884|19-03-2022|   08:30:00|2023-08-19 08:45:00|conditions Sandst...|                Low |                0|      Drinks |      motorcycle |                  1|     No |          BANG|       (min) 26|\n|0x7a6a |  COIMBRES13DEL02 |                 38|                    4.7|          11.003669|           76.976494|                 11.053669|                  77.026494|05-04-2022|   18:00:00|2023-08-19 18:10:00|    conditions Sunny|             Medium |                0|      Buffet |      motorcycle |                  1|     No |         COIMB|       (min) 21|\n|0x70a2 |   CHENRES12DEL01 |                 32|                    4.6|          12.972793|           80.249982|                 13.012793|                  80.289982|26-03-2022|   13:30:00|2023-08-19 13:45:00|   conditions Cloudy|               High |                1|       Snack |         scooter |                  1|     No |          CHEN|       (min) 30|\n|0x9bb4 |    HYDRES09DEL03 |                 22|                    4.8|          17.431668|           78.408321|                 17.461668|                  78.438321|11-03-2022|   21:20:00|2023-08-19 21:30:00|   conditions Cloudy|                Jam |                0|      Buffet |      motorcycle |                  1|     No |           HYD|       (min) 26|\n|0x95b4 | RANCHIRES15DEL01 |                 33|                    4.7|          23.369746|            85.33982|                 23.479746|                   85.44982|04-03-2022|   19:15:00|2023-08-19 19:30:00|      conditions Fog|                Jam |                1|        Meal |         scooter |                  1|     No |        RANCHI|       (min) 40|\n|0x9eb2 |    MYSRES15DEL02 |                 35|                    4.6|          12.352058|            76.60665|                 12.482058|                   76.73665|14-03-2022|   17:25:00|2023-08-19 17:30:00|   conditions Cloudy|             Medium |                2|        Meal |      motorcycle |                  1|     No |           MYS|       (min) 32|\n|0x1102 |    HYDRES05DEL02 |                 22|                    4.8|          17.433809|           78.386744|                 17.563809|                  78.516744|20-03-2022|   20:55:00|2023-08-19 21:05:00|   conditions Stormy|                Jam |                0|      Buffet |      motorcycle |                  1|     No |           HYD|       (min) 34|\n|0xcdcd |    DEHRES17DEL01 |                 36|                    4.2|          30.327968|           78.046106|                 30.397968|                  78.116106|12-02-2022|   21:55:00|2023-08-19 22:10:00|      conditions Fog|                Jam |                2|       Snack |      motorcycle |                  3|     No |           DEH|       (min) 46|\n|0xd987 |    KOCRES16DEL01 |                 21|                    4.7|          10.003064|           76.307589|                 10.043064|                  76.347589|13-02-2022|   14:55:00|2023-08-19 15:05:00|   conditions Stormy|               High |                1|        Meal |      motorcycle |                  1|     No |           KOC|       (min) 23|\n|0x2784 |   PUNERES13DEL03 |                 23|                    4.7|           18.56245|           73.916619|                  18.65245|                  74.006619|04-03-2022|   17:30:00|2023-08-19 17:40:00|conditions Sandst...|             Medium |                1|      Drinks |         scooter |                  1|     No |          PUNE|       (min) 21|\n|0xc8b6 |   LUDHRES15DEL02 |                 34|                    4.3|          30.899584|           75.809346|                 30.919584|                  75.829346|13-02-2022|   09:20:00|2023-08-19 09:30:00|conditions Sandst...|                Low |                0|      Buffet |      motorcycle |                  0|     No |          LUDH|       (min) 20|\n|0xdb64 |    KNPRES14DEL02 |                 24|                    4.7|          26.463504|           80.372929|                 26.593504|                  80.502929|14-02-2022|   19:50:00|2023-08-19 20:05:00|      conditions Fog|                Jam |                1|       Snack |         scooter |                  1|     No |           KNP|       (min) 41|\n|0x3af3 |    MUMRES15DEL03 |                 29|                    4.5|          19.176269|           72.836721|                 19.266269|                  72.926721|02-04-2022|   20:25:00|2023-08-19 20:35:00|conditions Sandst...|                Jam |                2|      Buffet |electric_scooter |                  1|     No |           MUM|       (min) 20|\n|0x3aab |    MYSRES01DEL01 |                 35|                    4.0|          12.311072|           76.654878|                 12.351072|                  76.694878|01-03-2022|   14:55:00|2023-08-19 15:10:00|    conditions Windy|               High |                1|        Meal |         scooter |                  1|     No |           MYS|       (min) 33|\n|0x689b |   PUNERES20DEL01 |                 33|                    4.2|          18.592718|           73.773572|                 18.702718|                  73.883572|16-03-2022|   20:30:00|2023-08-19 20:40:00|conditions Sandst...|                Jam |                2|       Snack |      motorcycle |                  1|     No |          PUNE|       (min) 40|\n|0x6f67 |    HYDRES14DEL01 |                 34|                    4.9|          17.426228|           78.407495|                 17.496228|                  78.477495|20-03-2022|   20:40:00|2023-08-19 20:50:00|   conditions Cloudy|                Jam |                0|       Snack |      motorcycle |                  0|     No |           HYD|       (min) 41|\n|0xc9cf |    KOLRES15DEL03 |                 21|                    4.7|          22.552672|           88.352885|                 22.582672|                  88.382885|15-02-2022|   21:15:00|2023-08-19 21:30:00|    conditions Windy|                Jam |                0|        Meal |      motorcycle |                  1|     No |           KOL|       (min) 15|\n|0x36b8 |   PUNERES19DEL02 |                 25|                    4.1|          18.563934|           73.915367|                 18.643935|                  73.995367|16-03-2022|   20:20:00|2023-08-19 20:25:00|conditions Sandst...|                Jam |                0|       Snack |      motorcycle |                  2|     No |          PUNE|       (min) 36|\n+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-------------------+--------------------+--------------------+-----------------+-------------+-----------------+-------------------+--------+--------------+---------------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:17:29.959085Z","iopub.execute_input":"2023-08-19T11:17:29.959530Z","iopub.status.idle":"2023-08-19T11:17:31.097916Z","shell.execute_reply.started":"2023-08-19T11:17:29.959494Z","shell.execute_reply":"2023-08-19T11:17:31.096734Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stderr","text":"[Stage 82:=============================>                            (1 + 1) / 2]\r","output_type":"stream"},{"name":"stdout","text":"+------+-----+\n|  City|count|\n+------+-----+\n|   JAP| 3443|\n|RANCHI| 3229|\n|  BANG| 3195|\n|   SUR| 3187|\n|   HYD| 3181|\n|   MUM| 3173|\n|   MYS| 3171|\n| COIMB| 3170|\n|   VAD| 3166|\n|  INDO| 3159|\n|  CHEN| 3145|\n|  PUNE| 3132|\n|   AGR|  763|\n|  LUDH|  758|\n|   ALH|  740|\n|   KNP|  740|\n|   DEH|  737|\n|   GOA|  709|\n|  AURG|  703|\n|   KOC|  701|\n|   KOL|  700|\n|   BHP|  691|\n+------+-----+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">3. Cleaning the Weatherconditions column:</p></blockquote>","metadata":{}},{"cell_type":"code","source":"df.select(\"Weatherconditions\").show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:12:37.713234Z","iopub.execute_input":"2023-08-19T11:12:37.714602Z","iopub.status.idle":"2023-08-19T11:12:37.824995Z","shell.execute_reply.started":"2023-08-19T11:12:37.714531Z","shell.execute_reply":"2023-08-19T11:12:37.824003Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"+--------------------+\n|   Weatherconditions|\n+--------------------+\n|    conditions Sunny|\n|   conditions Stormy|\n|conditions Sandst...|\n|    conditions Sunny|\n|   conditions Cloudy|\n|   conditions Cloudy|\n|      conditions Fog|\n|   conditions Cloudy|\n|   conditions Stormy|\n|      conditions Fog|\n|   conditions Stormy|\n|conditions Sandst...|\n|conditions Sandst...|\n|      conditions Fog|\n|conditions Sandst...|\n|    conditions Windy|\n|conditions Sandst...|\n|   conditions Cloudy|\n|    conditions Windy|\n|conditions Sandst...|\n+--------------------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df.groupBy(\"Weatherconditions\").count().sort(col(\"count\").desc()).show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:14:08.338959Z","iopub.execute_input":"2023-08-19T11:14:08.339412Z","iopub.status.idle":"2023-08-19T11:14:08.850202Z","shell.execute_reply.started":"2023-08-19T11:14:08.339375Z","shell.execute_reply":"2023-08-19T11:14:08.849003Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"+--------------------+-----+\n|   Weatherconditions|count|\n+--------------------+-----+\n|      conditions Fog| 7654|\n|   conditions Stormy| 7586|\n|   conditions Cloudy| 7536|\n|conditions Sandst...| 7495|\n|    conditions Windy| 7422|\n|    conditions Sunny| 7284|\n|      conditions NaN|  616|\n+--------------------+-----+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Looks like there are NaN conditions as well. We will have to clean those 616 data points.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">4. Getting Pickup time:</p></blockquote>","metadata":{}},{"cell_type":"code","source":"## equivalent value counts in python:\n## Looks like there are ~1700 rows of null values in this column.\ndf.groupBy('Time_Orderd').count().sort(col(\"count\").desc()).show(10)\n# df.groupBy('Time_Orderd').count().sort(col(\"count\").desc()).select(\"Time_Orderd\").show(1)  ### To view the NaN string","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:04:25.384905Z","iopub.execute_input":"2023-08-19T11:04:25.385370Z","iopub.status.idle":"2023-08-19T11:04:25.758240Z","shell.execute_reply.started":"2023-08-19T11:04:25.385336Z","shell.execute_reply":"2023-08-19T11:04:25.757026Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"+-----------+-----+\n|Time_Orderd|count|\n+-----------+-----+\n|       NaN | 1731|\n|   21:55:00|  461|\n|   17:55:00|  456|\n|   20:00:00|  449|\n|   22:20:00|  448|\n|   21:35:00|  446|\n|   19:50:00|  444|\n|   21:15:00|  442|\n|   21:20:00|  438|\n|   22:45:00|  438|\n+-----------+-----+\nonly showing top 10 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df.select(df['Order_Date']==\"NaN \").count()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:03:17.680739Z","iopub.execute_input":"2023-08-19T11:03:17.681255Z","iopub.status.idle":"2023-08-19T11:03:17.895289Z","shell.execute_reply.started":"2023-08-19T11:03:17.681217Z","shell.execute_reply":"2023-08-19T11:03:17.894012Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"45593"},"metadata":{}}]},{"cell_type":"code","source":"df.filter(df['Time_Orderd']==\"NaN \").show()#.select(['Time_Orderd']).show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:06:54.318031Z","iopub.execute_input":"2023-08-19T11:06:54.318484Z","iopub.status.idle":"2023-08-19T11:06:54.805375Z","shell.execute_reply.started":"2023-08-19T11:06:54.318452Z","shell.execute_reply":"2023-08-19T11:06:54.804170Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-------------------+--------------------+--------------------+-----------------+-------------+-----------------+-------------------+--------+------+---------------+\n|     ID|Delivery_person_ID|Delivery_person_Age|Delivery_person_Ratings|Restaurant_latitude|Restaurant_longitude|Delivery_location_latitude|Delivery_location_longitude|Order_Date|Time_Orderd|  Time_Order_picked|   Weatherconditions|Road_traffic_density|Vehicle_condition|Type_of_order|  Type_of_vehicle|multiple_deliveries|Festival|  City|Time_taken(min)|\n+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-------------------+--------------------+--------------------+-----------------+-------------+-----------------+-------------------+--------+------+---------------+\n| 0x4f0 |    MUMRES17DEL02 |                  0|                    NaN|          19.121999|           72.908493|                 19.201999|                  72.988493|06-04-2022|       NaN |2023-08-19 18:35:00|   conditions Cloudy|             Medium |                1|      Drinks |         scooter |                  1|     No |   MUM|       (min) 33|\n| 0xa9f |   BANGRES13DEL01 |                  0|                    NaN|          12.935662|            77.61413|                 12.975662|                   77.65413|11-03-2022|       NaN |2023-08-19 15:30:00|      conditions NaN|                NaN |                1|      Drinks |         scooter |                  0|     No |  BANG|       (min) 19|\n| 0x6e2 |    VADRES06DEL01 |                  0|                    NaN|           22.31279|           73.170283|                  22.42279|                  73.280283|02-04-2022|       NaN |2023-08-19 18:25:00|      conditions NaN|                NaN |                3|       Snack |electric_scooter |                  0|     No |   VAD|       (min) 25|\n|0x1ec7 |   PUNERES05DEL03 |                  0|                    NaN|          18.543626|           73.905101|                 18.573626|                  73.935101|09-03-2022|       NaN |2023-08-19 23:40:00|   conditions Stormy|                Low |                2|       Snack |         scooter |                  0|     No |  PUNE|       (min) 19|\n|0x1b58 |    JAPRES03DEL01 |                  0|                    NaN|          26.913483|           75.803139|                 26.983483|                  75.873139|12-03-2022|       NaN |2023-08-19 17:50:00|conditions Sandst...|             Medium |                0|      Drinks |      motorcycle |                  0|     No |   JAP|       (min) 43|\n|0x1b5b | RANCHIRES14DEL01 |                  0|                    NaN|                0.0|                 0.0|                      0.07|                       0.07|02-03-2022|       NaN |2023-08-19 20:50:00|conditions Sandst...|                Jam |                1|       Snack |         scooter |                  0|     No |RANCHI|       (min) 24|\n|0x1b50 |   PUNERES16DEL01 |                  0|                    NaN|          18.536718|           73.830327|                 18.646718|                  73.940327|08-03-2022|       NaN |2023-08-19 21:25:00|      conditions NaN|                NaN |                3|      Buffet |      motorcycle |                  1|    Yes |  PUNE|       (min) 43|\n| 0x4db |   PUNERES02DEL03 |                  0|                    NaN|          -18.55144|          -73.804855|                  18.61144|                  73.864855|05-03-2022|       NaN |2023-08-19 22:05:00|   conditions Stormy|                Jam |                2|        Meal |         scooter |                  0|     No |  PUNE|       (min) 12|\n| 0x6a7 | RANCHIRES08DEL01 |                  0|                    NaN|          23.353783|           85.326967|                 23.463783|                  85.436967|04-04-2022|       NaN |2023-08-19 19:45:00|      conditions Fog|                Jam |                2|      Drinks |         scooter |                  1|     No |RANCHI|       (min) 39|\n| 0x73c |    VADRES12DEL03 |                  0|                    NaN|                0.0|                 0.0|                      0.03|                       0.03|05-03-2022|       NaN |2023-08-19 22:40:00|      conditions Fog|                Low |                0|        Meal |      motorcycle |                  1|     No |   VAD|       (min) 26|\n| 0x4c1 |   BANGRES13DEL02 |                  0|                    NaN|          12.935662|            77.61413|                 13.065662|                   77.74413|02-03-2022|       NaN |2023-08-19 00:05:00|      conditions NaN|                NaN |                3|      Drinks |         scooter |                  1|     No |  BANG|       (min) 26|\n| 0xcee |   BANGRES20DEL01 |                  0|                    NaN|          12.972161|           77.596014|                 13.042161|                  77.666014|20-03-2022|       NaN |2023-08-19 23:05:00|    conditions Sunny|                Low |                1|      Drinks |         scooter |                  2|    Yes |  BANG|       (min) 42|\n| 0x8f8 |    VADRES18DEL01 |                  0|                    NaN|                0.0|                 0.0|                      0.07|                       0.07|10-03-2022|       NaN |2023-08-19 17:55:00|    conditions Sunny|             Medium |                0|       Snack |      motorcycle |                  0|     No |   VAD|       (min) 17|\n|0x1b6a | RANCHIRES18DEL02 |                  0|                    NaN|          23.351489|           85.324253|                 23.431489|                  85.404253|04-04-2022|       NaN |2023-08-19 22:35:00|    conditions Windy|                Low |                1|        Meal |         scooter |                  1|     No |RANCHI|       (min) 28|\n|0xc003 |    DEHRES13DEL02 |                  0|                    NaN|         -30.366322|          -78.070453|                 30.496322|                  78.200453|18-02-2022|       NaN |2023-08-19 22:30:00|      conditions NaN|                NaN |                3|       Snack |         scooter |                  1|     No |   DEH|       (min) 20|\n|0x18c0 |    MYSRES06DEL03 |                  0|                    NaN|          12.323994|           76.626167|                 12.353994|                  76.656167|28-03-2022|       NaN |2023-08-19 18:10:00|   conditions Stormy|             Medium |                0|      Buffet |      motorcycle |                  0|     No |   MYS|       (min) 13|\n| 0x604 |    MYSRES20DEL01 |                  0|                    NaN|          12.337978|           76.616792|                 12.377978|                  76.656792|07-03-2022|       NaN |2023-08-19 13:00:00|      conditions NaN|                NaN |                1|      Drinks |         scooter |                  0|     No |   MYS|       (min) 27|\n| 0xbb9 |   BANGRES08DEL03 |                  0|                    NaN|          12.906229|           77.596791|                 12.966229|                  77.656791|01-03-2022|       NaN |2023-08-19 22:45:00|      conditions NaN|                NaN |                3|      Drinks |         scooter |                  0|     No |  BANG|       (min) 24|\n| 0x6ef |    HYDRES14DEL02 |                  0|                    NaN|          17.426228|           78.407495|                 17.556228|                  78.537495|12-03-2022|       NaN |2023-08-19 17:50:00|    conditions Windy|             Medium |                0|      Drinks |      motorcycle |                  0|     No |   HYD|       (min) 24|\n| 0x953 |    MYSRES07DEL01 |                  0|                    NaN|          12.325461|           76.632278|                 12.335461|                  76.642278|09-03-2022|       NaN |2023-08-19 12:10:00|   conditions Cloudy|               High |                1|      Buffet |      motorcycle |                  1|     No |   MYS|       (min) 23|\n+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-------------------+--------------------+--------------------+-----------------+-------------+-----------------+-------------------+--------+------+---------------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df.select(['Order_Date','Time_Orderd']).show()\n\ndef funct_combine(x):\n    return x['Order_Date']+\" \"+x['Time_Orderd']","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:59:40.406071Z","iopub.execute_input":"2023-08-19T10:59:40.406539Z","iopub.status.idle":"2023-08-19T10:59:40.508678Z","shell.execute_reply.started":"2023-08-19T10:59:40.406501Z","shell.execute_reply":"2023-08-19T10:59:40.507421Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"+----------+-----------+\n|Order_Date|Time_Orderd|\n+----------+-----------+\n|19-03-2022|   11:30:00|\n|25-03-2022|   19:45:00|\n|19-03-2022|   08:30:00|\n|05-04-2022|   18:00:00|\n|26-03-2022|   13:30:00|\n|11-03-2022|   21:20:00|\n|04-03-2022|   19:15:00|\n|14-03-2022|   17:25:00|\n|20-03-2022|   20:55:00|\n|12-02-2022|   21:55:00|\n|13-02-2022|   14:55:00|\n|04-03-2022|   17:30:00|\n|13-02-2022|   09:20:00|\n|14-02-2022|   19:50:00|\n|02-04-2022|   20:25:00|\n|01-03-2022|   14:55:00|\n|16-03-2022|   20:30:00|\n|20-03-2022|   20:40:00|\n|15-02-2022|   21:15:00|\n|16-03-2022|   20:20:00|\n+----------+-----------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df.groupBy('Time_Order_picked').count().sort(col(\"count\").desc()).show(10)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:41:15.896961Z","iopub.execute_input":"2023-08-19T10:41:15.897438Z","iopub.status.idle":"2023-08-19T10:41:16.458374Z","shell.execute_reply.started":"2023-08-19T10:41:15.897395Z","shell.execute_reply":"2023-08-19T10:41:16.457208Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"+-------------------+-----+\n|  Time_Order_picked|count|\n+-------------------+-----+\n|2023-08-19 21:30:00|  496|\n|2023-08-19 22:50:00|  474|\n|2023-08-19 22:40:00|  458|\n|2023-08-19 18:40:00|  457|\n|2023-08-19 17:55:00|  456|\n|2023-08-19 21:45:00|  456|\n|2023-08-19 22:25:00|  455|\n|2023-08-19 18:05:00|  454|\n|2023-08-19 23:50:00|  453|\n|2023-08-19 20:50:00|  453|\n+-------------------+-----+\nonly showing top 10 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As you can see there are NaN values in the 'Time_Orderd' attribute. We cannot calculate delivery time with NaNs in this column. How do we tackle this. ANY IDEAS?? Let me know your ideas in the comments.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"background-color: #F3FF00;font-size: 35px\">📌</span>A go-to approach will be to calculate average pickup time using other non null rows and then imputing the null rows with the average obtained.","metadata":{}},{"cell_type":"code","source":"df.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:41:30.551370Z","iopub.execute_input":"2023-08-19T10:41:30.551757Z","iopub.status.idle":"2023-08-19T10:41:30.988648Z","shell.execute_reply.started":"2023-08-19T10:41:30.551726Z","shell.execute_reply":"2023-08-19T10:41:30.987434Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-------------------+--------------------+--------------------+-----------------+-------------+---------------+-------------------+--------+-----+---------------+\n|     ID|Delivery_person_ID|Delivery_person_Age|Delivery_person_Ratings|Restaurant_latitude|Restaurant_longitude|Delivery_location_latitude|Delivery_location_longitude|Order_Date|Time_Orderd|  Time_Order_picked|   Weatherconditions|Road_traffic_density|Vehicle_condition|Type_of_order|Type_of_vehicle|multiple_deliveries|Festival| City|Time_taken(min)|\n+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-------------------+--------------------+--------------------+-----------------+-------------+---------------+-------------------+--------+-----+---------------+\n|0x4607 |   INDORES13DEL02 |                 37|                    4.9|          22.745049|           75.892471|                 22.765049|                  75.912471|19-03-2022|   11:30:00|2023-08-19 11:45:00|    conditions Sunny|               High |                2|       Snack |    motorcycle |                  0|     No | INDO|       (min) 24|\n|0xb379 |   BANGRES18DEL02 |                 34|                    4.5|          12.913041|           77.683237|                 13.043041|                  77.813237|25-03-2022|   19:45:00|2023-08-19 19:50:00|   conditions Stormy|                Jam |                2|       Snack |       scooter |                  1|     No | BANG|       (min) 33|\n|0x5d6d |   BANGRES19DEL01 |                 23|                    4.4|          12.914264|             77.6784|                 12.924264|                    77.6884|19-03-2022|   08:30:00|2023-08-19 08:45:00|conditions Sandst...|                Low |                0|      Drinks |    motorcycle |                  1|     No | BANG|       (min) 26|\n|0x7a6a |  COIMBRES13DEL02 |                 38|                    4.7|          11.003669|           76.976494|                 11.053669|                  77.026494|05-04-2022|   18:00:00|2023-08-19 18:10:00|    conditions Sunny|             Medium |                0|      Buffet |    motorcycle |                  1|     No |COIMB|       (min) 21|\n|0x70a2 |   CHENRES12DEL01 |                 32|                    4.6|          12.972793|           80.249982|                 13.012793|                  80.289982|26-03-2022|   13:30:00|2023-08-19 13:45:00|   conditions Cloudy|               High |                1|       Snack |       scooter |                  1|     No | CHEN|       (min) 30|\n+-------+------------------+-------------------+-----------------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-------------------+--------------------+--------------------+-----------------+-------------+---------------+-------------------+--------+-----+---------------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df.select(\"Time_Orderd\").show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:57:57.748318Z","iopub.execute_input":"2023-08-19T10:57:57.748726Z","iopub.status.idle":"2023-08-19T10:57:57.846356Z","shell.execute_reply.started":"2023-08-19T10:57:57.748696Z","shell.execute_reply":"2023-08-19T10:57:57.845247Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"+-----------+\n|Time_Orderd|\n+-----------+\n|   11:30:00|\n|   19:45:00|\n|   08:30:00|\n|   18:00:00|\n|   13:30:00|\n|   21:20:00|\n|   19:15:00|\n|   17:25:00|\n|   20:55:00|\n|   21:55:00|\n|   14:55:00|\n|   17:30:00|\n|   09:20:00|\n|   19:50:00|\n|   20:25:00|\n|   14:55:00|\n|   20:30:00|\n|   20:40:00|\n|   21:15:00|\n|   20:20:00|\n+-----------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df.filter(df[\"Time_Orderd\"]!=\"NaN \").select([\"Time_Orderd\",\"Time_Order_picked\"]).show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:57:35.615968Z","iopub.execute_input":"2023-08-19T10:57:35.616378Z","iopub.status.idle":"2023-08-19T10:57:35.748069Z","shell.execute_reply.started":"2023-08-19T10:57:35.616348Z","shell.execute_reply":"2023-08-19T10:57:35.746826Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"+-----------+-------------------+\n|Time_Orderd|  Time_Order_picked|\n+-----------+-------------------+\n|   11:30:00|2023-08-19 11:45:00|\n|   19:45:00|2023-08-19 19:50:00|\n|   08:30:00|2023-08-19 08:45:00|\n|   18:00:00|2023-08-19 18:10:00|\n|   13:30:00|2023-08-19 13:45:00|\n|   21:20:00|2023-08-19 21:30:00|\n|   19:15:00|2023-08-19 19:30:00|\n|   17:25:00|2023-08-19 17:30:00|\n|   20:55:00|2023-08-19 21:05:00|\n|   21:55:00|2023-08-19 22:10:00|\n|   14:55:00|2023-08-19 15:05:00|\n|   17:30:00|2023-08-19 17:40:00|\n|   09:20:00|2023-08-19 09:30:00|\n|   19:50:00|2023-08-19 20:05:00|\n|   20:25:00|2023-08-19 20:35:00|\n|   14:55:00|2023-08-19 15:10:00|\n|   20:30:00|2023-08-19 20:40:00|\n|   20:40:00|2023-08-19 20:50:00|\n|   21:15:00|2023-08-19 21:30:00|\n|   20:20:00|2023-08-19 20:25:00|\n+-----------+-------------------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df.groupBy('Time_Orderd').count().sort(col(\"count\").desc()).select(\"Time_Orderd\").show(1)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:55:25.287264Z","iopub.execute_input":"2023-08-19T10:55:25.287665Z","iopub.status.idle":"2023-08-19T10:55:25.696245Z","shell.execute_reply.started":"2023-08-19T10:55:25.287636Z","shell.execute_reply":"2023-08-19T10:55:25.694990Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"+-----------+\n|Time_Orderd|\n+-----------+\n|       NaN |\n+-----------+\nonly showing top 1 row\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">5. Cleaning the target variable:</p></blockquote>","metadata":{}},{"cell_type":"code","source":"## Before transformation:\ndf.select(\"Time_taken(min)\").show(5)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:34:36.429790Z","iopub.execute_input":"2023-08-17T20:34:36.430275Z","iopub.status.idle":"2023-08-17T20:34:36.518410Z","shell.execute_reply.started":"2023-08-17T20:34:36.430238Z","shell.execute_reply":"2023-08-17T20:34:36.517106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Renaming the column name::\ndf=df.withColumnRenamed('Time_taken(min)','time_taken')\n\n## Removing the preffix (i.e. '(min)') in the column values with the help of a UDF:\ndef target_clean(x):\n    return x[-2:]\n\ntarget_clean_udf=udf(lambda x:target_clean(x),StringType())\ndf=df.withColumn(\"time_taken\",target_clean_udf(col(\"time_taken\")))\n## Converting type:\ndf=df.withColumn(\"time_taken\",col(\"time_taken\").cast(IntegerType()))","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:34:39.455770Z","iopub.execute_input":"2023-08-17T20:34:39.456197Z","iopub.status.idle":"2023-08-17T20:34:39.507656Z","shell.execute_reply.started":"2023-08-17T20:34:39.456162Z","shell.execute_reply":"2023-08-17T20:34:39.506693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## As you can see, the values have been cleaned and the type has been changed:\ndf.select(\"time_taken\").show(5),df.select(\"time_taken\").dtypes","metadata":{"execution":{"iopub.status.busy":"2023-08-17T20:35:26.383056Z","iopub.execute_input":"2023-08-17T20:35:26.383461Z","iopub.status.idle":"2023-08-17T20:35:26.754811Z","shell.execute_reply.started":"2023-08-17T20:35:26.383432Z","shell.execute_reply":"2023-08-17T20:35:26.753783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"font-size:20px; color:#159364; font-family:verdana;\">7. Handling the Geo Data:</p></blockquote>","metadata":{}},{"cell_type":"code","source":"7. \n\n# from geopy.distance import geodesic \n\n# train['distance_diff_KM']=np.zeros(len(train))\n# restaurant_cordinates_train=train[['Restaurant_latitude','Restaurant_longitude']].to_numpy()\n# delivery_location_cordinates_train=train[['Delivery_location_latitude','Delivery_location_longitude']].to_numpy()\n\n# for i in range(len(train)):\n#     train['distance_diff_KM'].loc[i]=geodesic(restaurant_cordinates_train[i],delivery_location_cordinates_train[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}